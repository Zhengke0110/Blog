---
title: 自注意力与位置编码
date: 2025-07-12
type: notes-nlp
---


## 自注意力 (Self-Attention)

在自注意力模型中，序列中的每个词元都会与其他所有词元进行比较，从而计算出该词元的加权表示。这与传统注意力机制不同，后者通常是将一个外部查询与一系列键值对进行比较。在自注意力中，查询（Query）、键（Key）和值（Value）均派生自同一个输入序列。

### 核心思想

给定一个由词元组成的输入序列 $x_1, \dots, x_n$，其中任意 $x_i \in \mathbb{R}^d$。该序列的自注意力输出为一个长度相同的序列 $y_1, \dots, y_n$，其中：

$$
y_i = f(x_i, (x_1, x_1), \dots, (x_n, x_n)) \in \mathbb{R}^d
$$

这里的 $f$ 是注意力池化函数。简单来说，每个输出 $y_i$ 都是通过将查询 $x_i$ 与序列中所有的键值对 $(x_j, x_j)$ 进行注意力计算得到的。

### 与 CNN 和 RNN 的比较

自注意力机制在处理序列数据时，与卷积神经网络（CNN）和循环神经网络（RNN）相比，具有独特的优势和权衡。

| 架构                          | 计算复杂度 | 并行能力               | 最大路径长度 |
| :---------------------------- | :--------- | :--------------------- | :----------- |
| **卷积神经网络 (CNN)**        | $O(knd^2)$ | 可并行 (O(1) 顺序操作) | $O(n/k)$     |
| **循环神经网络 (RNN)**        | $O(nd^2)$  | 顺序 (O(n) 顺序操作)   | $O(n)$       |
| **自注意力 (Self-Attention)** | $O(n^2d)$  | 可并行 (O(1) 顺序操作) | $O(1)$       |

![CNN、RNN和自注意力架构比较](/images/notes/nlp/cnn-rnn-self-attention.svg)

## 位置编码 (Positional Encoding)

自注意力机制本身不处理序列的顺序信息。为了解决这个问题，我们需要在输入表示中引入位置编码，以注入关于词元在序列中绝对或相对位置的信息。

### 基于三角函数的位置编码

一种常用的固定位置编码方法是使用正弦和余弦函数。假设输入表示 $X \in \mathbb{R}^{n \times d}$，位置编码矩阵 $P \in \mathbb{R}^{n \times d}$ 与 $X$ 形状相同，最终的输出为 $X+P$。

矩阵 $P$ 的计算方式如下：

$$
p_{i, 2j} = \sin\left(\frac{i}{10000^{2j/d}}\right)
$$

$$
p_{i, 2j+1} = \cos\left(\frac{i}{10000^{2j/d}}\right)
$$

其中，$i$ 是位置索引，$j$ 是维度索引。

> **换一种方式理解位置编码：**
>
> 想象一下，自注意力机制让模型中的每个词都能“看到”所有其他词，但它本身并没有顺序感，就像一袋子词（a bag of words）。为了让模型理解词语的顺序，我们需要给每个词一个“位置身份证”。
>
> 这个“身份证”不是一个简单的数字（比如 1, 2, 3...），因为这样不利于模型学习。相反，我们用一个向量来表示它，这个向量的生成方式很巧妙，类似于用二进制数表示数字：
>
> - 数字 0 的二进制是 `000`
> - 数字 1 的二进制是 `001`
> - 数字 2 的二进制是 `010`
> - 数字 3 的二进制是 `011`
>
> 注意到吗？最低位（最右边）每个数字都变化，中间位每两个数字变化一次，最高位（最左边）则每四个数字变化一次。
>
> **位置编码就借鉴了这种思想，但使用的是平滑连续的正弦/余弦函数，而不是离散的 0 和 1。**
>
> ![位置编码热力图](/images/notes/nlp/output_self-attention-and-positional-encoding_d76d5a_79_0.svg)
>
> - 编码向量的**低维度**（`j` 较小）使用**高频**函数（类似二进制的低位），它们在每个位置上都快速变化。
> - 编码向量的**高维度**（`j` 较大）使用**低频**函数（类似二进制的高位），它们的变化非常缓慢。
>
> 将这些不同频率的波组合起来，就为序列中的每个位置都创建了一个独一无二的、复杂的“波形签名”或“指纹”。模型可以通过学习识别这些独特的“指纹”来理解词语之间的相对和绝对位置关系。

![位置编码函数图](/images/notes/nlp/output_self-attention-and-positional-encoding_d76d5a_49_0.svg)

### 位置编码的优势

- **绝对位置信息**: 每个位置都有一个唯一的编码向量。
- **相对位置信息**: 对于任何确定的位置偏移 $\delta$，位置 $i+\delta$ 的位置编码可以由位置 $i$ 的编码线性表示。这使得模型能够轻松学习相对位置关系。

## 总结

- **自注意力**: 查询、键和值都来自同一组输入，实现了序列内部的注意力计算。
- **优势与劣势**: 自注意力支持并行计算，善于捕捉长距离依赖，但计算复杂度较高。
- **位置编码**: 通过在输入中添加位置信息，弥补了自注意力模型本身无法感知顺序的缺陷。

