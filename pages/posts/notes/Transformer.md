---
title: Transformer
date: 2025-07-01
type: notes
---

## ä¸€ã€å¼•è¨€ä¸èƒŒæ™¯

### 1. Transformer çš„é‡è¦æ€§

Transformer æ˜¯ä¸€ç§åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œç”± Google åœ¨ 2017 å¹´çš„è®ºæ–‡ã€ŠAttention is All You Needã€‹ä¸­æå‡ºã€‚å®ƒå½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œæˆä¸ºäº†ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ GPTã€BERT ç­‰ï¼‰çš„åŸºç¡€æ¶æ„ã€‚

### 2. ä¼ ç»Ÿæ¨¡å‹çš„å±€é™æ€§

#### 2.1 RNN/LSTM çš„é—®é¢˜

- **é¡ºåºä¾èµ–æ€§**ï¼šRNN å¿…é¡»æŒ‰é¡ºåºå¤„ç†è¾“å…¥ï¼Œæ— æ³•å¹¶è¡Œè®¡ç®—ï¼Œè®­ç»ƒæ•ˆç‡ä½
- **æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸**ï¼šé•¿åºåˆ—ä¸­ä¿¡æ¯ä¼ é€’å›°éš¾ï¼Œéš¾ä»¥æ•è·é•¿è·ç¦»ä¾èµ–å…³ç³»
- **å†…å­˜é™åˆ¶**ï¼šéšçŠ¶æ€å®¹é‡æœ‰é™ï¼Œéš¾ä»¥å­˜å‚¨å¤æ‚çš„ä¸Šä¸‹æ–‡ä¿¡æ¯

#### 2.2 ä¼ ç»Ÿè¯å‘é‡çš„é—®é¢˜

- **é™æ€è¡¨ç¤º**ï¼šWord2Vec ç­‰é¢„è®­ç»ƒè¯å‘é‡æ˜¯å›ºå®šçš„ï¼Œæ— æ³•æ ¹æ®ä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´
- **å¤šä¹‰è¯å›°æ‰°**ï¼šåŒä¸€ä¸ªè¯åœ¨ä¸åŒè¯­å¢ƒä¸­çš„å«ä¹‰æ— æ³•åŒºåˆ†
- **ä¸Šä¸‹æ–‡ç¼ºå¤±**ï¼šæ— æ³•å……åˆ†åˆ©ç”¨å¥å­çº§åˆ«çš„è¯­ä¹‰ä¿¡æ¯

## äºŒã€æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttention Mechanismï¼‰

### 1. æ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒæ€æƒ³

æ³¨æ„åŠ›æœºåˆ¶æ¨¡æ‹Ÿäººç±»çš„æ³¨æ„åŠ›è¿‡ç¨‹ï¼Œè®©æ¨¡å‹èƒ½å¤ŸåŠ¨æ€åœ°å…³æ³¨è¾“å…¥åºåˆ—ä¸­çš„é‡è¦éƒ¨åˆ†ï¼Œè€Œä¸æ˜¯å¹³ç­‰å¯¹å¾…æ‰€æœ‰ä¿¡æ¯ã€‚

### 2. Self-Attentionï¼ˆè‡ªæ³¨æ„åŠ›ï¼‰

#### 2.1 åŸºæœ¬æ¦‚å¿µ

Self-Attention æ˜¯æŒ‡åºåˆ—å†…éƒ¨å…ƒç´ ä¹‹é—´çš„æ³¨æ„åŠ›è®¡ç®—ï¼Œæ¯ä¸ªä½ç½®éƒ½å¯ä»¥å…³æ³¨åºåˆ—ä¸­çš„ä»»æ„ä½ç½®ï¼ŒåŒ…æ‹¬è‡ªèº«ã€‚

**ç»å…¸ä¾‹å­**ï¼š

- "The animal didn't cross the street because **it** was too tired." â†’ "it"æŒ‡å‘"animal"
- "The animal didn't cross the street because **it** was too narrow." â†’ "it"æŒ‡å‘"street"

#### 2.2 æ•°å­¦è®¡ç®—è¿‡ç¨‹

å¯¹äºè¾“å…¥åºåˆ— $X \in \mathbb{R}^{n \times d}$ï¼ŒSelf-Attention çš„è®¡ç®—æ­¥éª¤å¦‚ä¸‹ï¼š

1. **ç”Ÿæˆ Qã€Kã€V çŸ©é˜µ**ï¼š
   $$Q = XW^Q, \quad K = XW^K, \quad V = XW^V$$
   å…¶ä¸­ $W^Q, W^K, W^V \in \mathbb{R}^{d \times d_k}$ æ˜¯å¯å­¦ä¹ çš„æƒé‡çŸ©é˜µ

2. **è®¡ç®—æ³¨æ„åŠ›å¾—åˆ†**ï¼š
   $$\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

3. **ç¼©æ”¾å› å­**ï¼š$\sqrt{d_k}$ ç”¨äºé˜²æ­¢ç‚¹ç§¯è¿‡å¤§å¯¼è‡´ softmax é¥±å’Œ

4. **è¯¦ç»†è®¡ç®—æ­¥éª¤**ï¼š
   - æ³¨æ„åŠ›å¾—åˆ†è®¡ç®—ï¼š$\text{Score}_{ij} = \frac{Q_i \cdot K_j^T}{\sqrt{d_k}}$
   - æ³¨æ„åŠ›æƒé‡ï¼š$\alpha_{ij} = \frac{\exp(\text{Score}_{ij})}{\sum_{k=1}^{n} \exp(\text{Score}_{ik})}$
   - è¾“å‡ºå‘é‡ï¼š$\text{Output}_i = \sum_{j=1}^{n} \alpha_{ij} V_j$

5. **æ—¶é—´å¤æ‚åº¦**ï¼š$O(n^2 \cdot d)$ï¼Œå…¶ä¸­ $n$ æ˜¯åºåˆ—é•¿åº¦ï¼Œ$d$ æ˜¯ç‰¹å¾ç»´åº¦

#### 2.4 Self-Attention è®¡ç®—æµç¨‹å›¾

```mermaid
graph LR
    A["è¾“å…¥åºåˆ— X<br/>Input Sequence<br/>nÃ—d"] --> B["çº¿æ€§å˜æ¢<br/>Linear Transform<br/>ç”ŸæˆQ,K,V"]
    B --> C["æŸ¥è¯¢çŸ©é˜µ Q<br/>Query Matrix<br/>nÃ—d_k"]
    B --> D["é”®çŸ©é˜µ K<br/>Key Matrix<br/>nÃ—d_k"]
    B --> E["å€¼çŸ©é˜µ V<br/>Value Matrix<br/>nÃ—d_v"]

    C --> F["çŸ©é˜µä¹˜æ³•<br/>Matrix Multiply<br/>QK^T"]
    D --> F
    F --> G["ç¼©æ”¾å¤„ç†<br/>Scale<br/>/âˆšd_k"]
    G --> H["Softmax<br/>å½’ä¸€åŒ–<br/>æ³¨æ„åŠ›åˆ†å¸ƒ"]
    H --> I["æ³¨æ„åŠ›æƒé‡<br/>Attention Weights<br/>æ¦‚ç‡åˆ†å¸ƒ"]

    I --> J["åŠ æƒæ±‚å’Œ<br/>Weighted Sum<br/>AttentionÃ—V"]
    E --> J
    J --> K["è¾“å‡ºç‰¹å¾<br/>Output Features<br/>ä¸Šä¸‹æ–‡è¡¨ç¤º"]

    style A fill:#3F51B5,stroke:#303F9F,stroke-width:3px,color:#fff
    style B fill:#FF9800,stroke:#F57C00,stroke-width:3px,color:#fff
    style C fill:#4CAF50,stroke:#43A047,stroke-width:3px,color:#fff
    style D fill:#E91E63,stroke:#C2185B,stroke-width:3px,color:#fff
    style E fill:#9C27B0,stroke:#7B1FA2,stroke-width:3px,color:#fff
    style F fill:#FF5722,stroke:#E64A19,stroke-width:3px,color:#fff
    style G fill:#607D8B,stroke:#455A64,stroke-width:3px,color:#fff
    style H fill:#FFC107,stroke:#FF8F00,stroke-width:3px,color:#333
    style I fill:#2196F3,stroke:#1976D2,stroke-width:3px,color:#fff
    style J fill:#8BC34A,stroke:#689F38,stroke-width:3px,color:#fff
    style K fill:#F44336,stroke:#E53935,stroke-width:3px,color:#fff

    linkStyle 0 stroke:#FF6B6B,stroke-width:3px
    linkStyle 1 stroke:#FF6B6B,stroke-width:3px
    linkStyle 2 stroke:#4ECDC4,stroke-width:3px
    linkStyle 3 stroke:#45B7D1,stroke-width:3px
    linkStyle 4 stroke:#96CEB4,stroke-width:3px
    linkStyle 5 stroke:#FFEAA7,stroke-width:3px
    linkStyle 6 stroke:#DDA0DD,stroke-width:3px
    linkStyle 7 stroke:#98D8C8,stroke-width:3px
    linkStyle 8 stroke:#FFB347,stroke-width:3px
    linkStyle 9 stroke:#87CEEB,stroke-width:3px
    linkStyle 10 stroke:#F7DC6F,stroke-width:3px
    linkStyle 11 stroke:#BB8FCE,stroke-width:3px
```

#### 2.3 Self-Attention çš„ä¼˜åŠ¿

- **å¹¶è¡Œè®¡ç®—**ï¼šæ‰€æœ‰ä½ç½®å¯ä»¥åŒæ—¶è®¡ç®—ï¼Œä¸å­˜åœ¨é¡ºåºä¾èµ–
- **é•¿è·ç¦»ä¾èµ–**ï¼šä»»æ„ä¸¤ä¸ªä½ç½®ä¹‹é—´å¯ä»¥ç›´æ¥å»ºç«‹è¿æ¥
- **åŠ¨æ€æƒé‡**ï¼šæ ¹æ®ä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´æ³¨æ„åŠ›æƒé‡

### 3. Multi-Head Attentionï¼ˆå¤šå¤´æ³¨æ„åŠ›ï¼‰

#### 3.1 è®¾è®¡åŠ¨æœº

å•ä¸ªæ³¨æ„åŠ›å¤´å¯èƒ½åªå…³æ³¨æŸç§ç‰¹å®šçš„æ¨¡å¼ï¼Œå¤šå¤´æ³¨æ„åŠ›å…è®¸æ¨¡å‹åŒæ—¶å…³æ³¨ä¸åŒå­ç©ºé—´çš„ä¿¡æ¯ã€‚

#### 3.2 è®¡ç®—å…¬å¼

$$\text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O$$

å…¶ä¸­æ¯ä¸ªå¤´ï¼š
$$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$

#### 3.3 å‚æ•°ç»´åº¦

- è¾“å…¥ç»´åº¦ï¼š$d_{\text{model}} = 512$
- æ³¨æ„åŠ›å¤´æ•°ï¼š$h = 8$
- æ¯ä¸ªå¤´çš„ç»´åº¦ï¼š$d_k = d_v = \frac{d_{\text{model}}}{h} = 64$
- æƒé‡çŸ©é˜µç»´åº¦ï¼š
  - $W_i^Q, W_i^K, W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_k}$
  - $W^O \in \mathbb{R}^{h \cdot d_v \times d_{\text{model}}}$

#### 3.4 è®¡ç®—å¤æ‚åº¦

- å•å¤´æ³¨æ„åŠ›ï¼š$O(n^2 d_k + n d_k^2)$
- å¤šå¤´æ³¨æ„åŠ›ï¼š$O(h \cdot n^2 d_k + h \cdot n d_k^2) = O(n^2 d_{\text{model}} + n d_{\text{model}}^2)$

#### 3.3 Multi-Head Attention æ¶æ„å›¾

```mermaid
graph LR
    A["è¾“å…¥<br/>Q,K,V<br/>æŸ¥è¯¢é”®å€¼"] --> B1["æ³¨æ„åŠ›å¤´ 1<br/>Head 1<br/>WQ1,WK1,WV1"]
    A --> B2["æ³¨æ„åŠ›å¤´ 2<br/>Head 2<br/>WQ2,WK2,WV2"]
    A --> B3["æ³¨æ„åŠ›å¤´ 3<br/>Head 3<br/>WQ3,WK3,WV3"]
    A --> B4["æ³¨æ„åŠ›å¤´ h<br/>Head h<br/>WQh,WKh,WVh"]

    B1 --> C1["Self-Attention 1<br/>ç‹¬ç«‹æ³¨æ„åŠ›è®¡ç®—<br/>å­ç©ºé—´ç‰¹å¾1"]
    B2 --> C2["Self-Attention 2<br/>ç‹¬ç«‹æ³¨æ„åŠ›è®¡ç®—<br/>å­ç©ºé—´ç‰¹å¾2"]
    B3 --> C3["Self-Attention 3<br/>ç‹¬ç«‹æ³¨æ„åŠ›è®¡ç®—<br/>å­ç©ºé—´ç‰¹å¾3"]
    B4 --> C4["Self-Attention h<br/>ç‹¬ç«‹æ³¨æ„åŠ›è®¡ç®—<br/>å­ç©ºé—´ç‰¹å¾h"]

    C1 --> D["ç‰¹å¾æ‹¼æ¥<br/>Concatenate<br/>å¤šå¤´ç‰¹å¾èåˆ"]
    C2 --> D
    C3 --> D
    C4 --> D

    D --> E["çº¿æ€§å˜æ¢<br/>Linear W^O<br/>è¾“å‡ºæŠ•å½±"]
    E --> F["æœ€ç»ˆè¾“å‡º<br/>Multi-Head Output<br/>ç»¼åˆè¡¨ç¤º"]

    style A fill:#3F51B5,stroke:#303F9F,stroke-width:3px,color:#fff
    style B1 fill:#FF9800,stroke:#F57C00,stroke-width:3px,color:#fff
    style B2 fill:#4CAF50,stroke:#43A047,stroke-width:3px,color:#fff
    style B3 fill:#E91E63,stroke:#C2185B,stroke-width:3px,color:#fff
    style B4 fill:#9C27B0,stroke:#7B1FA2,stroke-width:3px,color:#fff
    style C1 fill:#FF5722,stroke:#E64A19,stroke-width:3px,color:#fff
    style C2 fill:#607D8B,stroke:#455A64,stroke-width:3px,color:#fff
    style C3 fill:#FFC107,stroke:#FF8F00,stroke-width:3px,color:#333
    style C4 fill:#2196F3,stroke:#1976D2,stroke-width:3px,color:#fff
    style D fill:#8BC34A,stroke:#689F38,stroke-width:3px,color:#fff
    style E fill:#795548,stroke:#5D4037,stroke-width:3px,color:#fff
    style F fill:#F44336,stroke:#E53935,stroke-width:3px,color:#fff

    linkStyle 0 stroke:#FF6B6B,stroke-width:3px
    linkStyle 1 stroke:#4ECDC4,stroke-width:3px
    linkStyle 2 stroke:#45B7D1,stroke-width:3px
    linkStyle 3 stroke:#96CEB4,stroke-width:3px
    linkStyle 4 stroke:#FFEAA7,stroke-width:3px
    linkStyle 5 stroke:#DDA0DD,stroke-width:3px
    linkStyle 6 stroke:#98D8C8,stroke-width:3px
    linkStyle 7 stroke:#FFB347,stroke-width:3px
    linkStyle 8 stroke:#87CEEB,stroke-width:3px
    linkStyle 9 stroke:#F7DC6F,stroke-width:3px
    linkStyle 10 stroke:#BB8FCE,stroke-width:3px
    linkStyle 11 stroke:#FF9999,stroke-width:3px
    linkStyle 12 stroke:#85C1E9,stroke-width:3px
    linkStyle 13 stroke:#F8C471,stroke-width:3px
```

#### 3.4 å‚æ•°è¯´æ˜

- $h$ï¼šæ³¨æ„åŠ›å¤´çš„æ•°é‡ï¼ˆé€šå¸¸ä¸º 8 æˆ– 16ï¼‰
- $W_i^Q, W_i^K, W_i^V \in \mathbb{R}^{d \times d_k}$ï¼šç¬¬$i$ä¸ªå¤´çš„æŠ•å½±çŸ©é˜µ
- $W^O \in \mathbb{R}^{hd_v \times d}$ï¼šè¾“å‡ºæŠ•å½±çŸ©é˜µ
- é€šå¸¸è®¾ç½® $d_k = d_v = d/h$ï¼Œä¿è¯å‚æ•°é‡ä¸å˜

## ä¸‰ã€Transformer æ¶æ„è¯¦è§£

### 1. æ•´ä½“æ¶æ„

Transformer é‡‡ç”¨ Encoder-Decoder æ¶æ„ï¼š

- **Encoder**ï¼šå°†è¾“å…¥åºåˆ—ç¼–ç ä¸ºéšçŠ¶æ€è¡¨ç¤º
- **Decoder**ï¼šåŸºäºç¼–ç ç»“æœç”Ÿæˆè¾“å‡ºåºåˆ—

#### 1.1 Transformer æ•´ä½“æ¶æ„æµç¨‹å›¾

```mermaid
graph LR
    A["è¾“å…¥æ–‡æœ¬<br/>Input Text<br/>åŸå§‹åºåˆ—"] --> B["è¯åµŒå…¥<br/>Token Embedding<br/>è¯æ±‡å‘é‡åŒ–"]
    B --> C["ä½ç½®ç¼–ç <br/>Positional Encoding<br/>ä½ç½®ä¿¡æ¯æ³¨å…¥"]
    C --> D["ç¼–ç å™¨æ ˆ<br/>Encoder Stack<br/>Nå±‚ç¼–ç å™¨"]

    D --> E["ç¼–ç è¡¨ç¤º<br/>Encoded Representation<br/>ä¸Šä¸‹æ–‡ç‰¹å¾"]
    E --> F["è§£ç å™¨æ ˆ<br/>Decoder Stack<br/>Nå±‚è§£ç å™¨"]

    G["ç›®æ ‡åºåˆ—<br/>Target Sequence<br/>è¾“å‡ºå¼•å¯¼"] --> H["è¯åµŒå…¥<br/>Token Embedding<br/>ç›®æ ‡å‘é‡åŒ–"]
    H --> I["ä½ç½®ç¼–ç <br/>Positional Encoding<br/>ç›®æ ‡ä½ç½®ä¿¡æ¯"]
    I --> F

    F --> J["çº¿æ€§å±‚<br/>Linear Layer<br/>ç‰¹å¾æ˜ å°„"]
    J --> K["Softmaxå±‚<br/>Softmax<br/>æ¦‚ç‡åˆ†å¸ƒ"]
    K --> L["è¾“å‡ºæ¦‚ç‡<br/>Output Probabilities<br/>è¯æ±‡é¢„æµ‹"]

    style A fill:#3F51B5,stroke:#303F9F,stroke-width:3px,color:#fff
    style B fill:#FF9800,stroke:#F57C00,stroke-width:3px,color:#fff
    style C fill:#4CAF50,stroke:#43A047,stroke-width:3px,color:#fff
    style D fill:#E91E63,stroke:#C2185B,stroke-width:3px,color:#fff
    style E fill:#9C27B0,stroke:#7B1FA2,stroke-width:3px,color:#fff
    style F fill:#FF5722,stroke:#E64A19,stroke-width:3px,color:#fff
    style G fill:#607D8B,stroke:#455A64,stroke-width:3px,color:#fff
    style H fill:#FFC107,stroke:#FF8F00,stroke-width:3px,color:#333
    style I fill:#2196F3,stroke:#1976D2,stroke-width:3px,color:#fff
    style J fill:#8BC34A,stroke:#689F38,stroke-width:3px,color:#fff
    style K fill:#795548,stroke:#5D4037,stroke-width:3px,color:#fff
    style L fill:#F44336,stroke:#E53935,stroke-width:3px,color:#fff

    linkStyle 0 stroke:#FF6B6B,stroke-width:3px
    linkStyle 1 stroke:#4ECDC4,stroke-width:3px
    linkStyle 2 stroke:#45B7D1,stroke-width:3px
    linkStyle 3 stroke:#96CEB4,stroke-width:3px
    linkStyle 4 stroke:#FFEAA7,stroke-width:3px
    linkStyle 5 stroke:#DDA0DD,stroke-width:3px
    linkStyle 6 stroke:#98D8C8,stroke-width:3px
    linkStyle 7 stroke:#FFB347,stroke-width:3px
    linkStyle 8 stroke:#87CEEB,stroke-width:3px
    linkStyle 9 stroke:#F7DC6F,stroke-width:3px
    linkStyle 10 stroke:#BB8FCE,stroke-width:3px
```

### 2. è¾“å…¥å¤„ç†

#### 2.1 è¯åµŒå…¥ï¼ˆToken Embeddingï¼‰

å°†ç¦»æ•£çš„è¯æ±‡è½¬æ¢ä¸ºè¿ç»­çš„å‘é‡è¡¨ç¤ºï¼š
$$\text{Embedding}: \text{vocab\_size} \rightarrow d_{\text{model}}$$

#### 2.2 ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰

ç”±äº Self-Attention ç¼ºä¹ä½ç½®ä¿¡æ¯ï¼Œéœ€è¦æ·»åŠ ä½ç½®ç¼–ç ï¼š

$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)$$
$$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)$$

å…¶ä¸­ï¼š

- $pos$ï¼šä½ç½®ç´¢å¼•ï¼ˆ$0 \leq pos < \text{max\_len}$ï¼‰
- $i$ï¼šç»´åº¦ç´¢å¼•ï¼ˆ$0 \leq i < d_{\text{model}}/2$ï¼‰
- $d_{\text{model}}$ï¼šæ¨¡å‹ç»´åº¦

#### 2.3 ä½ç½®ç¼–ç çš„æ•°å­¦ç‰¹æ€§

ä½ç½®ç¼–ç å…·æœ‰ä»¥ä¸‹é‡è¦ç‰¹æ€§ï¼š

1. **å”¯ä¸€æ€§**ï¼šæ¯ä¸ªä½ç½®éƒ½æœ‰å”¯ä¸€çš„ç¼–ç å‘é‡
2. **ç›¸å¯¹ä½ç½®æ„ŸçŸ¥**ï¼šé€šè¿‡ä¸‰è§’å‡½æ•°çš„åŠ æ³•å®šç†å®ç°
3. **å¤–æ¨èƒ½åŠ›**ï¼šå¯ä»¥å¤„ç†æ¯”è®­ç»ƒæ—¶æ›´é•¿çš„åºåˆ—

**ç›¸å¯¹ä½ç½®è®¡ç®—**ï¼š
$$PE_{pos+k} = PE_{pos} \cdot M_k + PE_{pos}^{\perp} \cdot N_k$$

å…¶ä¸­ $M_k$ å’Œ $N_k$ åªä¾èµ–äºç›¸å¯¹è·ç¦» $k$ã€‚

#### 2.4 è¾“å…¥ç»„åˆ

æœ€ç»ˆè¾“å…¥ä¸ºè¯åµŒå…¥ä¸ä½ç½®ç¼–ç çš„å…ƒç´ çº§ç›¸åŠ ï¼š
$$\text{Input} = \text{TokenEmbedding} + \text{PositionalEncoding}$$

### 3. Encoder ç»“æ„

æ¯ä¸ª Encoder å±‚åŒ…å«ï¼š

#### 3.1 Encoder å±‚å†…éƒ¨æµç¨‹å›¾

```mermaid
graph LR
    A["è¾“å…¥ç‰¹å¾<br/>Input Features<br/>åºåˆ—è¡¨ç¤º"] --> B["å¤šå¤´è‡ªæ³¨æ„åŠ›<br/>Multi-Head Self-Attention<br/>å…¨å±€ä¾èµ–å»ºæ¨¡"]
    B --> C["æ®‹å·®è¿æ¥&å½’ä¸€åŒ–<br/>Add & Norm<br/>ç¨³å®šè®­ç»ƒ"]
    A --> C
    C --> D["å‰é¦ˆç¥ç»ç½‘ç»œ<br/>Feed Forward<br/>éçº¿æ€§å˜æ¢"]
    D --> E["æ®‹å·®è¿æ¥&å½’ä¸€åŒ–<br/>Add & Norm<br/>è¾“å‡ºç¨³å®š"]
    C --> E
    E --> F["è¾“å‡ºç‰¹å¾<br/>Output Features<br/>ç¼–ç è¡¨ç¤º"]

    style A fill:#3F51B5,stroke:#303F9F,stroke-width:3px,color:#fff
    style B fill:#FF9800,stroke:#F57C00,stroke-width:3px,color:#fff
    style C fill:#4CAF50,stroke:#43A047,stroke-width:3px,color:#fff
    style D fill:#E91E63,stroke:#C2185B,stroke-width:3px,color:#fff
    style E fill:#9C27B0,stroke:#7B1FA2,stroke-width:3px,color:#fff
    style F fill:#F44336,stroke:#E53935,stroke-width:3px,color:#fff

    linkStyle 0 stroke:#FF6B6B,stroke-width:3px
    linkStyle 1 stroke:#4ECDC4,stroke-width:3px
    linkStyle 2 stroke:#45B7D1,stroke-width:3px
    linkStyle 3 stroke:#96CEB4,stroke-width:3px
    linkStyle 4 stroke:#FFEAA7,stroke-width:3px
    linkStyle 5 stroke:#DDA0DD,stroke-width:3px
```

æ¯ä¸ª Encoder å±‚åŒ…å«ï¼š

#### 3.1 Multi-Head Self-Attention

- è¾“å…¥ï¼š$X \in \mathbb{R}^{n \times d}$
- è¾“å‡ºï¼šæ³¨æ„åŠ›åŠ æƒåçš„è¡¨ç¤º

#### 3.2 Position-wise Feed-Forward Network

$$\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2$$

- ä¸¤å±‚å…¨è¿æ¥ç½‘ç»œï¼Œä¸­é—´ä½¿ç”¨ ReLU æ¿€æ´»
- é€šå¸¸ä¸­é—´å±‚ç»´åº¦ä¸º $4d_{\text{model}}$

#### 3.3 æ®‹å·®è¿æ¥ä¸å±‚å½’ä¸€åŒ–

$$\text{LayerNorm}(x + \text{Sublayer}(x))$$

**å±‚å½’ä¸€åŒ–çš„è¯¦ç»†è®¡ç®—**ï¼š

1. **è®¡ç®—å‡å€¼å’Œæ–¹å·®**ï¼š
   $$\mu = \frac{1}{d} \sum_{i=1}^{d} x_i$$
   $$\sigma^2 = \frac{1}{d} \sum_{i=1}^{d} (x_i - \mu)^2$$

2. **æ ‡å‡†åŒ–**ï¼š
   $$\hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}$$

3. **ç¼©æ”¾å’Œå¹³ç§»**ï¼š
   $$\text{LayerNorm}(x_i) = \gamma \hat{x}_i + \beta$$

å…¶ä¸­ï¼š
- $\gamma$ å’Œ $\beta$ æ˜¯å¯å­¦ä¹ å‚æ•°
- $\epsilon$ æ˜¯é˜²æ­¢é™¤é›¶çš„å°å¸¸æ•°ï¼ˆé€šå¸¸ä¸º $10^{-6}$ï¼‰
- $d$ æ˜¯ç‰¹å¾ç»´åº¦

**æ®‹å·®è¿æ¥çš„ä½œç”¨**ï¼š
- ç¼“è§£æ·±å±‚ç½‘ç»œçš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
- æä¾›ä¿¡æ¯çš„ç›´æ¥é€šé“
- ä½¿å¾—ç½‘ç»œå¯ä»¥å­¦ä¹ æ’ç­‰æ˜ å°„

### 4. Decoder ç»“æ„

Decoder åœ¨ Encoder åŸºç¡€ä¸Šå¢åŠ äº†ï¼š

#### 4.1 Decoder å±‚å†…éƒ¨æµç¨‹å›¾

```mermaid
graph LR
    A["ç›®æ ‡è¾“å…¥<br/>Target Input<br/>è¾“å‡ºåºåˆ—"] --> B["æ©ç è‡ªæ³¨æ„åŠ›<br/>Masked Self-Attention<br/>é˜²æ­¢ä¿¡æ¯æ³„éœ²"]
    B --> C["æ®‹å·®è¿æ¥&å½’ä¸€åŒ–<br/>Add & Norm<br/>ç¬¬ä¸€å±‚ç¨³å®š"]
    A --> C
    C --> D["ç¼–ç -è§£ç æ³¨æ„åŠ›<br/>Encoder-Decoder Attention<br/>æºåºåˆ—å…³æ³¨"]
    E["ç¼–ç å™¨è¾“å‡º<br/>Encoder Output<br/>æºåºåˆ—è¡¨ç¤º"] --> D
    D --> F["æ®‹å·®è¿æ¥&å½’ä¸€åŒ–<br/>Add & Norm<br/>ç¬¬äºŒå±‚ç¨³å®š"]
    C --> F
    F --> G["å‰é¦ˆç¥ç»ç½‘ç»œ<br/>Feed Forward<br/>éçº¿æ€§å˜æ¢"]
    G --> H["æ®‹å·®è¿æ¥&å½’ä¸€åŒ–<br/>Add & Norm<br/>æœ€ç»ˆç¨³å®š"]
    F --> H
    H --> I["è§£ç è¾“å‡º<br/>Decoder Output<br/>ç”Ÿæˆè¡¨ç¤º"]

    style A fill:#3F51B5,stroke:#303F9F,stroke-width:3px,color:#fff
    style B fill:#FF9800,stroke:#F57C00,stroke-width:3px,color:#fff
    style C fill:#4CAF50,stroke:#43A047,stroke-width:3px,color:#fff
    style D fill:#E91E63,stroke:#C2185B,stroke-width:3px,color:#fff
    style E fill:#9C27B0,stroke:#7B1FA2,stroke-width:3px,color:#fff
    style F fill:#FF5722,stroke:#E64A19,stroke-width:3px,color:#fff
    style G fill:#607D8B,stroke:#455A64,stroke-width:3px,color:#fff
    style H fill:#FFC107,stroke:#FF8F00,stroke-width:3px,color:#333
    style I fill:#F44336,stroke:#E53935,stroke-width:3px,color:#fff

    linkStyle 0 stroke:#FF6B6B,stroke-width:3px
    linkStyle 1 stroke:#4ECDC4,stroke-width:3px
    linkStyle 2 stroke:#45B7D1,stroke-width:3px
    linkStyle 3 stroke:#96CEB4,stroke-width:3px
    linkStyle 4 stroke:#FFEAA7,stroke-width:3px
    linkStyle 5 stroke:#DDA0DD,stroke-width:3px
    linkStyle 6 stroke:#98D8C8,stroke-width:3px
    linkStyle 7 stroke:#FFB347,stroke-width:3px
    linkStyle 8 stroke:#87CEEB,stroke-width:3px
```

#### 4.2 Masked Self-Attention

- åœ¨è®­ç»ƒæ—¶é˜²æ­¢æ¨¡å‹"çœ‹åˆ°æœªæ¥"çš„ä¿¡æ¯
- ä½¿ç”¨ä¸‹ä¸‰è§’æ©ç çŸ©é˜µï¼š
  $$
  \text{mask}_{i,j} = \begin{cases}
  0 & \text{if } j \leq i \\
  -\infty & \text{if } j > i
  \end{cases}
  $$

#### 4.3 Encoder-Decoder Attention

- Query æ¥è‡ª Decoderï¼ŒKey å’Œ Value æ¥è‡ª Encoder
- å…è®¸ Decoder å…³æ³¨è¾“å…¥åºåˆ—çš„ç›¸å…³éƒ¨åˆ†

### 5. è¾“å‡ºå±‚

#### 5.1 çº¿æ€§å˜æ¢

$$\text{Linear}: d_{\text{model}} \rightarrow \text{vocab\_size}$$

#### 5.2 Softmax

$$P(w_i) = \frac{\exp(z_i)}{\sum_{j=1}^{|\text{vocab}|} \exp(z_j)}$$

## å››ã€è®­ç»ƒä¸ä¼˜åŒ–

### 1. æŸå¤±å‡½æ•°

ä½¿ç”¨äº¤å‰ç†µæŸå¤±ï¼š
$$\mathcal{L} = -\sum_{i=1}^{n} \sum_{j=1}^{|\text{vocab}|} y_{i,j} \log(\hat{y}_{i,j})$$

### 2. ä¼˜åŒ–æŠ€å·§

#### 2.1 å­¦ä¹ ç‡è°ƒåº¦

åŸè®ºæ–‡ä½¿ç”¨äº†é¢„çƒ­+è¡°å‡çš„å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥ï¼š

$$\text{lr} = d_{\text{model}}^{-0.5} \cdot \min(\text{step\_num}^{-0.5}, \text{step\_num} \cdot \text{warmup\_steps}^{-1.5})$$

- **é¢„çƒ­é˜¶æ®µ**ï¼šå­¦ä¹ ç‡çº¿æ€§å¢åŠ åˆ°å³°å€¼
- **è¡°å‡é˜¶æ®µ**ï¼šå­¦ä¹ ç‡æŒ‰æ­¥æ•°çš„å¹³æ–¹æ ¹è¡°å‡

#### 2.2 æ­£åˆ™åŒ–æŠ€æœ¯

- **Dropout**ï¼šåœ¨æ³¨æ„åŠ›æƒé‡å’Œå‰é¦ˆç½‘ç»œä¸­åº”ç”¨ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
  $$\text{Dropout}(x) = \begin{cases}
  \frac{x}{1-p} & \text{è®­ç»ƒæ—¶ï¼Œæ¦‚ç‡ä¸º } 1-p \\
  x & \text{æ¨ç†æ—¶}
  \end{cases}$$

- **Label Smoothing**ï¼šæé«˜æ³›åŒ–èƒ½åŠ›
  $$\tilde{y}_k = (1-\alpha) y_k + \frac{\alpha}{K}$$
  å…¶ä¸­ $\alpha$ æ˜¯å¹³æ»‘å‚æ•°ï¼Œ$K$ æ˜¯ç±»åˆ«æ•°

### 3. è®¡ç®—å¤æ‚åº¦

- Self-Attentionï¼š$O(n^2 \cdot d)$
- Feed-Forwardï¼š$O(n \cdot d^2)$
- å…¶ä¸­$n$æ˜¯åºåˆ—é•¿åº¦ï¼Œ$d$æ˜¯æ¨¡å‹ç»´åº¦

## äº”ã€åº”ç”¨ä¸å˜ä½“

### 1. ä¸»è¦åº”ç”¨

- **æœºå™¨ç¿»è¯‘**ï¼šåŸå§‹ Transformer çš„ä¸»è¦ä»»åŠ¡
- **è¯­è¨€å»ºæ¨¡**ï¼šGPT ç³»åˆ—
- **æ–‡æœ¬ç†è§£**ï¼šBERT ç³»åˆ—
- **å¤šæ¨¡æ€**ï¼šCLIPã€ViT ç­‰

### 2. é‡è¦å˜ä½“

- **BERT**ï¼šåªä½¿ç”¨ Encoderï¼ŒåŒå‘å»ºæ¨¡
- **GPT**ï¼šåªä½¿ç”¨ Decoderï¼Œè‡ªå›å½’ç”Ÿæˆ
- **T5**ï¼šText-to-Text ç»Ÿä¸€æ¡†æ¶

## å…­ã€æ€»ç»“

Transformer çš„æ ¸å¿ƒè´¡çŒ®ï¼š

1. **å®Œå…¨åŸºäºæ³¨æ„åŠ›**ï¼šæ‘’å¼ƒäº†å¾ªç¯å’Œå·ç§¯ç»“æ„
2. **å¹¶è¡ŒåŒ–è®­ç»ƒ**ï¼šå¤§å¹…æå‡è®­ç»ƒæ•ˆç‡
3. **é•¿è·ç¦»å»ºæ¨¡**ï¼šæœ‰æ•ˆæ•è·é•¿è·ç¦»ä¾èµ–å…³ç³»
4. **å¯æ‰©å±•æ€§å¼º**ï¼šä¸ºå¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹å¥ å®šåŸºç¡€

### Transformer vs ä¼ ç»Ÿæ¨¡å‹å¯¹æ¯”å›¾

```mermaid
graph LR
    subgraph RNN [" "]
        RNN_Title["ğŸ”„ ä¼ ç»ŸRNNæ¨¡å‹<br/>Sequential Processing<br/>é¡ºåºå¤„ç†æ¶æ„"]
        A1["æ—¶é—´æ­¥ t1<br/>Hidden State<br/>h1"] --> A2["æ—¶é—´æ­¥ t2<br/>Hidden State<br/>h2"]
        A2 --> A3["æ—¶é—´æ­¥ t3<br/>Hidden State<br/>h3"]
        A3 --> A4["æ—¶é—´æ­¥ t4<br/>Hidden State<br/>h4"]

        style RNN_Title fill:#1A237E,stroke:#0D47A1,stroke-width:3px,color:#fff
        style A1 fill:#3F51B5,stroke:#303F9F,stroke-width:3px,color:#fff
        style A2 fill:#3F51B5,stroke:#303F9F,stroke-width:3px,color:#fff
        style A3 fill:#3F51B5,stroke:#303F9F,stroke-width:3px,color:#fff
        style A4 fill:#3F51B5,stroke:#303F9F,stroke-width:3px,color:#fff
    end

    subgraph Trans [" "]
        Trans_Title["âš¡ Transformeræ¨¡å‹<br/>Parallel Processing<br/>å¹¶è¡Œå¤„ç†æ¶æ„"]
        B1["ä½ç½® 1<br/>Position 1<br/>å…¨è¿æ¥æ³¨æ„åŠ›"]
        B2["ä½ç½® 2<br/>Position 2<br/>å…¨è¿æ¥æ³¨æ„åŠ›"]
        B3["ä½ç½® 3<br/>Position 3<br/>å…¨è¿æ¥æ³¨æ„åŠ›"]
        B4["ä½ç½® 4<br/>Position 4<br/>å…¨è¿æ¥æ³¨æ„åŠ›"]

        B1 -.-> B2
        B1 -.-> B3
        B1 -.-> B4
        B2 -.-> B1
        B2 -.-> B3
        B2 -.-> B4
        B3 -.-> B1
        B3 -.-> B2
        B3 -.-> B4
        B4 -.-> B1
        B4 -.-> B2
        B4 -.-> B3

        style Trans_Title fill:#E65100,stroke:#BF360C,stroke-width:3px,color:#fff
        style B1 fill:#FF9800,stroke:#F57C00,stroke-width:3px,color:#fff
        style B2 fill:#4CAF50,stroke:#43A047,stroke-width:3px,color:#fff
        style B3 fill:#E91E63,stroke:#C2185B,stroke-width:3px,color:#fff
        style B4 fill:#9C27B0,stroke:#7B1FA2,stroke-width:3px,color:#fff
    end

    linkStyle 0 stroke:#2196F3,stroke-width:3px
    linkStyle 1 stroke:#2196F3,stroke-width:3px
    linkStyle 2 stroke:#2196F3,stroke-width:3px
```

---

Transformer ä¸ä»…é©å‘½æ€§åœ°æ”¹å˜äº† NLP é¢†åŸŸï¼Œä¹Ÿä¸ºè®¡ç®—æœºè§†è§‰ã€è¯­éŸ³å¤„ç†ç­‰é¢†åŸŸå¸¦æ¥äº†æ–°çš„æ€è·¯ï¼Œæ˜¯æ·±åº¦å­¦ä¹ å†å²ä¸Šçš„é‡è¦é‡Œç¨‹ç¢‘ã€‚
