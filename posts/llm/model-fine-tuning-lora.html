<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="author" content="ZhengKe"><meta http-equiv="X-UA-Compatible" content="chrome=1"><meta name="revisit-after" content="7 days"><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="icon" href="/favicon.svg" type="image/svg+xml"><script src="/lib/mermaid.min.js"></script><meta name="msapplication-TileColor" content="#ffffff"><meta name="theme-color" content="#ffffff"><title>LoRA：低秩适应技术</title><script>(()=>{var e=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,a=localStorage.getItem("vueuse-color-scheme")||"auto";("dark"===a||e&&"light"!==a)&&document.documentElement.classList.toggle("dark",!0)})()</script><script type="module" crossorigin="" src="/assets/app.95384a36.js"></script><style>*,:after,:before{box-sizing:border-box;border-width:0;border-style:solid;border-color:currentColor}html{line-height:1.5;-webkit-text-size-adjust:100%;-moz-tab-size:4;tab-size:4;font-family:ui-sans-serif,system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,"Apple Color Emoji","Segoe UI Emoji",Segoe UI Symbol,"Noto Color Emoji"}body{margin:0;line-height:inherit}h1,h2,h3,h4{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}strong{font-weight:bolder}code,pre{font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1em}table{text-indent:0;border-color:inherit;border-collapse:collapse}h1,h2,h3,h4,p,pre{margin:0}ol,ul{list-style:none;margin:0;padding:0}img,svg{display:block;vertical-align:middle}img{max-width:100%;height:auto}.prose mjx-container{margin:0 .2em!important;display:inline-block!important}.prose mjx-container[display=true]{display:block!important;margin:1.5em auto!important;text-align:center!important}@media (max-width:768px){.prose mjx-container[display=true]{font-size:.9em!important;margin:1.2em auto!important}}@media (max-width:480px){.prose mjx-container[display=true]{font-size:.8em!important;margin:1em auto!important}}.prose mjx-container{opacity:1!important;z-index:1!important}.prose mjx-assistive-mml{position:absolute!important;clip:rect(1px,1px,1px,1px)!important;padding:0!important;border:0!important;height:1px!important;width:1px!important;overflow:hidden!important}:root{--c-bg:#fff;--c-scrollbar:#eee;--c-scrollbar-hover:#bbb}html{background-color:var(--c-bg)}html{overflow:scroll}*{scrollbar-color:var(--c-scrollbar) var(--c-bg)}.prose{color:var(--fg);max-width:75ch;font-size:1rem;line-height:1.75}.prose a{color:var(--fg-deeper);text-decoration:none;font-weight:500}.prose strong{color:var(--fg-deep);font-weight:600}.prose ol>li{position:relative;padding-left:1.75em}.prose ol>li:before{content:counter(list-item,var(--list-counter-style,decimal)) ".";position:absolute;font-weight:400;color:#6b7280;left:0}.prose ul>li{position:relative;padding-left:1.75em}.prose ul>li:before{content:"";position:absolute;background-color:#d1d5db;border-radius:50%;width:.375em;height:.375em;top:.6875em;left:.25em}.prose h1{color:var(--fg-deeper);font-weight:800;font-size:2.25em;margin-top:0;margin-bottom:.8888889em;line-height:1.1111111}.prose h2{color:var(--fg-deep);font-weight:700;font-size:1.5em;margin-top:2em;margin-bottom:1em;line-height:1.3333333}.prose h3{color:inherit;font-weight:600;font-size:1.25em;margin-top:1.6em;margin-bottom:.6em;line-height:1.6}.prose h4{color:inherit;font-weight:600;margin-top:1.5em;margin-bottom:.5em;line-height:1.5}.prose code{color:var(--fg-deep);font-weight:600;font-size:.875em}.prose code:before{content:"`"}.prose code:after{content:"`"}.prose pre{color:#e5e7eb;background-color:#1f2937;overflow-x:auto;font-size:.875em;line-height:1.7142857;margin-top:1.7142857em;margin-bottom:1.7142857em;border-radius:.375rem;padding:.8571429em 1.1428571em}.prose pre code{background-color:transparent;border-width:0;border-radius:0;padding:0;font-weight:400;color:inherit;font-size:inherit;font-family:inherit;line-height:inherit}.prose pre code:before{content:none}.prose pre code:after{content:none}.prose table{width:100%;table-layout:auto;text-align:left;margin-top:2em;margin-bottom:2em;font-size:.875em;line-height:1.7142857}.prose thead{color:#111827;font-weight:600;border-bottom-width:1px;border-bottom-color:#d1d5db}.prose thead th{vertical-align:bottom;padding-right:.5714286em;padding-bottom:.5714286em;padding-left:.5714286em}.prose tbody tr{border-bottom-width:1px;border-bottom-color:#e5e7eb}.prose tbody tr:last-child{border-bottom-width:0}.prose tbody td{vertical-align:top;padding:.5714286em}.prose p{margin-top:1.25em;margin-bottom:1.25em}.prose ol,.prose ul{margin-top:1.25em;margin-bottom:1.25em;list-style-type:none}.prose li{margin-top:.5em;margin-bottom:.5em}.prose>ol>li>:first-child{margin-top:1.25em}.prose>ol>li>:last-child{margin-bottom:1.25em}.prose thead th:first-child{padding-left:0}.prose thead th:last-child{padding-right:0}.prose tbody td:first-child{padding-left:0}.prose tbody td:last-child{padding-right:0}.prose>:first-child{margin-top:0}.prose>:last-child{margin-bottom:0}:root{--prism-scheme:light;--prism-foreground:#6e6e6e;--prism-background:#f4f4f4;--prism-comment:#a8a8a8;--prism-string:#555555;--prism-literal:#333333;--prism-keyword:#000000;--prism-function:#4f4f4f;--prism-deleted:#333333;--prism-class:#333333;--prism-builtin:#757575;--prism-property:#333333;--prism-namespace:#4f4f4f;--prism-punctuation:#ababab;--prism-decorator:var(--prism-class);--prism-operator:var(--prism-punctuation);--prism-number:var(--prism-literal);--prism-boolean:var(--prism-literal);--prism-variable:var(--prism-literal);--prism-constant:var(--prism-literal);--prism-symbol:var(--prism-literal);--prism-interpolation:var(--prism-literal);--prism-selector:var(--prism-keyword);--prism-keyword-control:var(--prism-keyword);--prism-regex:var(--prism-string);--prism-json-property:var(--prism-property);--prism-inline-background:var(--prism-background);--prism-comment-style:italic;--prism-url-decoration:underline;--prism-line-number:#a5a5a5;--prism-line-number-gutter:#333333;--prism-line-highlight-background:#eeeeee;--prism-selection-background:#dddddd;--prism-marker-color:var(--prism-foreground);--prism-marker-opacity:.4;--prism-marker-font-size:.8em;--prism-font-size:1em;--prism-line-height:1.5em;--prism-font-family:monospace;--prism-inline-font-size:var(--prism-font-size);--prism-block-font-size:var(--prism-font-size);--prism-tab-size:2;--prism-block-padding-x:1em;--prism-block-padding-y:1em;--prism-block-margin-x:0;--prism-block-margin-y:.5em;--prism-block-radius:.3em;--prism-inline-padding-x:.3em;--prism-inline-padding-y:.1em;--prism-inline-radius:.3em}code[class*=language-],pre[class*=language-]{font-size:var(--prism-font-size);font-family:var(--prism-font-family);direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;line-height:var(--prism-line-height);-moz-tab-size:var(--prism-tab-size);-o-tab-size:var(--prism-tab-size);tab-size:var(--prism-tab-size);-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;color:var(--prism-foreground)!important}pre[class*=language-]{font-size:var(--prism-block-font-size);padding:var(--prism-block-padding-y) var(--prism-block-padding-x);margin:var(--prism-block-margin-y) var(--prism-block-margin-x);border-radius:var(--prism-block-radius);overflow:auto;background:var(--prism-background)}code[class*=language-] ::-moz-selection,code[class*=language-]::-moz-selection,pre[class*=language-] ::-moz-selection,pre[class*=language-]::-moz-selection{background:var(--prism-selection-background)}code[class*=language-] ::selection,code[class*=language-]::selection,pre[class*=language-] ::selection,pre[class*=language-]::selection{background:var(--prism-selection-background)}.token.comment{color:var(--prism-comment);font-style:var(--prism-comment-style)}.token.interpolation{color:var(--prism-interpolation)}.token.string{color:var(--prism-string)}.token.punctuation{color:var(--prism-punctuation)}.token.operator{color:var(--prism-operator)}.token.boolean{color:var(--prism-boolean)}.token.number{color:var(--prism-number)}.token.keyword{color:var(--prism-keyword)}.token.function{color:var(--prism-function)}.token.class-name{color:var(--prism-class)}.token.builtin{color:var(--prism-builtin)}.token.property{color:var(--prism-property)}:root{--prism-font-size:.9rem;--prism-font-family:"Fira Code",monospace}:root{--prism-font-family:"Input Mono",monospace}html:not(.dark){--prism-foreground:#393a34;--prism-background:#fbfbfb;--prism-comment:#a0ada0;--prism-string:#b56959;--prism-literal:#2f8a89;--prism-number:#296aa3;--prism-keyword:#1c6b48;--prism-function:#6c7834;--prism-boolean:#1c6b48;--prism-constant:#a65e2b;--prism-deleted:#a14f55;--prism-class:#2993a3;--prism-builtin:#ab5959;--prism-property:#b58451;--prism-namespace:#b05a78;--prism-punctuation:#8e8f8b;--prism-decorator:#bd8f8f;--prism-regex:#ab5e3f;--prism-json-property:#698c96}.prose{--fg:#555;--fg-deep:#222;--fg-deeper:#000;color:var(--fg)}.prose a{font-weight:inherit;text-decoration:none;border-bottom:1px solid rgba(125,125,125,.3);transition:border .3s ease-in-out}.prose a:hover{border-bottom:1px solid var(--fg)}a.header-anchor{float:left;margin-top:.125em;margin-left:-1.2em;padding-right:.5em;font-size:.85em;opacity:0;text-decoration:none;border:0!important}a.header-anchor:focus,a.header-anchor:hover{text-decoration:none}h2:focus .header-anchor,h2:hover .header-anchor,h3:focus .header-anchor,h3:hover .header-anchor,h4:focus .header-anchor,h4:hover .header-anchor{opacity:.5}.prose .MathJax{font-size:inherit!important;color:inherit!important}.prose .MathJax svg{max-width:100%!important;height:auto!important}.absolute{position:absolute}.z-40{z-index:40}.m-6{margin:1.5rem}.m-auto{margin:auto}.\!-mt-2{margin-top:-.5rem!important}.mb-0{margin-bottom:0}.mb-6{margin-bottom:1.5rem}.mb-8{margin-bottom:2rem}.mt-10{margin-top:2.5rem}.mt-8{margin-top:2rem}.h-10{height:2.5rem}.w-10{width:2.5rem}.flex{display:flex}.flex-auto{flex:1 1 auto}.select-none{user-select:none}.px-7{padding-left:1.75rem;padding-right:1.75rem}.py-10{padding-top:2.5rem;padding-bottom:2.5rem}.font-mono{font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace}.font-sans{font-family:Inter,Inter var,system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}.text-sm{font-size:.875rem;line-height:1.25rem}.text-gray-700{--un-text-opacity:1;color:rgba(55,65,81,var(--un-text-opacity))}.no-underline{text-decoration:none}.opacity-50{opacity:.5}.hover\:opacity-75:hover{opacity:.75}.outline-none{outline:2px solid transparent;outline-offset:2px}@media (max-width:768px){.lt-md\:hidden{display:none}}@media (min-width:768px){.md\:hidden{display:none}}@media (min-width:1024px){.lg\:fixed{position:fixed}}.nav[data-v-568133b8]{padding:2rem;width:100%;display:grid;grid-template-columns:auto max-content;box-sizing:border-box}.nav[data-v-568133b8]>*{margin:auto}.nav a[data-v-568133b8]{cursor:pointer;text-decoration:none;color:inherit;transition:opacity .2s ease;opacity:.6;outline:0}.nav a[data-v-568133b8]:hover{opacity:1;text-decoration-color:inherit}.nav .right[data-v-568133b8]{display:grid;grid-gap:1.2rem;grid-auto-flow:column}.nav .right[data-v-568133b8]>*{margin:auto}.mathjax-container[data-v-011ba505]{color:inherit;font-family:inherit;line-height:inherit}</style><link rel="preload" href="/assets/app.baf61519.css" as="style"><link rel="modulepreload" crossorigin="" href="/assets/Model-fine-tuning-LoRA.4f571136.js"><link rel="preload" href="/assets/Model-Format-GGUF.c4ecc021.css" as="style"><link rel="modulepreload" crossorigin="" href="/assets/Post.14d9d5ce.js"><link rel="preload" href="/assets/Post.63afffbd.css" as="style"><meta property="og:title" content="LoRA：低秩适应技术"><meta name="head:count" content="1"></head><body class="font-sans text-gray-700 dark:text-gray-200"><div id="app" data-server-rendered="true"><!--[--><header class="header z-40" data-v-568133b8=""><a href="/" class="w-10 h-10 absolute lg:fixed m-6 select-none outline-none" focusable="false" data-v-568133b8=""><img src="/logo-dark.svg" alt="logo" style="display:none" data-v-568133b8=""><img src="/logo.svg" alt="logo" data-v-568133b8=""></a><nav class="nav" data-v-568133b8=""><div class="spacer" data-v-568133b8=""></div><div class="right" data-v-568133b8=""><a href="/posts" class="" data-v-568133b8=""><span class="lt-md:hidden" data-v-568133b8="">Blog</span><svg style="vertical-align:sub" class="inline md:hidden" width="1.2em" height="1.2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24" data-v-568133b8=""><path fill="currentColor" d="M20 22H4a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1h16a1 1 0 0 1 1 1v18a1 1 0 0 1-1 1m-1-2V4H5v16zM7 6h4v4H7zm0 6h10v2H7zm0 4h10v2H7zm6-9h4v2h-4z"></path></svg></a><a href="/llm" class="lt-md:hidden" data-v-568133b8="">LLM </a><a href="/notes" class="" title="Notes" data-v-568133b8=""><span class="lt-md:hidden" data-v-568133b8="">Notes</span><svg style="vertical-align:sub" class="inline md:hidden" width="1.2em" height="1.2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24" data-v-568133b8=""><path fill="currentColor" d="m17.85 11.698l-.708-.707l-9.9 9.9H3v-4.243L14.314 5.334l5.657 5.657a1 1 0 0 1 0 1.414L12.9 19.477l-1.415-1.415zm-2.122-2.121l-1.414-1.414L5 17.477v1.414h1.414zm2.828-7.071l2.829 2.828a1 1 0 0 1 0 1.415L19.97 8.163L15.728 3.92l1.414-1.414a1 1 0 0 1 1.414 0"></path></svg></a><a href="https://github.com/ZhengKe0110" target="_blank" title="GitHub" class="lt-md:hidden" data-v-568133b8=""><svg style="vertical-align:sub" class="inline" width="1.2em" height="1.2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24" data-v-568133b8=""><path fill="currentColor" d="M10.07 20.503a1 1 0 0 0-1.18-.983c-1.31.24-2.963.276-3.402-.958a5.7 5.7 0 0 0-1.837-2.415a1 1 0 0 1-.167-.11a1 1 0 0 0-.93-.645h-.005a1 1 0 0 0-1 .995c-.004.815.81 1.338 1.141 1.514a4.4 4.4 0 0 1 .924 1.36c.365 1.023 1.423 2.576 4.466 2.376l.003.098l.004.268a1 1 0 0 0 2 0l-.005-.318c-.005-.19-.012-.464-.012-1.182M20.737 5.377q.049-.187.09-.42a6.3 6.3 0 0 0-.408-3.293a1 1 0 0 0-.615-.58c-.356-.12-1.67-.357-4.184 1.25a13.9 13.9 0 0 0-6.354 0C6.762.75 5.455.966 5.102 1.079a1 1 0 0 0-.631.584a6.3 6.3 0 0 0-.404 3.357q.037.191.079.354a6.27 6.27 0 0 0-1.256 3.83a8 8 0 0 0 .043.921c.334 4.603 3.334 5.984 5.424 6.459a5 5 0 0 0-.118.4a1 1 0 0 0 1.942.479a1.7 1.7 0 0 1 .468-.878a1 1 0 0 0-.546-1.745c-3.454-.395-4.954-1.802-5.18-4.899a7 7 0 0 1-.033-.738a4.26 4.26 0 0 1 .92-2.713a3 3 0 0 1 .195-.231a1 1 0 0 0 .188-1.025a3.4 3.4 0 0 1-.155-.555a4.1 4.1 0 0 1 .079-1.616a7.5 7.5 0 0 1 2.415 1.18a1 1 0 0 0 .827.133a11.8 11.8 0 0 1 6.173.001a1 1 0 0 0 .83-.138a7.6 7.6 0 0 1 2.406-1.19a4 4 0 0 1 .087 1.578a3.2 3.2 0 0 1-.169.607a1 1 0 0 0 .188 1.025c.078.087.155.18.224.268A4.12 4.12 0 0 1 20 9.203a7 7 0 0 1-.038.777c-.22 3.056-1.725 4.464-5.195 4.86a1 1 0 0 0-.546 1.746a1.63 1.63 0 0 1 .466.908a3 3 0 0 1 .093.82v2.333c-.01.648-.01 1.133-.01 1.356a1 1 0 1 0 2 0c0-.217 0-.692.01-1.34v-2.35a5 5 0 0 0-.155-1.311a4 4 0 0 0-.116-.416a6.51 6.51 0 0 0 5.445-6.424A9 9 0 0 0 22 9.203a6.13 6.13 0 0 0-1.263-3.826"></path></svg></a><a class="select-none" title="Toggle Color Scheme" data-v-568133b8=""><svg style="vertical-align:sub;display:none" class="inline" width="1.2em" height="1.2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="currentColor" d="M10 7a7 7 0 0 0 12 4.9v.1c0 5.523-4.477 10-10 10S2 17.523 2 12S6.477 2 12 2h.1A6.98 6.98 0 0 0 10 7m-6 5a8 8 0 0 0 15.062 3.762A9 9 0 0 1 8.238 4.938A8 8 0 0 0 4 12"></path></svg><svg style="vertical-align:sub" class="inline" width="1.2em" height="1.2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="currentColor" d="M12 18a6 6 0 1 1 0-12a6 6 0 0 1 0 12m0-2a4 4 0 1 0 0-8a4 4 0 0 0 0 8M11 1h2v3h-2zm0 19h2v3h-2zM3.515 4.929l1.414-1.414L7.05 5.636L5.636 7.05zM16.95 18.364l1.414-1.414l2.121 2.121l-1.414 1.414zm2.121-14.85l1.414 1.415l-2.121 2.121l-1.414-1.414zM5.636 16.95l1.414 1.414l-2.121 2.121l-1.414-1.414zM23 11v2h-3v-2zM4 11v2H1v-2z"></path></svg></a></div></nav></header><main class="px-7 py-10"><!--[--><div class="prose m-auto mb-8"><h1 class="mb-0">LoRA：低秩适应技术</h1><p class="opacity-50 !-mt-2">Aug 13<!----></p><!----></div><article><div class="mathjax-container" data-v-011ba505=""><!--[--><!--[--><div class="prose m-auto"><h2 id="概述" tabindex="-1">概述 <a class="header-anchor" href="#概述" aria-hidden="true">#</a></h2><p>LoRA (Low-Rank Adaptation) 是一种参数高效的微调技术，由微软研究团队在 2021 年提出。该技术通过学习低秩分解矩阵来适应预训练的大语言模型，同时保持原始权重冻结，极大地减少了可训练参数的数量和存储需求。</p><h3 id="核心思想" tabindex="-1">核心思想 <a class="header-anchor" href="#核心思想" aria-hidden="true">#</a></h3><p>LoRA 的核心思想是：<strong>模型微调过程中的权重更新具有内在的低秩特性</strong>。具体来说：</p><ul><li>将权重更新分解为两个低秩矩阵的乘积</li><li>只训练这些低秩矩阵，保持原始权重冻结</li><li>在推理时可以将低秩矩阵合并到原始权重中，无额外延迟</li></ul><pre class="language-mermaid"><code class="language-mermaid"><span class="token keyword">graph</span> LR
    A<span class="token text string">[原始权重 W₀]</span> <span class="token arrow operator">--&gt;</span> B<span class="token text string">[冻结状态]</span>
    C<span class="token text string">[低秩矩阵 A]</span> <span class="token arrow operator">--&gt;</span> D<span class="token text string">[可训练]</span>
    E<span class="token text string">[低秩矩阵 B]</span> <span class="token arrow operator">--&gt;</span> F<span class="token text string">[可训练]</span>
    G<span class="token text string">[输入 x]</span> <span class="token arrow operator">--&gt;</span> H<span class="token text string">[LoRA 层]</span>
    H <span class="token arrow operator">--&gt;</span> I<span class="token text string">[W₀x + BAx]</span>

    <span class="token keyword">style</span> B <span class="token style"><span class="token property">fill</span><span class="token operator">:</span>#f9f<span class="token punctuation">,</span><span class="token property">stroke</span><span class="token operator">:</span>#333<span class="token punctuation">,</span><span class="token property">stroke-width</span><span class="token operator">:</span>2px</span>
    <span class="token keyword">style</span> D <span class="token style"><span class="token property">fill</span><span class="token operator">:</span>#9f9<span class="token punctuation">,</span><span class="token property">stroke</span><span class="token operator">:</span>#333<span class="token punctuation">,</span><span class="token property">stroke-width</span><span class="token operator">:</span>2px</span>
    <span class="token keyword">style</span> F <span class="token style"><span class="token property">fill</span><span class="token operator">:</span>#9f9<span class="token punctuation">,</span><span class="token property">stroke</span><span class="token operator">:</span>#333<span class="token punctuation">,</span><span class="token property">stroke-width</span><span class="token operator">:</span>2px</span>
</code></pre><h3 id="技术优势" tabindex="-1">技术优势 <a class="header-anchor" href="#技术优势" aria-hidden="true">#</a></h3><p>根据原论文实验结果，LoRA 具有以下显著优势：</p><ol><li><strong>参数效率</strong>：可训练参数减少 10,000 倍</li><li><strong>内存效率</strong>：GPU 内存使用减少 3 倍</li><li><strong>存储效率</strong>：模型检查点大小减少到几 MB</li><li><strong>任务切换</strong>：支持高效的多任务部署</li><li><strong>推理性能</strong>：零延迟推理（权重合并后）</li></ol><h2 id="数学原理" tabindex="-1">数学原理 <a class="header-anchor" href="#数学原理" aria-hidden="true">#</a></h2><h3 id="低秩分解理论" tabindex="-1">低秩分解理论 <a class="header-anchor" href="#低秩分解理论" aria-hidden="true">#</a></h3><p>对于预训练权重矩阵<mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-.375ex" xmlns="http://www.w3.org/2000/svg" width="10.621ex" height="2.306ex" role="img" focusable="false" viewBox="0 -853.7 4694.3 1019.3" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3"></path></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" style="stroke-width:3"></path></g></g><g data-mml-node="mo" transform="translate(1658.3,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z" style="stroke-width:3"></path></g><g data-mml-node="msup" transform="translate(2603.1,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z" style="stroke-width:3"></path></g></g><g data-mml-node="TeXAtom" transform="translate(755,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width:3"></path></g><g data-mml-node="mo" transform="translate(520,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z" style="stroke-width:3"></path></g><g data-mml-node="mi" transform="translate(1298,0)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z" style="stroke-width:3"></path></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0;left:0;clip:rect(1px,1px,1px,1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0 0 0;border:0;display:block;width:auto;overflow:hidden"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>W</mi><mn>0</mn></msub><mo>∈</mo><msup><mrow data-mjx-texclass="ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow data-mjx-texclass="ORD"><mi>d</mi><mo>×</mo><mi>k</mi></mrow></msup></math></mjx-assistive-mml></mjx-container>，LoRA 通过以下方式进行适应：</p><mjx-container class="MathJax" jax="SVG" display="true" style="direction:ltr;display:block;text-align:center;margin:1em 0;position:relative"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-.375ex" xmlns="http://www.w3.org/2000/svg" width="31.962ex" height="1.994ex" role="img" focusable="false" viewBox="0 -716 14127.1 881.6" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z" style="stroke-width:3"></path></g><g data-mml-node="mo" transform="translate(853.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3"></path></g><g data-mml-node="msub" transform="translate(1909.6,0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3"></path></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" style="stroke-width:3"></path></g></g><g data-mml-node="mi" transform="translate(3290.1,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3"></path></g><g data-mml-node="mo" transform="translate(4084.3,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width:3"></path></g><g data-mml-node="mi" transform="translate(5084.6,0)"><path data-c="394" d="M51 0Q46 4 46 7Q46 9 215 357T388 709Q391 716 416 716Q439 716 444 709Q447 705 616 357T786 7Q786 4 781 0H51ZM507 344L384 596L137 92L383 91H630Q630 93 507 344Z" style="stroke-width:3"></path></g><g data-mml-node="mi" transform="translate(5917.6,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3"></path></g><g data-mml-node="mi" transform="translate(6965.6,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3"></path></g><g data-mml-node="mo" transform="translate(7815.3,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3"></path></g><g data-mml-node="msub" transform="translate(8871.1,0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3"></path></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" style="stroke-width:3"></path></g></g><g data-mml-node="mi" transform="translate(10251.7,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3"></path></g><g data-mml-node="mo" transform="translate(11045.9,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width:3"></path></g><g data-mml-node="mi" transform="translate(12046.1,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z" style="stroke-width:3"></path></g><g data-mml-node="mi" transform="translate(12805.1,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z" style="stroke-width:3"></path></g><g data-mml-node="mi" transform="translate(13555.1,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="block" style="top:0;left:0;clip:rect(1px,1px,1px,1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0 0 0;border:0;display:block;overflow:hidden;width:100%"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>h</mi><mo>=</mo><msub><mi>W</mi><mn>0</mn></msub><mi>x</mi><mo>+</mo><mi mathvariant="normal">Δ</mi><mi>W</mi><mi>x</mi><mo>=</mo><msub><mi>W</mi><mn>0</mn></msub><mi>x</mi><mo>+</mo><mi>B</mi><mi>A</mi><mi>x</mi></math></mjx-assistive-mml></mjx-container><p>其中：</p><ul><li><mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-.09ex" xmlns="http://www.w3.org/2000/svg" width="9.082ex" height="2.022ex" role="img" focusable="false" viewBox="0 -853.7 4014.3 893.7" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z" style="stroke-width:3"></path></g><g data-mml-node="mo" transform="translate(1027.8,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z" style="stroke-width:3"></path></g><g data-mml-node="msup" transform="translate(1972.6,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z" style="stroke-width:3"></path></g></g><g data-mml-node="TeXAtom" transform="translate(755,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3"></path></g><g data-mml-node="mo" transform="translate(451,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z" style="stroke-width:3"></path></g><g data-mml-node="mi" transform="translate(1229,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width:3"></path></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0;left:0;clip:rect(1px,1px,1px,1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0 0 0;border:0;display:block;width:auto;overflow:hidden"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi><mo>∈</mo><msup><mrow data-mjx-texclass="ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow data-mjx-texclass="ORD"><mi>r</mi><mo>×</mo><mi>d</mi></mrow></msup></math></mjx-assistive-mml></mjx-container>，<mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-.09ex" xmlns="http://www.w3.org/2000/svg" width="9.104ex" height="2.022ex" role="img" focusable="false" viewBox="0 -853.7 4024 893.7" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z" style="stroke-width:3"></path></g><g data-mml-node="mo" transform="translate(1036.8,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z" style="stroke-width:3"></path></g><g data-mml-node="msup" transform="translate(1981.6,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z" style="stroke-width:3"></path></g></g><g data-mml-node="TeXAtom" transform="translate(755,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z" style="stroke-width:3"></path></g><g data-mml-node="mo" transform="translate(521,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z" style="stroke-width:3"></path></g><g data-mml-node="mi" transform="translate(1299,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3"></path></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0;left:0;clip:rect(1px,1px,1px,1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0 0 0;border:0;display:block;width:auto;overflow:hidden"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>B</mi><mo>∈</mo><msup><mrow data-mjx-texclass="ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow data-mjx-texclass="ORD"><mi>k</mi><mo>×</mo><mi>r</mi></mrow></msup></math></mjx-assistive-mml></mjx-container></li><li><mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-.566ex" xmlns="http://www.w3.org/2000/svg" width="13.433ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 5937.2 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3"></path></g><g data-mml-node="mo" transform="translate(728.8,0)"><path data-c="226A" d="M639 -48Q639 -54 634 -60T619 -67H618Q612 -67 536 -26Q430 33 329 88Q61 235 59 239Q56 243 56 250T59 261Q62 266 336 415T615 567L619 568Q622 567 625 567Q639 562 639 548Q639 540 633 534Q632 532 374 391L117 250L374 109Q632 -32 633 -34Q639 -40 639 -48ZM944 -48Q944 -54 939 -60T924 -67H923Q917 -67 841 -26Q735 33 634 88Q366 235 364 239Q361 243 361 250T364 261Q367 266 641 415T920 567L924 568Q927 567 930 567Q944 562 944 548Q944 540 938 534Q937 532 679 391L422 250L679 109Q937 -32 938 -34Q944 -40 944 -48Z" style="stroke-width:3"></path></g><g data-mml-node="mo" transform="translate(2006.6,0)"><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" style="stroke-width:3"></path><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(833,0)" style="stroke-width:3"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1111,0)" style="stroke-width:3"></path></g><g data-mml-node="mo" transform="translate(3673.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3"></path></g><g data-mml-node="mi" transform="translate(4062.6,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width:3"></path></g><g data-mml-node="mo" transform="translate(4582.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3"></path></g><g data-mml-node="mi" transform="translate(5027.2,0)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z" style="stroke-width:3"></path></g><g data-mml-node="mo" transform="translate(5548.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0;left:0;clip:rect(1px,1px,1px,1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0 0 0;border:0;display:block;width:auto;overflow:hidden"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>r</mi><mo>≪</mo><mo data-mjx-texclass="OP" movablelimits="true">min</mo><mo stretchy="false">(</mo><mi>d</mi><mo>,</mo><mi>k</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>（<mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-.025ex" xmlns="http://www.w3.org/2000/svg" width="1.02ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 451 453" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0;left:0;clip:rect(1px,1px,1px,1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0 0 0;border:0;display:block;width:auto;overflow:hidden"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>r</mi></math></mjx-assistive-mml></mjx-container>是秩）</li><li><mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-.186ex" xmlns="http://www.w3.org/2000/svg" width="10.687ex" height="1.805ex" role="img" focusable="false" viewBox="0 -716 4723.6 798" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="394" d="M51 0Q46 4 46 7Q46 9 215 357T388 709Q391 716 416 716Q439 716 444 709Q447 705 616 357T786 7Q786 4 781 0H51ZM507 344L384 596L137 92L383 91H630Q630 93 507 344Z" style="stroke-width:3"></path></g><g data-mml-node="mi" transform="translate(833,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3"></path></g><g data-mml-node="mo" transform="translate(2158.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3"></path></g><g data-mml-node="mi" transform="translate(3214.6,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z" style="stroke-width:3"></path></g><g data-mml-node="mi" transform="translate(3973.6,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z" style="stroke-width:3"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0;left:0;clip:rect(1px,1px,1px,1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0 0 0;border:0;display:block;width:auto;overflow:hidden"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">Δ</mi><mi>W</mi><mo>=</mo><mi>B</mi><mi>A</mi></math></mjx-assistive-mml></mjx-container>是权重更新矩阵</li></ul><h3 id="初始化策略" tabindex="-1">初始化策略 <a class="header-anchor" href="#初始化策略" aria-hidden="true">#</a></h3><p>LoRA 采用特殊的初始化策略来确保训练开始时的稳定性：</p><ul><li><mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:0" xmlns="http://www.w3.org/2000/svg" width="1.697ex" height="1.62ex" role="img" focusable="false" viewBox="0 -716 750 716" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z" style="stroke-width:3"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0;left:0;clip:rect(1px,1px,1px,1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0 0 0;border:0;display:block;width:auto;overflow:hidden"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi></math></mjx-assistive-mml></mjx-container>使用随机高斯初始化</li><li><mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:0" xmlns="http://www.w3.org/2000/svg" width="1.717ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 759 683" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z" style="stroke-width:3"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0;left:0;clip:rect(1px,1px,1px,1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0 0 0;border:0;display:block;width:auto;overflow:hidden"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>B</mi></math></mjx-assistive-mml></mjx-container>初始化为零矩阵</li><li>这确保了训练开始时<mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-.186ex" xmlns="http://www.w3.org/2000/svg" width="14.835ex" height="1.805ex" role="img" focusable="false" viewBox="0 -716 6557.1 798" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="394" d="M51 0Q46 4 46 7Q46 9 215 357T388 709Q391 716 416 716Q439 716 444 709Q447 705 616 357T786 7Q786 4 781 0H51ZM507 344L384 596L137 92L383 91H630Q630 93 507 344Z" style="stroke-width:3"></path></g><g data-mml-node="mi" transform="translate(833,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3"></path></g><g data-mml-node="mo" transform="translate(2158.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3"></path></g><g data-mml-node="mi" transform="translate(3214.6,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z" style="stroke-width:3"></path></g><g data-mml-node="mi" transform="translate(3973.6,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z" style="stroke-width:3"></path></g><g data-mml-node="mo" transform="translate(5001.3,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3"></path></g><g data-mml-node="mn" transform="translate(6057.1,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" style="stroke-width:3"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0;left:0;clip:rect(1px,1px,1px,1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0 0 0;border:0;display:block;width:auto;overflow:hidden"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">Δ</mi><mi>W</mi><mo>=</mo><mi>B</mi><mi>A</mi><mo>=</mo><mn>0</mn></math></mjx-assistive-mml></mjx-container></li></ul><h3 id="缩放因子" tabindex="-1">缩放因子 <a class="header-anchor" href="#缩放因子" aria-hidden="true">#</a></h3><p>为了控制适应的强度，LoRA 引入缩放因子：</p><mjx-container class="MathJax" jax="SVG" display="true" style="direction:ltr;display:block;text-align:center;margin:1em 0;position:relative"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-1.577ex" xmlns="http://www.w3.org/2000/svg" width="18.655ex" height="4.106ex" role="img" focusable="false" viewBox="0 -1118 8245.6 1815" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z" style="stroke-width:3"></path></g><g data-mml-node="mo" transform="translate(853.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3"></path></g><g data-mml-node="msub" transform="translate(1909.6,0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3"></path></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" style="stroke-width:3"></path></g></g><g data-mml-node="mi" transform="translate(3290.1,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3"></path></g><g data-mml-node="mo" transform="translate(4084.3,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width:3"></path></g><g data-mml-node="mfrac" transform="translate(5084.6,0)"><g data-mml-node="mi" transform="translate(220,676)"><path data-c="1D6FC" d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z" style="stroke-width:3"></path></g><g data-mml-node="mi" transform="translate(314.5,-686)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3"></path></g><rect width="840" height="60" x="120" y="220"></rect></g><g data-mml-node="mi" transform="translate(6164.6,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z" style="stroke-width:3"></path></g><g data-mml-node="mi" transform="translate(6923.6,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z" style="stroke-width:3"></path></g><g data-mml-node="mi" transform="translate(7673.6,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="block" style="top:0;left:0;clip:rect(1px,1px,1px,1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0 0 0;border:0;display:block;overflow:hidden;width:100%"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>h</mi><mo>=</mo><msub><mi>W</mi><mn>0</mn></msub><mi>x</mi><mo>+</mo><mfrac><mi>α</mi><mi>r</mi></mfrac><mi>B</mi><mi>A</mi><mi>x</mi></math></mjx-assistive-mml></mjx-container><p>其中<mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-.025ex" xmlns="http://www.w3.org/2000/svg" width="1.448ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 640 453" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FC" d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z" style="stroke-width:3"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0;left:0;clip:rect(1px,1px,1px,1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0 0 0;border:0;display:block;width:auto;overflow:hidden"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>α</mi></math></mjx-assistive-mml></mjx-container>是可调节的超参数，通常设置为<mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-.025ex" xmlns="http://www.w3.org/2000/svg" width="1.02ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 451 453" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0;left:0;clip:rect(1px,1px,1px,1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0 0 0;border:0;display:block;width:auto;overflow:hidden"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>r</mi></math></mjx-assistive-mml></mjx-container>的整数倍。</p><h2 id="实现细节" tabindex="-1">实现细节 <a class="header-anchor" href="#实现细节" aria-hidden="true">#</a></h2><h3 id="基础-lora-层实现" tabindex="-1">基础 LoRA 层实现 <a class="header-anchor" href="#基础-lora-层实现" aria-hidden="true">#</a></h3><h4 id="lora-基础接口" tabindex="-1">LoRA 基础接口 <a class="header-anchor" href="#lora-基础接口" aria-hidden="true">#</a></h4><pre class="language-python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
<span class="token keyword">import</span> math

<span class="token keyword">class</span> <span class="token class-name">LoRALayer</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""LoRA 基础层接口"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        r<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>
        lora_alpha<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>
        lora_dropout<span class="token punctuation">:</span> <span class="token builtin">float</span><span class="token punctuation">,</span>
        merge_weights<span class="token punctuation">:</span> <span class="token builtin">bool</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>r <span class="token operator">=</span> r
        self<span class="token punctuation">.</span>lora_alpha <span class="token operator">=</span> lora_alpha
        <span class="token comment"># 可选的 dropout 用于训练正则化</span>
        <span class="token keyword">if</span> lora_dropout <span class="token operator">&gt;</span> <span class="token number">0.0</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>lora_dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span>lora_dropout<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>lora_dropout <span class="token operator">=</span> <span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x

        <span class="token comment"># 标记是否合并权重</span>
        self<span class="token punctuation">.</span>merged <span class="token operator">=</span> <span class="token boolean">False</span>
        self<span class="token punctuation">.</span>merge_weights <span class="token operator">=</span> merge_weights
</code></pre><h4 id="lora-线性层初始化" tabindex="-1">LoRA 线性层初始化 <a class="header-anchor" href="#lora-线性层初始化" aria-hidden="true">#</a></h4><pre class="language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Linear</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">,</span> LoRALayer<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""LoRA 线性层实现"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        in_features<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>
        out_features<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>
        r<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span>
        lora_alpha<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>
        lora_dropout<span class="token punctuation">:</span> <span class="token builtin">float</span> <span class="token operator">=</span> <span class="token number">0.0</span><span class="token punctuation">,</span>
        fan_in_fan_out<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
        merge_weights<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span>
        <span class="token operator">**</span>kwargs
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        nn<span class="token punctuation">.</span>Module<span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>self<span class="token punctuation">)</span>
        LoRALayer<span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>self<span class="token punctuation">,</span> r<span class="token operator">=</span>r<span class="token punctuation">,</span> lora_alpha<span class="token operator">=</span>lora_alpha<span class="token punctuation">,</span>
                          lora_dropout<span class="token operator">=</span>lora_dropout<span class="token punctuation">,</span>
                          merge_weights<span class="token operator">=</span>merge_weights<span class="token punctuation">)</span>

        <span class="token comment"># 原始线性层配置</span>
        self<span class="token punctuation">.</span>in_features <span class="token operator">=</span> in_features
        self<span class="token punctuation">.</span>out_features <span class="token operator">=</span> out_features
        self<span class="token punctuation">.</span>fan_in_fan_out <span class="token operator">=</span> fan_in_fan_out

        <span class="token comment"># 原始权重（冻结）</span>
        self<span class="token punctuation">.</span>weight <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>out_features<span class="token punctuation">,</span> in_features<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token string">'bias'</span> <span class="token keyword">in</span> kwargs<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>bias <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>out_features<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>register_parameter<span class="token punctuation">(</span><span class="token string">'bias'</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>

        <span class="token comment"># LoRA 权重初始化</span>
        <span class="token keyword">if</span> r <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>lora_A <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>r<span class="token punctuation">,</span> in_features<span class="token punctuation">)</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>lora_B <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>out_features<span class="token punctuation">,</span> r<span class="token punctuation">)</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>scaling <span class="token operator">=</span> self<span class="token punctuation">.</span>lora_alpha <span class="token operator">/</span> self<span class="token punctuation">.</span>r

            <span class="token comment"># 初始化策略</span>
            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>kaiming_uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>lora_A<span class="token punctuation">,</span> a<span class="token operator">=</span>math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>zeros_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>lora_B<span class="token punctuation">)</span>

        <span class="token comment"># 冻结原始权重</span>
        self<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">False</span>
</code></pre><h4 id="训练-评估模式切换" tabindex="-1">训练/评估模式切换 <a class="header-anchor" href="#训练-评估模式切换" aria-hidden="true">#</a></h4><pre class="language-python"><code class="language-python">    <span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> mode<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""训练模式切换"""</span>
        nn<span class="token punctuation">.</span>Module<span class="token punctuation">.</span>train<span class="token punctuation">(</span>self<span class="token punctuation">,</span> mode<span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>merge_weights <span class="token keyword">and</span> self<span class="token punctuation">.</span>merged<span class="token punctuation">:</span>
            <span class="token comment"># 训练时解除权重合并</span>
            <span class="token keyword">if</span> self<span class="token punctuation">.</span>r <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
                self<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data <span class="token operator">-=</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>lora_B @ self<span class="token punctuation">.</span>lora_A<span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>scaling
            self<span class="token punctuation">.</span>merged <span class="token operator">=</span> <span class="token boolean">False</span>

    <span class="token keyword">def</span> <span class="token function">eval</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""评估模式"""</span>
        nn<span class="token punctuation">.</span>Module<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>merge_weights <span class="token keyword">and</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>merged<span class="token punctuation">:</span>
            <span class="token comment"># 评估时合并权重</span>
            <span class="token keyword">if</span> self<span class="token punctuation">.</span>r <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
                self<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data <span class="token operator">+=</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>lora_B @ self<span class="token punctuation">.</span>lora_A<span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>scaling
            self<span class="token punctuation">.</span>merged <span class="token operator">=</span> <span class="token boolean">True</span>
</code></pre><h4 id="前向传播实现" tabindex="-1">前向传播实现 <a class="header-anchor" href="#前向传播实现" aria-hidden="true">#</a></h4><pre class="language-python"><code class="language-python">    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""前向传播"""</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>r <span class="token operator">&gt;</span> <span class="token number">0</span> <span class="token keyword">and</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>merged<span class="token punctuation">:</span>
            <span class="token comment"># 未合并时的计算</span>
            result <span class="token operator">=</span> F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> bias<span class="token operator">=</span>self<span class="token punctuation">.</span>bias<span class="token punctuation">)</span>
            <span class="token keyword">if</span> self<span class="token punctuation">.</span>r <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
                <span class="token comment"># 添加 LoRA 适应</span>
                lora_result <span class="token operator">=</span> self<span class="token punctuation">.</span>lora_dropout<span class="token punctuation">(</span>x<span class="token punctuation">)</span> @ self<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>T @ self<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>T
                result <span class="token operator">+=</span> lora_result <span class="token operator">*</span> self<span class="token punctuation">.</span>scaling
            <span class="token keyword">return</span> result
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token comment"># 已合并或无 LoRA 时的计算</span>
            <span class="token keyword">return</span> F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> bias<span class="token operator">=</span>self<span class="token punctuation">.</span>bias<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">reset_parameters</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""重置参数"""</span>
        <span class="token keyword">if</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token string">'lora_A'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>kaiming_uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>lora_A<span class="token punctuation">,</span> a<span class="token operator">=</span>math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>zeros_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>lora_B<span class="token punctuation">)</span>
</code></pre><h3 id="模型适配示例" tabindex="-1">模型适配示例 <a class="header-anchor" href="#模型适配示例" aria-hidden="true">#</a></h3><h4 id="原始-transformer-块" tabindex="-1">原始 Transformer 块 <a class="header-anchor" href="#原始-transformer-块" aria-hidden="true">#</a></h4><pre class="language-python"><code class="language-python"><span class="token keyword">import</span> loralib <span class="token keyword">as</span> lora

<span class="token comment"># 原始模型定义</span>
<span class="token keyword">class</span> <span class="token class-name">TransformerBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> n_heads<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>attention <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> n_heads<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>feed_forward <span class="token operator">=</span> FeedForward<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 注意力机制</span>
        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>attention<span class="token punctuation">(</span>self<span class="token punctuation">.</span>norm1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 前馈网络</span>
        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>feed_forward<span class="token punctuation">(</span>self<span class="token punctuation">.</span>norm2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> x
</code></pre><h4 id="lora-适配的-transformer" tabindex="-1">LoRA 适配的 Transformer <a class="header-anchor" href="#lora-适配的-transformer" aria-hidden="true">#</a></h4><pre class="language-python"><code class="language-python"><span class="token comment"># 使用 LoRA 适配</span>
<span class="token keyword">class</span> <span class="token class-name">LoRATransformerBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> n_heads<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> lora_r<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token comment"># 用 LoRA 层替换注意力机制中的线性层</span>
        self<span class="token punctuation">.</span>query_proj <span class="token operator">=</span> lora<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> r<span class="token operator">=</span>lora_r<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>key_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>  <span class="token comment"># 可选择不适配</span>
        self<span class="token punctuation">.</span>value_proj <span class="token operator">=</span> lora<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> r<span class="token operator">=</span>lora_r<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>out_proj <span class="token operator">=</span> lora<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> r<span class="token operator">=</span>lora_r<span class="token punctuation">)</span>

        <span class="token comment"># 前馈网络</span>
        self<span class="token punctuation">.</span>ff1 <span class="token operator">=</span> lora<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> <span class="token number">4</span> <span class="token operator">*</span> d_model<span class="token punctuation">,</span> r<span class="token operator">=</span>lora_r<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ff2 <span class="token operator">=</span> lora<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4</span> <span class="token operator">*</span> d_model<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> r<span class="token operator">=</span>lora_r<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>norm1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>
</code></pre><h4 id="transformer-前向传播" tabindex="-1">Transformer 前向传播 <a class="header-anchor" href="#transformer-前向传播" aria-hidden="true">#</a></h4><pre class="language-python"><code class="language-python">    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 多头注意力</span>
        q <span class="token operator">=</span> self<span class="token punctuation">.</span>query_proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        k <span class="token operator">=</span> self<span class="token punctuation">.</span>key_proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        v <span class="token operator">=</span> self<span class="token punctuation">.</span>value_proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token comment"># 简化的注意力计算</span>
        attn_out <span class="token operator">=</span> self<span class="token punctuation">.</span>scaled_dot_product_attention<span class="token punctuation">(</span>q<span class="token punctuation">,</span> k<span class="token punctuation">,</span> v<span class="token punctuation">)</span>
        attn_out <span class="token operator">=</span> self<span class="token punctuation">.</span>out_proj<span class="token punctuation">(</span>attn_out<span class="token punctuation">)</span>
        x <span class="token operator">=</span> x <span class="token operator">+</span> attn_out
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token comment"># 前馈网络</span>
        ff_out <span class="token operator">=</span> self<span class="token punctuation">.</span>ff2<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>ff1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> x <span class="token operator">+</span> ff_out
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token keyword">return</span> x
</code></pre><h3 id="训练设置" tabindex="-1">训练设置 <a class="header-anchor" href="#训练设置" aria-hidden="true">#</a></h3><h4 id="训练配置函数" tabindex="-1">训练配置函数 <a class="header-anchor" href="#训练配置函数" aria-hidden="true">#</a></h4><pre class="language-python"><code class="language-python"><span class="token keyword">import</span> loralib <span class="token keyword">as</span> lora

<span class="token keyword">def</span> <span class="token function">setup_lora_training</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> learning_rate<span class="token operator">=</span><span class="token number">1e-4</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""设置 LoRA 训练配置"""</span>

    <span class="token comment"># 1. 标记只有 LoRA 参数可训练</span>
    lora<span class="token punctuation">.</span>mark_only_lora_as_trainable<span class="token punctuation">(</span>model<span class="token punctuation">)</span>

    <span class="token comment"># 2. 打印可训练参数统计</span>
    trainable_params <span class="token operator">=</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>p<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> p <span class="token keyword">in</span> model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> p<span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span>
    total_params <span class="token operator">=</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>p<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> p <span class="token keyword">in</span> model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"可训练参数: </span><span class="token interpolation"><span class="token punctuation">{</span>trainable_params<span class="token punctuation">:</span><span class="token format-spec">,</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"总参数: </span><span class="token interpolation"><span class="token punctuation">{</span>total_params<span class="token punctuation">:</span><span class="token format-spec">,</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"可训练参数比例: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token number">100</span> <span class="token operator">*</span> trainable_params <span class="token operator">/</span> total_params<span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">%"</span></span><span class="token punctuation">)</span>

    <span class="token comment"># 3. 设置优化器（只优化 LoRA 参数）</span>
    optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>AdamW<span class="token punctuation">(</span>
        <span class="token punctuation">[</span>p <span class="token keyword">for</span> p <span class="token keyword">in</span> model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> p<span class="token punctuation">.</span>requires_grad<span class="token punctuation">]</span><span class="token punctuation">,</span>
        lr<span class="token operator">=</span>learning_rate<span class="token punctuation">,</span>
        weight_decay<span class="token operator">=</span><span class="token number">0.01</span>
    <span class="token punctuation">)</span>

    <span class="token keyword">return</span> optimizer
</code></pre><h4 id="检查点管理" tabindex="-1">检查点管理 <a class="header-anchor" href="#检查点管理" aria-hidden="true">#</a></h4><pre class="language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">save_lora_checkpoint</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> path<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""保存 LoRA 检查点"""</span>
    <span class="token comment"># 只保存 LoRA 参数</span>
    lora_state_dict <span class="token operator">=</span> lora<span class="token punctuation">.</span>lora_state_dict<span class="token punctuation">(</span>model<span class="token punctuation">)</span>
    torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>lora_state_dict<span class="token punctuation">,</span> path<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"LoRA 检查点已保存到: </span><span class="token interpolation"><span class="token punctuation">{</span>path<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"文件大小: </span><span class="token interpolation"><span class="token punctuation">{</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>getsize<span class="token punctuation">(</span>path<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">1024</span> <span class="token operator">/</span> <span class="token number">1024</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string"> MB"</span></span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">load_lora_checkpoint</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> pretrained_path<span class="token punctuation">,</span> lora_path<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""加载预训练模型和 LoRA 检查点"""</span>
    <span class="token comment"># 1. 加载预训练权重</span>
    model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>pretrained_path<span class="token punctuation">)</span><span class="token punctuation">,</span> strict<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"已加载预训练权重"</span><span class="token punctuation">)</span>

    <span class="token comment"># 2. 加载 LoRA 权重</span>
    model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>lora_path<span class="token punctuation">)</span><span class="token punctuation">,</span> strict<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"已加载 LoRA 权重"</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> model
</code></pre><h2 id="性能分析与对比" tabindex="-1">性能分析与对比 <a class="header-anchor" href="#性能分析与对比" aria-hidden="true">#</a></h2><h3 id="参数效率对比" tabindex="-1">参数效率对比 <a class="header-anchor" href="#参数效率对比" aria-hidden="true">#</a></h3><p>根据原论文和 Microsoft LoRA 实现的实验结果：</p><table><thead><tr><th>模型</th><th>方法</th><th>可训练参数</th><th>存储需求</th><th>GLUE 平均分</th></tr></thead><tbody><tr><td>RoBERTa-Base</td><td>全量微调</td><td>125M</td><td>~500MB</td><td>87.6</td></tr><tr><td>RoBERTa-Base</td><td>LoRA</td><td>0.8M</td><td>~3.2MB</td><td>87.5±0.3</td></tr><tr><td>DeBERTa-XXL</td><td>全量微调</td><td>1.5B</td><td>~6GB</td><td>91.7</td></tr><tr><td>DeBERTa-XXL</td><td>LoRA</td><td>4.7M</td><td>~18.8MB</td><td>91.9±0.1</td></tr></tbody></table><h3 id="内存使用对比" tabindex="-1">内存使用对比 <a class="header-anchor" href="#内存使用对比" aria-hidden="true">#</a></h3><pre class="language-mermaid"><code class="language-mermaid"><span class="token keyword">graph</span> LR
    A<span class="token text string">[全量微调]</span> <span class="token arrow operator">--&gt;</span> B<span class="token text string">[高内存占用]</span>
    C<span class="token text string">[LoRA]</span> <span class="token arrow operator">--&gt;</span> D<span class="token text string">[低内存占用]</span>

    B <span class="token arrow operator">--&gt;</span> E<span class="token text string">[前向传播&lt;br/&gt;反向传播&lt;br/&gt;优化器状态&lt;br/&gt;梯度存储]</span>

    D <span class="token arrow operator">--&gt;</span> F<span class="token text string">[前向传播&lt;br/&gt;少量梯度&lt;br/&gt;小优化器状态]</span>

    <span class="token keyword">style</span> B <span class="token style"><span class="token property">fill</span><span class="token operator">:</span>#ffcccc</span>
    <span class="token keyword">style</span> D <span class="token style"><span class="token property">fill</span><span class="token operator">:</span>#ccffcc</span>
</code></pre><h3 id="推理延迟分析" tabindex="-1">推理延迟分析 <a class="header-anchor" href="#推理延迟分析" aria-hidden="true">#</a></h3><h4 id="基准测试函数" tabindex="-1">基准测试函数 <a class="header-anchor" href="#基准测试函数" aria-hidden="true">#</a></h4><pre class="language-python"><code class="language-python"><span class="token keyword">import</span> time
<span class="token keyword">import</span> torch

<span class="token keyword">def</span> <span class="token function">benchmark_inference</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> input_data<span class="token punctuation">,</span> num_runs<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""推理性能基准测试"""</span>
    model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># 预热</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            _ <span class="token operator">=</span> model<span class="token punctuation">(</span>input_data<span class="token punctuation">)</span>

    <span class="token comment"># 正式测试</span>
    torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>synchronize<span class="token punctuation">(</span><span class="token punctuation">)</span>
    start_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_runs<span class="token punctuation">)</span><span class="token punctuation">:</span>
            output <span class="token operator">=</span> model<span class="token punctuation">(</span>input_data<span class="token punctuation">)</span>

    torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>synchronize<span class="token punctuation">(</span><span class="token punctuation">)</span>
    end_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>

    avg_time <span class="token operator">=</span> <span class="token punctuation">(</span>end_time <span class="token operator">-</span> start_time<span class="token punctuation">)</span> <span class="token operator">/</span> num_runs <span class="token operator">*</span> <span class="token number">1000</span>  <span class="token comment"># ms</span>
    <span class="token keyword">return</span> avg_time
</code></pre><h4 id="lora-性能对比" tabindex="-1">LoRA 性能对比 <a class="header-anchor" href="#lora-性能对比" aria-hidden="true">#</a></h4><pre class="language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">compare_lora_performance</span><span class="token punctuation">(</span>lora_model<span class="token punctuation">,</span> test_input<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""比较 LoRA 合并前后的性能"""</span>

    <span class="token comment"># 未合并时的性能</span>
    lora_model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 确保未合并状态</span>
    unmerged_time <span class="token operator">=</span> benchmark_inference<span class="token punctuation">(</span>lora_model<span class="token punctuation">,</span> test_input<span class="token punctuation">)</span>

    <span class="token comment"># 合并后的性能</span>
    lora_model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 触发权重合并</span>
    merged_time <span class="token operator">=</span> benchmark_inference<span class="token punctuation">(</span>lora_model<span class="token punctuation">,</span> test_input<span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"未合并延迟: </span><span class="token interpolation"><span class="token punctuation">{</span>unmerged_time<span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string"> ms"</span></span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"合并后延迟: </span><span class="token interpolation"><span class="token punctuation">{</span>merged_time<span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string"> ms"</span></span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"性能提升: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token punctuation">(</span>unmerged_time <span class="token operator">-</span> merged_time<span class="token punctuation">)</span> <span class="token operator">/</span> unmerged_time <span class="token operator">*</span> <span class="token number">100</span><span class="token punctuation">:</span><span class="token format-spec">.1f</span><span class="token punctuation">}</span></span><span class="token string">%"</span></span><span class="token punctuation">)</span>
</code></pre><h2 id="实际应用案例" tabindex="-1">实际应用案例 <a class="header-anchor" href="#实际应用案例" aria-hidden="true">#</a></h2><h3 id="文本分类任务" tabindex="-1">文本分类任务 <a class="header-anchor" href="#文本分类任务" aria-hidden="true">#</a></h3><h4 id="创建-lora-分类器" tabindex="-1">创建 LoRA 分类器 <a class="header-anchor" href="#创建-lora-分类器" aria-hidden="true">#</a></h4><pre class="language-python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> AutoModelForSequenceClassification
<span class="token keyword">import</span> loralib <span class="token keyword">as</span> lora

<span class="token keyword">def</span> <span class="token function">create_lora_classifier</span><span class="token punctuation">(</span>model_name<span class="token punctuation">,</span> num_classes<span class="token punctuation">,</span> lora_r<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""创建 LoRA 文本分类器"""</span>

    <span class="token comment"># 加载预训练模型</span>
    model <span class="token operator">=</span> AutoModelForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
        model_name<span class="token punctuation">,</span>
        num_labels<span class="token operator">=</span>num_classes
    <span class="token punctuation">)</span>

    <span class="token comment"># 替换分类头为 LoRA 层</span>
    model<span class="token punctuation">.</span>classifier <span class="token operator">=</span> lora<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>
        model<span class="token punctuation">.</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span>
        num_classes<span class="token punctuation">,</span>
        r<span class="token operator">=</span>lora_r
    <span class="token punctuation">)</span>

    <span class="token comment"># 适配注意力层</span>
    <span class="token keyword">for</span> layer <span class="token keyword">in</span> model<span class="token punctuation">.</span>bert<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>layer<span class="token punctuation">:</span>
        <span class="token comment"># 替换查询和值投影</span>
        layer<span class="token punctuation">.</span>attention<span class="token punctuation">.</span>self<span class="token punctuation">.</span>query <span class="token operator">=</span> lora<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>
            model<span class="token punctuation">.</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span>
            model<span class="token punctuation">.</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span>
            r<span class="token operator">=</span>lora_r
        <span class="token punctuation">)</span>
        layer<span class="token punctuation">.</span>attention<span class="token punctuation">.</span>self<span class="token punctuation">.</span>value <span class="token operator">=</span> lora<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>
            model<span class="token punctuation">.</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span>
            model<span class="token punctuation">.</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span>
            r<span class="token operator">=</span>lora_r
        <span class="token punctuation">)</span>

    <span class="token keyword">return</span> model
</code></pre><h4 id="训练循环实现" tabindex="-1">训练循环实现 <a class="header-anchor" href="#训练循环实现" aria-hidden="true">#</a></h4><pre class="language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train_lora_classifier</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""训练 LoRA 分类器"""</span>
    model <span class="token operator">=</span> create_lora_classifier<span class="token punctuation">(</span><span class="token string">"bert-base-uncased"</span><span class="token punctuation">,</span> num_classes<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>

    <span class="token comment"># 设置 LoRA 训练</span>
    lora<span class="token punctuation">.</span>mark_only_lora_as_trainable<span class="token punctuation">(</span>model<span class="token punctuation">)</span>

    <span class="token comment"># 优化器</span>
    optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>AdamW<span class="token punctuation">(</span>
        <span class="token punctuation">[</span>p <span class="token keyword">for</span> p <span class="token keyword">in</span> model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> p<span class="token punctuation">.</span>requires_grad<span class="token punctuation">]</span><span class="token punctuation">,</span>
        lr<span class="token operator">=</span><span class="token number">1e-4</span>
    <span class="token punctuation">)</span>

    <span class="token comment"># 训练循环</span>
    model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> batch <span class="token keyword">in</span> dataloader<span class="token punctuation">:</span>
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>

        outputs <span class="token operator">=</span> model<span class="token punctuation">(</span>
            input_ids<span class="token operator">=</span>batch<span class="token punctuation">[</span><span class="token string">'input_ids'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
            attention_mask<span class="token operator">=</span>batch<span class="token punctuation">[</span><span class="token string">'attention_mask'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
            labels<span class="token operator">=</span>batch<span class="token punctuation">[</span><span class="token string">'labels'</span><span class="token punctuation">]</span>
        <span class="token punctuation">)</span>

        loss <span class="token operator">=</span> outputs<span class="token punctuation">.</span>loss
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><h3 id="对话生成任务" tabindex="-1">对话生成任务 <a class="header-anchor" href="#对话生成任务" aria-hidden="true">#</a></h3><h4 id="创建-lora-对话机器人" tabindex="-1">创建 LoRA 对话机器人 <a class="header-anchor" href="#创建-lora-对话机器人" aria-hidden="true">#</a></h4><pre class="language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">create_lora_chatbot</span><span class="token punctuation">(</span>base_model_path<span class="token punctuation">,</span> lora_r<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""创建 LoRA 对话机器人"""</span>

    <span class="token keyword">from</span> transformers <span class="token keyword">import</span> GPT2LMHeadModel<span class="token punctuation">,</span> GPT2Tokenizer

    <span class="token comment"># 加载基础模型</span>
    model <span class="token operator">=</span> GPT2LMHeadModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>base_model_path<span class="token punctuation">)</span>
    tokenizer <span class="token operator">=</span> GPT2Tokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>base_model_path<span class="token punctuation">)</span>

    <span class="token comment"># 应用 LoRA 到 Transformer 块</span>
    <span class="token keyword">for</span> block <span class="token keyword">in</span> model<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>h<span class="token punctuation">:</span>
        <span class="token comment"># 注意力层</span>
        block<span class="token punctuation">.</span>attn<span class="token punctuation">.</span>c_attn <span class="token operator">=</span> lora<span class="token punctuation">.</span>MergedLinear<span class="token punctuation">(</span>
            model<span class="token punctuation">.</span>config<span class="token punctuation">.</span>n_embd<span class="token punctuation">,</span>
            <span class="token number">3</span> <span class="token operator">*</span> model<span class="token punctuation">.</span>config<span class="token punctuation">.</span>n_embd<span class="token punctuation">,</span>  <span class="token comment"># q, k, v</span>
            r<span class="token operator">=</span>lora_r<span class="token punctuation">,</span>
            enable_lora<span class="token operator">=</span><span class="token punctuation">[</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">]</span>  <span class="token comment"># 只适配 q 和 v</span>
        <span class="token punctuation">)</span>

        <span class="token comment"># 前馈网络</span>
        block<span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>c_fc <span class="token operator">=</span> lora<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>
            model<span class="token punctuation">.</span>config<span class="token punctuation">.</span>n_embd<span class="token punctuation">,</span>
            <span class="token number">4</span> <span class="token operator">*</span> model<span class="token punctuation">.</span>config<span class="token punctuation">.</span>n_embd<span class="token punctuation">,</span>
            r<span class="token operator">=</span>lora_r
        <span class="token punctuation">)</span>
        block<span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>c_proj <span class="token operator">=</span> lora<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>
            <span class="token number">4</span> <span class="token operator">*</span> model<span class="token punctuation">.</span>config<span class="token punctuation">.</span>n_embd<span class="token punctuation">,</span>
            model<span class="token punctuation">.</span>config<span class="token punctuation">.</span>n_embd<span class="token punctuation">,</span>
            r<span class="token operator">=</span>lora_r
        <span class="token punctuation">)</span>

    <span class="token keyword">return</span> model<span class="token punctuation">,</span> tokenizer
</code></pre><h4 id="生成函数实现" tabindex="-1">生成函数实现 <a class="header-anchor" href="#生成函数实现" aria-hidden="true">#</a></h4><pre class="language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">generate_with_lora</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> tokenizer<span class="token punctuation">,</span> prompt<span class="token punctuation">,</span> max_length<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""使用 LoRA 模型生成对话"""</span>
    model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

    inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>prompt<span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">)</span>

    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        outputs <span class="token operator">=</span> model<span class="token punctuation">.</span>generate<span class="token punctuation">(</span>
            inputs<span class="token punctuation">,</span>
            max_length<span class="token operator">=</span>max_length<span class="token punctuation">,</span>
            num_return_sequences<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
            temperature<span class="token operator">=</span><span class="token number">0.7</span><span class="token punctuation">,</span>
            do_sample<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
            pad_token_id<span class="token operator">=</span>tokenizer<span class="token punctuation">.</span>eos_token_id
        <span class="token punctuation">)</span>

    generated_text <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> skip_special_tokens<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> generated_text<span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>prompt<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
</code></pre><h2 id="总结" tabindex="-1">总结 <a class="header-anchor" href="#总结" aria-hidden="true">#</a></h2><h3 id="lora-的优势与局限" tabindex="-1">LoRA 的优势与局限 <a class="header-anchor" href="#lora-的优势与局限" aria-hidden="true">#</a></h3><p><strong>优势：</strong></p><ul><li><strong>参数效率</strong>：显著减少可训练参数数量</li><li><strong>存储效率</strong>：模型检查点大小仅为 MB 级别</li><li><strong>部署灵活性</strong>：支持多任务快速切换</li><li><strong>训练稳定性</strong>：较少的参数更新，训练更稳定</li></ul><p><strong>局限：</strong></p><ul><li><strong>表现力限制</strong>：低秩假设可能限制某些复杂任务的性能</li><li><strong>秩选择</strong>：需要为不同任务和层选择合适的秩</li><li><strong>推理开销</strong>：未合并时会有额外的计算开销</li></ul><h3 id="相关资源" tabindex="-1">相关资源 <a class="header-anchor" href="#相关资源" aria-hidden="true">#</a></h3><ul><li><strong>论文地址</strong>：<a href="https://arxiv.org/abs/2106.09685" target="_blank" rel="noopener">LoRA: Low-Rank Adaptation of Large Language Models</a></li><li><strong>官方实现</strong>：<a href="https://github.com/microsoft/LoRA" target="_blank" rel="noopener">Microsoft LoRA GitHub</a></li><li><strong>PEFT 库</strong>：<a href="https://github.com/huggingface/peft" target="_blank" rel="noopener">Hugging Face PEFT</a></li><li><strong>视频介绍</strong>：<a href="https://www.youtube.com/watch?v=DhRoTONcyZE" target="_blank" rel="noopener">LoRA 技术讲解</a></li></ul></div><!--]--><!--]--></div></article><div class="prose m-auto mt-8 mb-8"><a class="font-mono no-underline opacity-50 hover:opacity-75">cd ..</a></div><!--]--><div class="mt-10 mb-6 prose m-auto opacity-50 flex"><span class="text-sm"><a target="_blank" href="https://beian.miit.gov.cn" style="color:inherit">浙ICP备2021022773号 &nbsp;&nbsp; </a>2022-PRESENT © ZhengKe</span><div class="flex-auto"></div></div></main><!--]--></div><link rel="stylesheet" href="/assets/app.baf61519.css"><link rel="stylesheet" href="/assets/Model-Format-GGUF.c4ecc021.css"><link rel="stylesheet" href="/assets/Post.63afffbd.css"></body></html>