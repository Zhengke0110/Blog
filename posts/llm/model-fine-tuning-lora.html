<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="author" content="ZhengKe"><meta http-equiv="X-UA-Compatible" content="chrome=1"><meta name="revisit-after" content="7 days"><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="icon" href="/favicon.svg" type="image/svg+xml"><script src="/lib/mermaid.min.js"></script><meta name="msapplication-TileColor" content="#ffffff"><meta name="theme-color" content="#ffffff"><title>LoRA：低秩适应技术</title><script>(()=>{var e=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,a=localStorage.getItem("vueuse-color-scheme")||"auto";("dark"===a||e&&"light"!==a)&&document.documentElement.classList.toggle("dark",!0)})()</script><script type="module" crossorigin="" src="/assets/app.038d67a7.js"></script><style>*,:after,:before{box-sizing:border-box;border-width:0;border-style:solid;border-color:currentColor}html{line-height:1.5;-webkit-text-size-adjust:100%;-moz-tab-size:4;tab-size:4;font-family:ui-sans-serif,system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,"Apple Color Emoji","Segoe UI Emoji",Segoe UI Symbol,"Noto Color Emoji"}body{margin:0;line-height:inherit}hr{height:0;color:inherit;border-top-width:1px}h1,h2,h3,h4{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}strong{font-weight:bolder}code,pre{font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1em}table{text-indent:0;border-color:inherit;border-collapse:collapse}h1,h2,h3,h4,hr,p,pre{margin:0}ol,ul{list-style:none;margin:0;padding:0}img,svg{display:block;vertical-align:middle}img{max-width:100%;height:auto}.prose mjx-container{margin:0 .2em!important;display:inline-block!important}.prose mjx-container[display=true]{display:block!important;margin:1.5em auto!important;text-align:center!important}@media (max-width:768px){.prose mjx-container[display=true]{font-size:.9em!important;margin:1.2em auto!important}}@media (max-width:480px){.prose mjx-container[display=true]{font-size:.8em!important;margin:1em auto!important}}.prose mjx-container{opacity:1!important;z-index:1!important}.prose mjx-assistive-mml{position:absolute!important;clip:rect(1px,1px,1px,1px)!important;padding:0!important;border:0!important;height:1px!important;width:1px!important;overflow:hidden!important}:root{--c-bg:#fff;--c-scrollbar:#eee;--c-scrollbar-hover:#bbb}html{background-color:var(--c-bg)}html{overflow:scroll}*{scrollbar-color:var(--c-scrollbar) var(--c-bg)}.prose{color:var(--fg);max-width:75ch;font-size:1rem;line-height:1.75}.prose a{color:var(--fg-deeper);text-decoration:none;font-weight:500}.prose strong{color:var(--fg-deep);font-weight:600}.prose ol>li{position:relative;padding-left:1.75em}.prose ol>li:before{content:counter(list-item,var(--list-counter-style,decimal)) ".";position:absolute;font-weight:400;color:#6b7280;left:0}.prose ul>li{position:relative;padding-left:1.75em}.prose ul>li:before{content:"";position:absolute;background-color:#d1d5db;border-radius:50%;width:.375em;height:.375em;top:.6875em;left:.25em}.prose hr{border-color:#7d7d7d4d;margin-top:3em;margin-bottom:3em}.prose h1{color:var(--fg-deeper);font-weight:800;font-size:2.25em;margin-top:0;margin-bottom:.8888889em;line-height:1.1111111}.prose h2{color:var(--fg-deep);font-weight:700;font-size:1.5em;margin-top:2em;margin-bottom:1em;line-height:1.3333333}.prose h3{color:inherit;font-weight:600;font-size:1.25em;margin-top:1.6em;margin-bottom:.6em;line-height:1.6}.prose h4{color:inherit;font-weight:600;margin-top:1.5em;margin-bottom:.5em;line-height:1.5}.prose code{color:var(--fg-deep);font-weight:600;font-size:.875em}.prose code:before{content:"`"}.prose code:after{content:"`"}.prose pre{color:#e5e7eb;background-color:#1f2937;overflow-x:auto;font-size:.875em;line-height:1.7142857;margin-top:1.7142857em;margin-bottom:1.7142857em;border-radius:.375rem;padding:.8571429em 1.1428571em}.prose pre code{background-color:transparent;border-width:0;border-radius:0;padding:0;font-weight:400;color:inherit;font-size:inherit;font-family:inherit;line-height:inherit}.prose pre code:before{content:none}.prose pre code:after{content:none}.prose table{width:100%;table-layout:auto;text-align:left;margin-top:2em;margin-bottom:2em;font-size:.875em;line-height:1.7142857}.prose thead{color:#111827;font-weight:600;border-bottom-width:1px;border-bottom-color:#d1d5db}.prose thead th{vertical-align:bottom;padding-right:.5714286em;padding-bottom:.5714286em;padding-left:.5714286em}.prose tbody tr{border-bottom-width:1px;border-bottom-color:#e5e7eb}.prose tbody tr:last-child{border-bottom-width:0}.prose tbody td{vertical-align:top;padding:.5714286em}.prose p{margin-top:1.25em;margin-bottom:1.25em}.prose ol,.prose ul{margin-top:1.25em;margin-bottom:1.25em;list-style-type:none}.prose li{margin-top:.5em;margin-bottom:.5em}.prose>ol>li>:first-child{margin-top:1.25em}.prose>ol>li>:last-child{margin-bottom:1.25em}.prose thead th:first-child{padding-left:0}.prose thead th:last-child{padding-right:0}.prose tbody td:first-child{padding-left:0}.prose tbody td:last-child{padding-right:0}.prose>:first-child{margin-top:0}.prose>:last-child{margin-bottom:0}:root{--prism-scheme:light;--prism-foreground:#6e6e6e;--prism-background:#f4f4f4;--prism-comment:#a8a8a8;--prism-string:#555555;--prism-literal:#333333;--prism-keyword:#000000;--prism-function:#4f4f4f;--prism-deleted:#333333;--prism-class:#333333;--prism-builtin:#757575;--prism-property:#333333;--prism-namespace:#4f4f4f;--prism-punctuation:#ababab;--prism-decorator:var(--prism-class);--prism-operator:var(--prism-punctuation);--prism-number:var(--prism-literal);--prism-boolean:var(--prism-literal);--prism-variable:var(--prism-literal);--prism-constant:var(--prism-literal);--prism-symbol:var(--prism-literal);--prism-interpolation:var(--prism-literal);--prism-selector:var(--prism-keyword);--prism-keyword-control:var(--prism-keyword);--prism-regex:var(--prism-string);--prism-json-property:var(--prism-property);--prism-inline-background:var(--prism-background);--prism-comment-style:italic;--prism-url-decoration:underline;--prism-line-number:#a5a5a5;--prism-line-number-gutter:#333333;--prism-line-highlight-background:#eeeeee;--prism-selection-background:#dddddd;--prism-marker-color:var(--prism-foreground);--prism-marker-opacity:.4;--prism-marker-font-size:.8em;--prism-font-size:1em;--prism-line-height:1.5em;--prism-font-family:monospace;--prism-inline-font-size:var(--prism-font-size);--prism-block-font-size:var(--prism-font-size);--prism-tab-size:2;--prism-block-padding-x:1em;--prism-block-padding-y:1em;--prism-block-margin-x:0;--prism-block-margin-y:.5em;--prism-block-radius:.3em;--prism-inline-padding-x:.3em;--prism-inline-padding-y:.1em;--prism-inline-radius:.3em}code[class*=language-],pre[class*=language-]{font-size:var(--prism-font-size);font-family:var(--prism-font-family);direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;line-height:var(--prism-line-height);-moz-tab-size:var(--prism-tab-size);-o-tab-size:var(--prism-tab-size);tab-size:var(--prism-tab-size);-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;color:var(--prism-foreground)!important}pre[class*=language-]{font-size:var(--prism-block-font-size);padding:var(--prism-block-padding-y) var(--prism-block-padding-x);margin:var(--prism-block-margin-y) var(--prism-block-margin-x);border-radius:var(--prism-block-radius);overflow:auto;background:var(--prism-background)}code[class*=language-] ::-moz-selection,code[class*=language-]::-moz-selection,pre[class*=language-] ::-moz-selection,pre[class*=language-]::-moz-selection{background:var(--prism-selection-background)}code[class*=language-] ::selection,code[class*=language-]::selection,pre[class*=language-] ::selection,pre[class*=language-]::selection{background:var(--prism-selection-background)}.token.string{color:var(--prism-string)}.token.punctuation{color:var(--prism-punctuation)}.token.operator{color:var(--prism-operator)}.token.keyword{color:var(--prism-keyword)}.token.property{color:var(--prism-property)}:root{--prism-font-size:.9rem;--prism-font-family:"Fira Code",monospace}:root{--prism-font-family:"Input Mono",monospace}html:not(.dark){--prism-foreground:#393a34;--prism-background:#fbfbfb;--prism-comment:#a0ada0;--prism-string:#b56959;--prism-literal:#2f8a89;--prism-number:#296aa3;--prism-keyword:#1c6b48;--prism-function:#6c7834;--prism-boolean:#1c6b48;--prism-constant:#a65e2b;--prism-deleted:#a14f55;--prism-class:#2993a3;--prism-builtin:#ab5959;--prism-property:#b58451;--prism-namespace:#b05a78;--prism-punctuation:#8e8f8b;--prism-decorator:#bd8f8f;--prism-regex:#ab5e3f;--prism-json-property:#698c96}.prose{--fg:#555;--fg-deep:#222;--fg-deeper:#000;color:var(--fg)}.prose a{font-weight:inherit;text-decoration:none;border-bottom:1px solid rgba(125,125,125,.3);transition:border .3s ease-in-out}.prose a:hover{border-bottom:1px solid var(--fg)}.prose hr{width:50px;margin:2em auto}a.header-anchor{float:left;margin-top:.125em;margin-left:-1.2em;padding-right:.5em;font-size:.85em;opacity:0;text-decoration:none;border:0!important}a.header-anchor:focus,a.header-anchor:hover{text-decoration:none}h2:focus .header-anchor,h2:hover .header-anchor,h3:focus .header-anchor,h3:hover .header-anchor,h4:focus .header-anchor,h4:hover .header-anchor{opacity:.5}.prose .MathJax{font-size:inherit!important;color:inherit!important}.prose .MathJax svg{max-width:100%!important;height:auto!important}.absolute{position:absolute}.z-40{z-index:40}.m-6{margin:1.5rem}.m-auto{margin:auto}.\!-mt-2{margin-top:-.5rem!important}.mb-0{margin-bottom:0}.mb-6{margin-bottom:1.5rem}.mb-8{margin-bottom:2rem}.mt-10{margin-top:2.5rem}.mt-8{margin-top:2rem}.h-10{height:2.5rem}.w-10{width:2.5rem}.flex{display:flex}.flex-auto{flex:1 1 auto}.select-none{user-select:none}.px-7{padding-left:1.75rem;padding-right:1.75rem}.py-10{padding-top:2.5rem;padding-bottom:2.5rem}.font-mono{font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace}.font-sans{font-family:Inter,Inter var,system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}.text-sm{font-size:.875rem;line-height:1.25rem}.text-gray-700{--un-text-opacity:1;color:rgba(55,65,81,var(--un-text-opacity))}.no-underline{text-decoration:none}.opacity-50{opacity:.5}.hover\:opacity-75:hover{opacity:.75}.outline-none{outline:2px solid transparent;outline-offset:2px}@media (max-width:768px){.lt-md\:hidden{display:none}}@media (min-width:768px){.md\:hidden{display:none}}@media (min-width:1024px){.lg\:fixed{position:fixed}}.nav[data-v-568133b8]{padding:2rem;width:100%;display:grid;grid-template-columns:auto max-content;box-sizing:border-box}.nav[data-v-568133b8]>*{margin:auto}.nav a[data-v-568133b8]{cursor:pointer;text-decoration:none;color:inherit;transition:opacity .2s ease;opacity:.6;outline:0}.nav a[data-v-568133b8]:hover{opacity:1;text-decoration-color:inherit}.nav .right[data-v-568133b8]{display:grid;grid-gap:1.2rem;grid-auto-flow:column}.nav .right[data-v-568133b8]>*{margin:auto}.mathjax-container[data-v-011ba505]{color:inherit;font-family:inherit;line-height:inherit}</style><link rel="preload" href="/assets/app.baf61519.css" as="style"><link rel="modulepreload" crossorigin="" href="/assets/Model-fine-tuning-LoRA.1320ec10.js"><link rel="modulepreload" crossorigin="" href="/assets/Post.dd5dd46a.js"><link rel="preload" href="/assets/Post.63afffbd.css" as="style"><meta property="og:title" content="LoRA：低秩适应技术"><meta name="head:count" content="1"></head><body class="font-sans text-gray-700 dark:text-gray-200"><div id="app" data-server-rendered="true"><!--[--><header class="header z-40" data-v-568133b8=""><a href="/" class="w-10 h-10 absolute lg:fixed m-6 select-none outline-none" focusable="false" data-v-568133b8=""><img src="/logo-dark.svg" alt="logo" style="display:none" data-v-568133b8=""><img src="/logo.svg" alt="logo" data-v-568133b8=""></a><nav class="nav" data-v-568133b8=""><div class="spacer" data-v-568133b8=""></div><div class="right" data-v-568133b8=""><a href="/posts" class="" data-v-568133b8=""><span class="lt-md:hidden" data-v-568133b8="">Blog</span><svg style="vertical-align:sub" class="inline md:hidden" width="1.2em" height="1.2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24" data-v-568133b8=""><path fill="currentColor" d="M20 22H4a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1h16a1 1 0 0 1 1 1v18a1 1 0 0 1-1 1m-1-2V4H5v16zM7 6h4v4H7zm0 6h10v2H7zm0 4h10v2H7zm6-9h4v2h-4z"></path></svg></a><a href="/llm" class="lt-md:hidden" data-v-568133b8="">LLM </a><a href="/notes" class="" title="Notes" data-v-568133b8=""><span class="lt-md:hidden" data-v-568133b8="">Notes</span><svg style="vertical-align:sub" class="inline md:hidden" width="1.2em" height="1.2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24" data-v-568133b8=""><path fill="currentColor" d="m17.85 11.698l-.708-.707l-9.9 9.9H3v-4.243L14.314 5.334l5.657 5.657a1 1 0 0 1 0 1.414L12.9 19.477l-1.415-1.415zm-2.122-2.121l-1.414-1.414L5 17.477v1.414h1.414zm2.828-7.071l2.829 2.828a1 1 0 0 1 0 1.415L19.97 8.163L15.728 3.92l1.414-1.414a1 1 0 0 1 1.414 0"></path></svg></a><a href="https://github.com/ZhengKe0110" target="_blank" title="GitHub" class="lt-md:hidden" data-v-568133b8=""><svg style="vertical-align:sub" class="inline" width="1.2em" height="1.2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24" data-v-568133b8=""><path fill="currentColor" d="M10.07 20.503a1 1 0 0 0-1.18-.983c-1.31.24-2.963.276-3.402-.958a5.7 5.7 0 0 0-1.837-2.415a1 1 0 0 1-.167-.11a1 1 0 0 0-.93-.645h-.005a1 1 0 0 0-1 .995c-.004.815.81 1.338 1.141 1.514a4.4 4.4 0 0 1 .924 1.36c.365 1.023 1.423 2.576 4.466 2.376l.003.098l.004.268a1 1 0 0 0 2 0l-.005-.318c-.005-.19-.012-.464-.012-1.182M20.737 5.377q.049-.187.09-.42a6.3 6.3 0 0 0-.408-3.293a1 1 0 0 0-.615-.58c-.356-.12-1.67-.357-4.184 1.25a13.9 13.9 0 0 0-6.354 0C6.762.75 5.455.966 5.102 1.079a1 1 0 0 0-.631.584a6.3 6.3 0 0 0-.404 3.357q.037.191.079.354a6.27 6.27 0 0 0-1.256 3.83a8 8 0 0 0 .043.921c.334 4.603 3.334 5.984 5.424 6.459a5 5 0 0 0-.118.4a1 1 0 0 0 1.942.479a1.7 1.7 0 0 1 .468-.878a1 1 0 0 0-.546-1.745c-3.454-.395-4.954-1.802-5.18-4.899a7 7 0 0 1-.033-.738a4.26 4.26 0 0 1 .92-2.713a3 3 0 0 1 .195-.231a1 1 0 0 0 .188-1.025a3.4 3.4 0 0 1-.155-.555a4.1 4.1 0 0 1 .079-1.616a7.5 7.5 0 0 1 2.415 1.18a1 1 0 0 0 .827.133a11.8 11.8 0 0 1 6.173.001a1 1 0 0 0 .83-.138a7.6 7.6 0 0 1 2.406-1.19a4 4 0 0 1 .087 1.578a3.2 3.2 0 0 1-.169.607a1 1 0 0 0 .188 1.025c.078.087.155.18.224.268A4.12 4.12 0 0 1 20 9.203a7 7 0 0 1-.038.777c-.22 3.056-1.725 4.464-5.195 4.86a1 1 0 0 0-.546 1.746a1.63 1.63 0 0 1 .466.908a3 3 0 0 1 .093.82v2.333c-.01.648-.01 1.133-.01 1.356a1 1 0 1 0 2 0c0-.217 0-.692.01-1.34v-2.35a5 5 0 0 0-.155-1.311a4 4 0 0 0-.116-.416a6.51 6.51 0 0 0 5.445-6.424A9 9 0 0 0 22 9.203a6.13 6.13 0 0 0-1.263-3.826"></path></svg></a><a class="select-none" title="Toggle Color Scheme" data-v-568133b8=""><svg style="vertical-align:sub;display:none" class="inline" width="1.2em" height="1.2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="currentColor" d="M10 7a7 7 0 0 0 12 4.9v.1c0 5.523-4.477 10-10 10S2 17.523 2 12S6.477 2 12 2h.1A6.98 6.98 0 0 0 10 7m-6 5a8 8 0 0 0 15.062 3.762A9 9 0 0 1 8.238 4.938A8 8 0 0 0 4 12"></path></svg><svg style="vertical-align:sub" class="inline" width="1.2em" height="1.2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="currentColor" d="M12 18a6 6 0 1 1 0-12a6 6 0 0 1 0 12m0-2a4 4 0 1 0 0-8a4 4 0 0 0 0 8M11 1h2v3h-2zm0 19h2v3h-2zM3.515 4.929l1.414-1.414L7.05 5.636L5.636 7.05zM16.95 18.364l1.414-1.414l2.121 2.121l-1.414 1.414zm2.121-14.85l1.414 1.415l-2.121 2.121l-1.414-1.414zM5.636 16.95l1.414 1.414l-2.121 2.121l-1.414-1.414zM23 11v2h-3v-2zM4 11v2H1v-2z"></path></svg></a></div></nav></header><main class="px-7 py-10"><!--[--><div class="prose m-auto mb-8"><h1 class="mb-0">LoRA：低秩适应技术</h1><p class="opacity-50 !-mt-2">Aug 13<!----></p><!----></div><article><div class="mathjax-container" data-v-011ba505=""><!--[--><!--[--><div class="prose m-auto"><h2 id="概述" tabindex="-1">概述 <a class="header-anchor" href="#概述" aria-hidden="true">#</a></h2><p>LoRA (Low-Rank Adaptation) 是一种参数高效的微调技术，由微软研究团队在 2021 年提出。该技术通过学习低秩分解矩阵来适应预训练的大语言模型，同时保持原始权重冻结，极大地减少了可训练参数的数量和存储需求。</p><h3 id="核心思想" tabindex="-1">核心思想 <a class="header-anchor" href="#核心思想" aria-hidden="true">#</a></h3><p>LoRA 的核心思想是：<strong>模型微调过程中的权重更新具有内在的低秩特性</strong>。具体来说：</p><ul><li>将权重更新分解为两个低秩矩阵的乘积</li><li>只训练这些低秩矩阵，保持原始权重冻结</li><li>在推理时可以将低秩矩阵合并到原始权重中，无额外延迟</li></ul><pre class="language-mermaid"><code class="language-mermaid"><span class="token keyword">graph</span> LR
    W<span class="token text string">[预训练权重 W 冻结]</span> <span class="token arrow operator">--&gt;</span><span class="token label property">|保持原样|</span> Stable<span class="token text string">[保持通用能力]</span>
    Delta<span class="token text string">[增量权重 ΔW = A×B]</span> <span class="token arrow operator">--&gt;</span><span class="token label property">|低秩训练|</span> Adapt<span class="token text string">[任务特定适配]</span>
    Input<span class="token text string">[输入 X]</span> <span class="token arrow operator">--&gt;</span> Calc<span class="token text string">[Y = W + AB X]</span>
    Stable <span class="token arrow operator">--&gt;</span> Calc
    Adapt <span class="token arrow operator">--&gt;</span> Calc
    <span class="token keyword">style</span> W <span class="token style"><span class="token property">fill</span><span class="token operator">:</span>#f9f<span class="token punctuation">,</span><span class="token property">stroke</span><span class="token operator">:</span>#333</span>
    <span class="token keyword">style</span> Delta <span class="token style"><span class="token property">fill</span><span class="token operator">:</span>#9f9<span class="token punctuation">,</span><span class="token property">stroke</span><span class="token operator">:</span>#333</span>
    <span class="token keyword">style</span> Calc <span class="token style"><span class="token property">fill</span><span class="token operator">:</span>#eef<span class="token punctuation">,</span><span class="token property">stroke</span><span class="token operator">:</span>#333</span>
</code></pre><hr><h2 id="一、lora核心定义与概述" tabindex="-1">一、LoRA核心定义与概述 <a class="header-anchor" href="#一、lora核心定义与概述" aria-hidden="true">#</a></h2><p>LoRA（Low-Rank Adaptation）是2021年由微软研究团队提出的<strong>参数高效型大模型微调技术</strong>，并非传统算法，核心是通过学习低秩分解矩阵适配预训练大语言模型，同时保持原始权重冻结，从根源上解决全量微调"资源消耗大、训练效率低"的痛点。</p><p>其关键特性可概括为三点：</p><ol><li><strong>大幅减少可训练参数</strong>：仅为全量微调的0.01%~3%，原论文中可减少10,000倍</li><li><strong>降低存储与显存需求</strong>：模型检查点仅几MB，GPU内存使用减少3倍</li><li><strong>支持灵活部署</strong>：多任务切换高效，推理时合并权重无额外延迟</li></ol><p>普通GPU即可实现十亿参数级模型的微调。</p><h2 id="二、lora核心思想与数学原理" tabindex="-1">二、LoRA核心思想与数学原理 <a class="header-anchor" href="#二、lora核心思想与数学原理" aria-hidden="true">#</a></h2><h3 id="_1-核心思想：权重更新的低秩特性" tabindex="-1">1. 核心思想：权重更新的低秩特性 <a class="header-anchor" href="#_1-核心思想：权重更新的低秩特性" aria-hidden="true">#</a></h3><p>LoRA的核心逻辑基于"<strong>预训练模型微调时的权重更新具有内在低秩特性</strong>"——即微调导致的权重变化量（ΔW = W任务 - W预训练）并非随机分布，其信息高度集中在少数几个维度（如GPT-3上ΔW的前10-20个奇异值占据90%以上信息），因此可通过两个低秩矩阵A、B的乘积近似表示ΔW，无需更新原始权重矩阵W。</p><p>这种设计既保留了原始模型的基础能力，又能针对性适配新任务，还避免了"灾难性遗忘"。</p><h3 id="_2-数学原理与关键公式" tabindex="-1">2. 数学原理与关键公式 <a class="header-anchor" href="#_2-数学原理与关键公式" aria-hidden="true">#</a></h3><h4 id="（1）基础公式：权重调整与低秩分解" tabindex="-1">（1）基础公式：权重调整与低秩分解 <a class="header-anchor" href="#（1）基础公式：权重调整与低秩分解" aria-hidden="true">#</a></h4><p>LoRA通过低秩矩阵叠加实现权重调整，核心公式为：</p><mjx-container class="MathJax" jax="SVG" display="true" style="direction:ltr;display:block;text-align:center;margin:1em 0;position:relative"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-.186ex" xmlns="http://www.w3.org/2000/svg" width="29.867ex" height="2.016ex" role="img" focusable="false" viewBox="0 -809 13201.1 891" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3"></path></g><g data-mml-node="mo" transform="translate(1136.2,413) scale(0.707)"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z" style="stroke-width:3"></path></g></g><g data-mml-node="mo" transform="translate(1658.4,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3"></path></g><g data-mml-node="mi" transform="translate(2714.2,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3"></path></g><g data-mml-node="mo" transform="translate(3984.4,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width:3"></path></g><g data-mml-node="mi" transform="translate(4984.7,0)"><path data-c="394" d="M51 0Q46 4 46 7Q46 9 215 357T388 709Q391 716 416 716Q439 716 444 709Q447 705 616 357T786 7Q786 4 781 0H51ZM507 344L384 596L137 92L383 91H630Q630 93 507 344Z" style="stroke-width:3"></path></g><g data-mml-node="mi" transform="translate(5817.7,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3"></path></g><g data-mml-node="mo" transform="translate(7143.4,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3"></path></g><g data-mml-node="mi" transform="translate(8199.2,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3"></path></g><g data-mml-node="mo" transform="translate(9469.4,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width:3"></path></g><g data-mml-node="mi" transform="translate(10469.7,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z" style="stroke-width:3"></path></g><g data-mml-node="mo" transform="translate(11441.9,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z" style="stroke-width:3"></path></g><g data-mml-node="mi" transform="translate(12442.1,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z" style="stroke-width:3"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="block" style="top:0;left:0;clip:rect(1px,1px,1px,1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0 0 0;border:0;display:block;overflow:hidden;width:100%"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msup><mi>W</mi><mo data-mjx-alternate="1">′</mo></msup><mo>=</mo><mi>W</mi><mo>+</mo><mi mathvariant="normal">Δ</mi><mi>W</mi><mo>=</mo><mi>W</mi><mo>+</mo><mi>A</mi><mo>×</mo><mi>B</mi></math></mjx-assistive-mml></mjx-container><p>其中：</p><ul><li><mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-.05ex" xmlns="http://www.w3.org/2000/svg" width="2.371ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 1048 705" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0;left:0;clip:rect(1px,1px,1px,1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0 0 0;border:0;display:block;width:auto;overflow:hidden"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W</mi></math></mjx-assistive-mml></mjx-container>：预训练模型原始权重矩阵（形状d×k，冻结，梯度设为0）</li><li><mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:0" xmlns="http://www.w3.org/2000/svg" width="1.697ex" height="1.62ex" role="img" focusable="false" viewBox="0 -716 750 716" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z" style="stroke-width:3"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0;left:0;clip:rect(1px,1px,1px,1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0 0 0;border:0;display:block;width:auto;overflow:hidden"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi></math></mjx-assistive-mml></mjx-container>：低秩矩阵（形状d×r，r为秩，通常远小于d、k，可训练）</li><li><mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:0" xmlns="http://www.w3.org/2000/svg" width="1.717ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 759 683" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z" style="stroke-width:3"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0;left:0;clip:rect(1px,1px,1px,1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0 0 0;border:0;display:block;width:auto;overflow:hidden"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>B</mi></math></mjx-assistive-mml></mjx-container>：低秩矩阵（形状r×k，可训练）</li><li><mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-.05ex" xmlns="http://www.w3.org/2000/svg" width="3.124ex" height="1.767ex" role="img" focusable="false" viewBox="0 -759 1380.7 781" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3"></path></g><g data-mml-node="mo" transform="translate(1136.2,363) scale(0.707)"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z" style="stroke-width:3"></path></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0;left:0;clip:rect(1px,1px,1px,1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0 0 0;border:0;display:block;width:auto;overflow:hidden"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>W</mi><mo data-mjx-alternate="1">′</mo></msup></math></mjx-assistive-mml></mjx-container>：调整后的有效权重，用于前向传播计算</li></ul><p>为控制适应强度，LoRA引入<strong>缩放因子</strong>优化公式，进一步平衡低秩矩阵对权重的影响：</p><mjx-container class="MathJax" jax="SVG" display="true" style="direction:ltr;display:block;text-align:center;margin:1em 0;position:relative"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-1.577ex" xmlns="http://www.w3.org/2000/svg" width="18.655ex" height="4.106ex" role="img" focusable="false" viewBox="0 -1118 8245.6 1815" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z" style="stroke-width:3"></path></g><g data-mml-node="mo" transform="translate(853.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3"></path></g><g data-mml-node="msub" transform="translate(1909.6,0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3"></path></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" style="stroke-width:3"></path></g></g><g data-mml-node="mi" transform="translate(3290.1,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3"></path></g><g data-mml-node="mo" transform="translate(4084.3,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width:3"></path></g><g data-mml-node="mfrac" transform="translate(5084.6,0)"><g data-mml-node="mi" transform="translate(220,676)"><path data-c="1D6FC" d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z" style="stroke-width:3"></path></g><g data-mml-node="mi" transform="translate(314.5,-686)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3"></path></g><rect width="840" height="60" x="120" y="220"></rect></g><g data-mml-node="mi" transform="translate(6164.6,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z" style="stroke-width:3"></path></g><g data-mml-node="mi" transform="translate(6923.6,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z" style="stroke-width:3"></path></g><g data-mml-node="mi" transform="translate(7673.6,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="block" style="top:0;left:0;clip:rect(1px,1px,1px,1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0 0 0;border:0;display:block;overflow:hidden;width:100%"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>h</mi><mo>=</mo><msub><mi>W</mi><mn>0</mn></msub><mi>x</mi><mo>+</mo><mfrac><mi>α</mi><mi>r</mi></mfrac><mi>B</mi><mi>A</mi><mi>x</mi></math></mjx-assistive-mml></mjx-container><p>其中<mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-.025ex" xmlns="http://www.w3.org/2000/svg" width="1.448ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 640 453" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FC" d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z" style="stroke-width:3"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0;left:0;clip:rect(1px,1px,1px,1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0 0 0;border:0;display:block;width:auto;overflow:hidden"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>α</mi></math></mjx-assistive-mml></mjx-container>为超参数，通常设置为秩r的整数倍（如r=16时α=32），确保训练过程中权重更新的稳定性。</p><h4 id="（2）初始化策略" tabindex="-1">（2）初始化策略 <a class="header-anchor" href="#（2）初始化策略" aria-hidden="true">#</a></h4><p>为避免初始阶段低秩矩阵对原始模型的干扰，LoRA采用特殊初始化方式：</p><ul><li>矩阵<mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:0" xmlns="http://www.w3.org/2000/svg" width="1.697ex" height="1.62ex" role="img" focusable="false" viewBox="0 -716 750 716" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z" style="stroke-width:3"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0;left:0;clip:rect(1px,1px,1px,1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0 0 0;border:0;display:block;width:auto;overflow:hidden"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi></math></mjx-assistive-mml></mjx-container>使用随机高斯分布初始化（如 kaiming_uniform 初始化），确保初始值较小</li><li>矩阵<mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:0" xmlns="http://www.w3.org/2000/svg" width="1.717ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 759 683" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z" style="stroke-width:3"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0;left:0;clip:rect(1px,1px,1px,1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0 0 0;border:0;display:block;width:auto;overflow:hidden"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>B</mi></math></mjx-assistive-mml></mjx-container>初始化为全零矩阵，使训练开始时 ΔW = A×B = 0，完全保留原始模型性能，后续通过反向传播逐步更新A、B参数</li></ul><h2 id="三、lora技术优势与性能表现" tabindex="-1">三、LoRA技术优势与性能表现 <a class="header-anchor" href="#三、lora技术优势与性能表现" aria-hidden="true">#</a></h2><h3 id="_1-核心技术优势" tabindex="-1">1. 核心技术优势 <a class="header-anchor" href="#_1-核心技术优势" aria-hidden="true">#</a></h3><p>结合原总结与新增文本，LoRA的优势可归纳为五大维度：</p><table><thead><tr><th>优势类型</th><th>具体表现</th><th>数据支撑（原论文/实验结果）</th></tr></thead><tbody><tr><td>参数效率</td><td>可训练参数极少，仅更新低秩矩阵A、B</td><td>RoBERTa-Base全量微调需125M参数，LoRA仅需0.8M；DeBERTa-XXL全量微调需1.5B参数，LoRA仅需4.7M</td></tr><tr><td>内存与存储效率</td><td>降低显存占用，减少模型存储成本</td><td>GPU内存使用减少3倍；模型检查点从GB级降至MB级（RoBERTa-Base LoRA checkpoint仅3.2MB）</td></tr><tr><td>训练效率</td><td>仅更新少量参数，反向传播计算量小，训练速度快</td><td>普通GPU可训练十亿参数模型，无需高端硬件支持</td></tr><tr><td>部署灵活性</td><td>支持多任务共享基础模型，LoRA模块“即插即用”，任务切换无需重新训练</td><td>切换任务仅需加载不同LoRA文件（几秒完成），一个基础模型可搭配多个LoRA适配器</td></tr><tr><td>推理性能</td><td>推理时可将低秩矩阵合并到原始权重，无额外延迟，性能接近全量微调</td><td>RoBERTa-Base LoRA在GLUE任务平均分87.5±0.3，与全量微调（87.6）几乎持平；DeBERTa-XXL LoRA平均分91.9±0.1，超越全量微调</td></tr></tbody></table><h3 id="_2-与全量微调的性能对比" tabindex="-1">2. 与全量微调的性能对比 <a class="header-anchor" href="#_2-与全量微调的性能对比" aria-hidden="true">#</a></h3><p>以文本分类、通用语言理解（GLUE任务）为例，LoRA在参数量大幅减少的前提下，性能仍能匹配甚至超越全量微调，具体对比如下：</p><table><thead><tr><th>模型</th><th>微调方法</th><th>可训练参数</th><th>存储需求</th><th>GLUE 平均分</th><th>性能差距（与全量微调比）</th></tr></thead><tbody><tr><td>RoBERTa-Base</td><td>全量微调</td><td>125M</td><td>~500MB</td><td>87.6</td><td>-</td></tr><tr><td>RoBERTa-Base</td><td>LoRA</td><td>0.8M</td><td>~3.2MB</td><td>87.5±0.3</td><td>差距仅0.1-0.4分</td></tr><tr><td>DeBERTa-XXL</td><td>全量微调</td><td>1.5B</td><td>~6GB</td><td>91.7</td><td>-</td></tr><tr><td>DeBERTa-XXL</td><td>LoRA</td><td>4.7M</td><td>~18.8MB</td><td>91.9±0.1</td><td>反超0.2-0.3分</td></tr></tbody></table><p>此外，在推理延迟上，LoRA合并权重后与原始模型几乎无差异，未合并时虽有少量额外计算，但通过优化可将延迟控制在极低范围（如合并后延迟比未合并时降低约5%-10%）。</p><h3 id="内存使用对比" tabindex="-1">内存使用对比 <a class="header-anchor" href="#内存使用对比" aria-hidden="true">#</a></h3><pre class="language-mermaid"><code class="language-mermaid"><span class="token keyword">graph</span> LR
    A<span class="token text string">[全量微调]</span> <span class="token arrow operator">--&gt;</span> B<span class="token text string">[高内存占用]</span>
    C<span class="token text string">[LoRA]</span> <span class="token arrow operator">--&gt;</span> D<span class="token text string">[低内存占用]</span>
    B <span class="token arrow operator">--&gt;</span> E<span class="token text string">[前向传播&lt;br/&gt;反向传播&lt;br/&gt;优化器状态&lt;br/&gt;梯度存储]</span>
    D <span class="token arrow operator">--&gt;</span> F<span class="token text string">[前向传播&lt;br/&gt;少量梯度&lt;br/&gt;小优化器状态]</span>
    <span class="token keyword">style</span> B <span class="token style"><span class="token property">fill</span><span class="token operator">:</span>#ffcccc</span>
    <span class="token keyword">style</span> D <span class="token style"><span class="token property">fill</span><span class="token operator">:</span>#ccffcc</span>
</code></pre><h2 id="四、lora实现细节与应用场景" tabindex="-1">四、LoRA实现细节与应用场景 <a class="header-anchor" href="#四、lora实现细节与应用场景" aria-hidden="true">#</a></h2><h3 id="_1-基础实现：lora层与模型适配" tabindex="-1">1. 基础实现：LoRA层与模型适配 <a class="header-anchor" href="#_1-基础实现：lora层与模型适配" aria-hidden="true">#</a></h3><h4 id="（1）lora核心层设计" tabindex="-1">（1）LoRA核心层设计 <a class="header-anchor" href="#（1）lora核心层设计" aria-hidden="true">#</a></h4><p>LoRA的实现需构建专用层结构，通常包含基础接口与线性层扩展：</p><ul><li><strong>基础接口（LoRALayer）</strong>：定义秩r、缩放因子α、Dropout概率等核心参数，提供权重合并标记（merged），控制训练/推理时的权重状态；</li><li><strong>LoRA线性层（Linear）</strong>：继承基础接口与PyTorch nn.Linear，冻结原始权重（weight.requires_grad = False），新增可训练的lora_A、lora_B矩阵，实现前向传播时“原始权重计算+低秩矩阵调整”的叠加逻辑。</li></ul><h4 id="（2）训练-推理模式切换" tabindex="-1">（2）训练/推理模式切换 <a class="header-anchor" href="#（2）训练-推理模式切换" aria-hidden="true">#</a></h4><p>为平衡训练灵活性与推理效率，LoRA设计了模式切换机制：</p><ul><li><strong>训练模式</strong>：解除权重合并，单独计算原始权重输出与低秩矩阵输出，叠加后得到结果（result = F.linear(x, W) + (x @ A.T @ B.T) * 缩放因子），确保反向传播仅更新A、B参数；</li><li><strong>推理模式</strong>：合并低秩矩阵到原始权重（W.data += A×B * 缩放因子），后续推理直接使用合并后的权重，消除额外计算延迟</li></ul><h3 id="_2-transformer模型适配" tabindex="-1">2. Transformer模型适配 <a class="header-anchor" href="#_2-transformer模型适配" aria-hidden="true">#</a></h3><p>LoRA在Transformer中的应用遵循“<strong>好钢用在刀刃上</strong>”的原则，优先适配对任务影响最大的层，具体如下：</p><h4 id="（1）核心适配层：注意力层与前馈层" tabindex="-1">（1）核心适配层：注意力层与前馈层 <a class="header-anchor" href="#（1）核心适配层：注意力层与前馈层" aria-hidden="true">#</a></h4><ul><li><strong>自注意力层</strong>：主要适配查询投影（Wq）与值投影（Wv）矩阵——Wq决定模型“关注输入中的哪些信息”，Wv决定“基于关注信息输出什么内容”，微调这两个矩阵可高效提升任务适配能力；Wk（键投影）因与Wq功能重叠，Wo（输出投影）因影响间接，通常可选适配或不适配；</li><li><strong>前馈层（FFN）</strong>：适配升维矩阵（W1）与降维矩阵（W2），FFN负责特征提取与非线性映射，微调后可增强模型对任务特定特征的表达能力，大模型（如GPT-3）或生成任务（如对话生成）中常用。</li></ul><h4 id="（2）冻结层：嵌入层、layernorm与偏置" tabindex="-1">（2）冻结层：嵌入层、LayerNorm与偏置 <a class="header-anchor" href="#（2）冻结层：嵌入层、layernorm与偏置" aria-hidden="true">#</a></h4><ul><li><strong>嵌入层</strong>：权重大但微调时变动小，冻结可节省资源</li><li><strong>LayerNorm</strong>：参数少，直接更新成本低，无需LoRA辅助</li><li><strong>偏置（Bias）</strong>：梯度更新学习速度快，无需低秩矩阵适配</li></ul><h3 id="_3-实际应用案例" tabindex="-1">3. 实际应用案例 <a class="header-anchor" href="#_3-实际应用案例" aria-hidden="true">#</a></h3><h4 id="（1）文本分类任务" tabindex="-1">（1）文本分类任务 <a class="header-anchor" href="#（1）文本分类任务" aria-hidden="true">#</a></h4><p>基于Hugging Face Transformers与LoRA库，构建分类模型的流程为：</p><ol><li>加载预训练模型（如bert-base-uncased）与Tokenizer；</li><li>替换模型分类头为LoRA线性层，适配注意力层的Wq、Wv矩阵；</li><li>标记仅LoRA参数可训练（lora.mark_only_lora_as_trainable(model)），使用AdamW优化器训练；</li><li>训练完成后，仅保存LoRA参数（lora_state_dict），后续加载时结合预训练模型权重即可使用</li></ol><h4 id="（2）对话生成任务" tabindex="-1">（2）对话生成任务 <a class="header-anchor" href="#（2）对话生成任务" aria-hidden="true">#</a></h4><p>以GPT-2为例，适配流程为：</p><ol><li>加载GPT-2基础模型与Tokenizer</li><li>对Transformer块的注意力层（c_attn）适配LoRA，仅启用Wq、Wv的低秩调整（enable_lora=[True, False, True]）</li><li>适配前馈层的c_fc（升维）与c_proj（降维）矩阵</li><li>生成时使用合并权重的模型，确保输出流畅性与效率</li></ol><h2 id="五、lora改进版本与调参指南" tabindex="-1">五、LoRA改进版本与调参指南 <a class="header-anchor" href="#五、lora改进版本与调参指南" aria-hidden="true">#</a></h2><h3 id="_1-主流改进版本" tabindex="-1">1. 主流改进版本 <a class="header-anchor" href="#_1-主流改进版本" aria-hidden="true">#</a></h3><p>为进一步优化性能、效率与灵活性，LoRA衍生出多个变体，各版本核心差异与适配场景如下：</p><table><thead><tr><th>改进版本</th><th>核心改进</th><th>优势</th><th>劣势</th><th>适用场景</th></tr></thead><tbody><tr><td>LoRA+</td><td>为A、B矩阵设置不同学习率（ηB/ηA=4-16，B靠近输出，学习率更高）</td><td>加速收敛，利用“靠近输出的权重对梯度更敏感”的特性提升训练效率</td><td>需调整学习率比例，参数稍复杂</td><td>训练资源有限，追求快收敛</td></tr><tr><td>DoRA</td><td>将原始权重W分解为幅度（m，标量）与方向（V，单位矩阵），分别微调V（+A×B）与m</td><td>性能提升1%-3%，接近全微调，解决LoRA可能丢失幅度信息的问题</td><td>训练复杂度高（需分解/归一化），内存占用稍增</td><td>追求高性能，可接受一定复杂度</td></tr><tr><td>rsLoRA</td><td>动态调整秩的影响，将缩放因子改为α/√r，使秩r的变化更平滑</td><td>对r选择不敏感（调参简单），支持大r（64+），性能提升1%-2%</td><td>需额外计算缩放因子，逻辑稍复杂</td><td>复杂任务，尝试大秩</td></tr><tr><td>PiSSA</td><td>通过奇异值分解（SVD）初始化A、B（A=U×√∑，B=√∑×V^T，取前r个奇异值）</td><td>初始化接近最优解，收敛速度快20%-50%</td><td>前期需SVD计算，初始化成本高，禁用量化模型</td><td>需高性能且能承担初始化成本</td></tr><tr><td>GaLore</td><td>将全权重梯度投影到低秩空间（∇W≈P×Q），仅更新P、Q梯度</td><td>内存占用比标准LoRA再降50%-70%，性能接近全微调（差距0.5%-2%）</td><td>每步需投影，训练时间稍长</td><td>内存资源极低，优先保性能</td></tr></tbody></table><h3 id="_2-关键调参指南（以llama-factory为例）" tabindex="-1">2. 关键调参指南（以LLaMA-Factory为例） <a class="header-anchor" href="#_2-关键调参指南（以llama-factory为例）" aria-hidden="true">#</a></h3><h4 id="（1）核心超参数说明" tabindex="-1">（1）核心超参数说明 <a class="header-anchor" href="#（1）核心超参数说明" aria-hidden="true">#</a></h4><table><thead><tr><th>参数名</th><th>含义</th><th>建议值</th><th>默认值</th></tr></thead><tbody><tr><td>lora_rank（r）</td><td>低秩矩阵的秩，决定表达能力</td><td>简单任务：8-16；中等任务：32；复杂任务：64+</td><td>8</td></tr><tr><td>lora_alpha</td><td>缩放因子，平衡低秩矩阵对权重的影响</td><td>设为lora_rank的1-2倍（如r=16时α=32）</td><td>None</td></tr><tr><td>lora_dropout</td><td>Dropout概率，防止过拟合</td><td>小数据集（&lt;5K样本）：0.05-0.1；大数据集：0.0</td><td>0.0</td></tr><tr><td>lora_target</td><td>适配LoRA的模块名称</td><td>默认“q_proj,v_proj”；复杂任务加“k_proj,FFN”</td><td>all</td></tr><tr><td>use_rslora</td><td>是否启用rsLoRA动态秩调整</td><td>小r（4-16）：false；大r（32+）：true</td><td>false</td></tr><tr><td>use_dora</td><td>是否启用DoRA权重分解</td><td>复杂任务、追求高性能：true</td><td>false</td></tr></tbody></table><h4 id="（2）实用调参技巧" tabindex="-1">（2）实用调参技巧 <a class="header-anchor" href="#（2）实用调参技巧" aria-hidden="true">#</a></h4><ol><li><strong>从小秩开始尝试</strong>：多数任务从r=8或r=16起步，评估性能后再决定是否增大，避免资源浪费</li><li><strong>结合数据集大小选r</strong>：小数据集（&lt;5K样本）用小秩（r=8）防止过拟合，大数据集（&gt;50K样本）可试大秩（r=32+）提升表达力</li><li><strong>复杂任务组合策略</strong>：推理类任务可组合"增大r至32/64 + 启用rsLoRA + 适配FFN层"，平衡性能与效率</li></ol><h2 id="六、lora的优势与局限总结" tabindex="-1">六、LoRA的优势与局限总结 <a class="header-anchor" href="#六、lora的优势与局限总结" aria-hidden="true">#</a></h2><h3 id="_1-核心优势" tabindex="-1">1. 核心优势 <a class="header-anchor" href="#_1-核心优势" aria-hidden="true">#</a></h3><ul><li><strong>低资源友好</strong>：普通GPU可训练十亿参数级模型，降低大模型微调的硬件门槛</li><li><strong>高效灵活</strong>：参数量少、存储成本低、任务切换快，适合多任务场景与团队协作</li><li><strong>性能稳定</strong>：在多数任务上接近或超越全量微调，且避免"灾难性遗忘"，保留原始模型能力</li></ul><h3 id="_2-主要局限" tabindex="-1">2. 主要局限 <a class="header-anchor" href="#_2-主要局限" aria-hidden="true">#</a></h3><ul><li><strong>表现力限制</strong>：低秩假设可能无法覆盖某些复杂任务（如高精度推理）的需求，导致性能上限低于全量微调</li><li><strong>秩选择依赖经验</strong>：不同任务需调整秩r，缺乏统一标准，需通过实验试错</li><li><strong>初始化成本差异</strong>：部分改进版本（如PiSSA）需前期SVD计算，初始化成本较高</li></ul><h3 id="_3-相关资源与发展趋势" tabindex="-1">3. 相关资源与发展趋势 <a class="header-anchor" href="#_3-相关资源与发展趋势" aria-hidden="true">#</a></h3><p>LoRA目前已形成完善的生态，核心资源包括：</p><ul><li>论文：《LoRA: Low-Rank Adaptation of Large Language Models》（arXiv:2106.09685）</li><li>官方实现：Microsoft LoRA GitHub仓库</li><li>工具库：Hugging Face PEFT（Parameter-Efficient Fine-Tuning）库，集成LoRA及多种高效微调方法</li></ul><p>未来发展方向将围绕"进一步提升性能上限、降低调参复杂度、适配更多模型结构"展开，持续优化资源利用效率与任务适配能力。</p><hr><h2 id="总结" tabindex="-1">总结 <a class="header-anchor" href="#总结" aria-hidden="true">#</a></h2><p>LoRA 作为一种参数高效的微调技术，通过低秩分解显著降低了大模型微调的门槛，在保持性能的同时大幅减少了计算和存储成本。其模块化设计使得多任务部署变得灵活高效，是当前大模型时代的重要技术突破。</p><p>无论是学术研究还是工业应用，LoRA 都为资源受限环境下的大模型微调提供了可行的解决方案，推动了参数高效微调技术的发展。</p></div><!--]--><!--]--></div></article><div class="prose m-auto mt-8 mb-8"><a class="font-mono no-underline opacity-50 hover:opacity-75">cd ..</a></div><!--]--><div class="mt-10 mb-6 prose m-auto opacity-50 flex"><span class="text-sm"><a target="_blank" href="https://beian.miit.gov.cn" style="color:inherit">浙ICP备2021022773号 &nbsp;&nbsp; </a>2022-PRESENT © ZhengKe</span><div class="flex-auto"></div></div></main><!--]--></div><link rel="stylesheet" href="/assets/app.baf61519.css"><link rel="stylesheet" href="/assets/Post.63afffbd.css"></body></html>