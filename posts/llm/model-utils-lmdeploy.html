<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="author" content="ZhengKe"><meta http-equiv="X-UA-Compatible" content="chrome=1"><meta name="revisit-after" content="7 days"><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="icon" href="/favicon.svg" type="image/svg+xml"><script src="/lib/mermaid.min.js"></script><meta name="msapplication-TileColor" content="#ffffff"><meta name="theme-color" content="#ffffff"><title>LMDeploy：高效的大语言模型推理和服务部署工具</title><script>(()=>{var e=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,a=localStorage.getItem("vueuse-color-scheme")||"auto";("dark"===a||e&&"light"!==a)&&document.documentElement.classList.toggle("dark",!0)})()</script><script type="module" crossorigin="" src="/assets/app.95384a36.js"></script><style>*,:after,:before{box-sizing:border-box;border-width:0;border-style:solid;border-color:currentColor}html{line-height:1.5;-webkit-text-size-adjust:100%;-moz-tab-size:4;tab-size:4;font-family:ui-sans-serif,system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,"Apple Color Emoji","Segoe UI Emoji",Segoe UI Symbol,"Noto Color Emoji"}body{margin:0;line-height:inherit}h1,h2,h3{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}strong{font-weight:bolder}code,pre{font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1em}table{text-indent:0;border-color:inherit;border-collapse:collapse}h1,h2,h3,p,pre{margin:0}ul{list-style:none;margin:0;padding:0}img,svg{display:block;vertical-align:middle}img{max-width:100%;height:auto}:root{--c-bg:#fff;--c-scrollbar:#eee;--c-scrollbar-hover:#bbb}html{background-color:var(--c-bg)}html{overflow:scroll}*{scrollbar-color:var(--c-scrollbar) var(--c-bg)}.prose{color:var(--fg);max-width:75ch;font-size:1rem;line-height:1.75}.prose a{color:var(--fg-deeper);text-decoration:none;font-weight:500}.prose strong{color:var(--fg-deep);font-weight:600}.prose ul>li{position:relative;padding-left:1.75em}.prose ul>li:before{content:"";position:absolute;background-color:#d1d5db;border-radius:50%;width:.375em;height:.375em;top:.6875em;left:.25em}.prose h1{color:var(--fg-deeper);font-weight:800;font-size:2.25em;margin-top:0;margin-bottom:.8888889em;line-height:1.1111111}.prose h2{color:var(--fg-deep);font-weight:700;font-size:1.5em;margin-top:2em;margin-bottom:1em;line-height:1.3333333}.prose h3{color:inherit;font-weight:600;font-size:1.25em;margin-top:1.6em;margin-bottom:.6em;line-height:1.6}.prose code{color:var(--fg-deep);font-weight:600;font-size:.875em}.prose code:before{content:"`"}.prose code:after{content:"`"}.prose pre{color:#e5e7eb;background-color:#1f2937;overflow-x:auto;font-size:.875em;line-height:1.7142857;margin-top:1.7142857em;margin-bottom:1.7142857em;border-radius:.375rem;padding:.8571429em 1.1428571em}.prose pre code{background-color:transparent;border-width:0;border-radius:0;padding:0;font-weight:400;color:inherit;font-size:inherit;font-family:inherit;line-height:inherit}.prose pre code:before{content:none}.prose pre code:after{content:none}.prose table{width:100%;table-layout:auto;text-align:left;margin-top:2em;margin-bottom:2em;font-size:.875em;line-height:1.7142857}.prose thead{color:#111827;font-weight:600;border-bottom-width:1px;border-bottom-color:#d1d5db}.prose thead th{vertical-align:bottom;padding-right:.5714286em;padding-bottom:.5714286em;padding-left:.5714286em}.prose tbody tr{border-bottom-width:1px;border-bottom-color:#e5e7eb}.prose tbody tr:last-child{border-bottom-width:0}.prose tbody td{vertical-align:top;padding:.5714286em}.prose p{margin-top:1.25em;margin-bottom:1.25em}.prose img{margin-top:2em;margin-bottom:2em}.prose ul{margin-top:1.25em;margin-bottom:1.25em;list-style-type:none}.prose li{margin-top:.5em;margin-bottom:.5em}.prose thead th:first-child{padding-left:0}.prose thead th:last-child{padding-right:0}.prose tbody td:first-child{padding-left:0}.prose tbody td:last-child{padding-right:0}.prose>:first-child{margin-top:0}.prose>:last-child{margin-bottom:0}:root{--prism-scheme:light;--prism-foreground:#6e6e6e;--prism-background:#f4f4f4;--prism-comment:#a8a8a8;--prism-string:#555555;--prism-literal:#333333;--prism-keyword:#000000;--prism-function:#4f4f4f;--prism-deleted:#333333;--prism-class:#333333;--prism-builtin:#757575;--prism-property:#333333;--prism-namespace:#4f4f4f;--prism-punctuation:#ababab;--prism-decorator:var(--prism-class);--prism-operator:var(--prism-punctuation);--prism-number:var(--prism-literal);--prism-boolean:var(--prism-literal);--prism-variable:var(--prism-literal);--prism-constant:var(--prism-literal);--prism-symbol:var(--prism-literal);--prism-interpolation:var(--prism-literal);--prism-selector:var(--prism-keyword);--prism-keyword-control:var(--prism-keyword);--prism-regex:var(--prism-string);--prism-json-property:var(--prism-property);--prism-inline-background:var(--prism-background);--prism-comment-style:italic;--prism-url-decoration:underline;--prism-line-number:#a5a5a5;--prism-line-number-gutter:#333333;--prism-line-highlight-background:#eeeeee;--prism-selection-background:#dddddd;--prism-marker-color:var(--prism-foreground);--prism-marker-opacity:.4;--prism-marker-font-size:.8em;--prism-font-size:1em;--prism-line-height:1.5em;--prism-font-family:monospace;--prism-inline-font-size:var(--prism-font-size);--prism-block-font-size:var(--prism-font-size);--prism-tab-size:2;--prism-block-padding-x:1em;--prism-block-padding-y:1em;--prism-block-margin-x:0;--prism-block-margin-y:.5em;--prism-block-radius:.3em;--prism-inline-padding-x:.3em;--prism-inline-padding-y:.1em;--prism-inline-radius:.3em}code[class*=language-],pre[class*=language-]{font-size:var(--prism-font-size);font-family:var(--prism-font-family);direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;line-height:var(--prism-line-height);-moz-tab-size:var(--prism-tab-size);-o-tab-size:var(--prism-tab-size);tab-size:var(--prism-tab-size);-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;color:var(--prism-foreground)!important}pre[class*=language-]{font-size:var(--prism-block-font-size);padding:var(--prism-block-padding-y) var(--prism-block-padding-x);margin:var(--prism-block-margin-y) var(--prism-block-margin-x);border-radius:var(--prism-block-radius);overflow:auto;background:var(--prism-background)}code[class*=language-] ::-moz-selection,code[class*=language-]::-moz-selection,pre[class*=language-] ::-moz-selection,pre[class*=language-]::-moz-selection{background:var(--prism-selection-background)}code[class*=language-] ::selection,code[class*=language-]::selection,pre[class*=language-] ::selection,pre[class*=language-]::selection{background:var(--prism-selection-background)}.token.comment{color:var(--prism-comment);font-style:var(--prism-comment-style)}.token.interpolation{color:var(--prism-interpolation)}.token.string{color:var(--prism-string)}.token.punctuation{color:var(--prism-punctuation)}.token.operator{color:var(--prism-operator)}.token.boolean{color:var(--prism-boolean)}.token.number{color:var(--prism-number)}.token.variable{color:var(--prism-variable)}.token.keyword{color:var(--prism-keyword)}.token.function{color:var(--prism-function)}.token.class-name{color:var(--prism-class)}.token.builtin{color:var(--prism-builtin)}:root{--prism-font-size:.9rem;--prism-font-family:"Fira Code",monospace}:root{--prism-font-family:"Input Mono",monospace}html:not(.dark){--prism-foreground:#393a34;--prism-background:#fbfbfb;--prism-comment:#a0ada0;--prism-string:#b56959;--prism-literal:#2f8a89;--prism-number:#296aa3;--prism-keyword:#1c6b48;--prism-function:#6c7834;--prism-boolean:#1c6b48;--prism-constant:#a65e2b;--prism-deleted:#a14f55;--prism-class:#2993a3;--prism-builtin:#ab5959;--prism-property:#b58451;--prism-namespace:#b05a78;--prism-punctuation:#8e8f8b;--prism-decorator:#bd8f8f;--prism-regex:#ab5e3f;--prism-json-property:#698c96}.prose{--fg:#555;--fg-deep:#222;--fg-deeper:#000;color:var(--fg)}.prose a{font-weight:inherit;text-decoration:none;border-bottom:1px solid rgba(125,125,125,.3);transition:border .3s ease-in-out}.prose a:hover{border-bottom:1px solid var(--fg)}a.header-anchor{float:left;margin-top:.125em;margin-left:-1.2em;padding-right:.5em;font-size:.85em;opacity:0;text-decoration:none;border:0!important}a.header-anchor:focus,a.header-anchor:hover{text-decoration:none}h2:focus .header-anchor,h2:hover .header-anchor,h3:focus .header-anchor,h3:hover .header-anchor{opacity:.5}.absolute{position:absolute}.z-40{z-index:40}.m-6{margin:1.5rem}.m-auto{margin:auto}.\!-mt-2{margin-top:-.5rem!important}.mb-0{margin-bottom:0}.mb-6{margin-bottom:1.5rem}.mb-8{margin-bottom:2rem}.mt-10{margin-top:2.5rem}.mt-8{margin-top:2rem}.h-10{height:2.5rem}.w-10{width:2.5rem}.flex{display:flex}.flex-auto{flex:1 1 auto}.select-none{user-select:none}.px-7{padding-left:1.75rem;padding-right:1.75rem}.py-10{padding-top:2.5rem;padding-bottom:2.5rem}.font-mono{font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace}.font-sans{font-family:Inter,Inter var,system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}.text-sm{font-size:.875rem;line-height:1.25rem}.text-gray-700{--un-text-opacity:1;color:rgba(55,65,81,var(--un-text-opacity))}.no-underline{text-decoration:none}.opacity-50{opacity:.5}.hover\:opacity-75:hover{opacity:.75}.outline-none{outline:2px solid transparent;outline-offset:2px}@media (max-width:768px){.lt-md\:hidden{display:none}}@media (min-width:768px){.md\:hidden{display:none}}@media (min-width:1024px){.lg\:fixed{position:fixed}}.nav[data-v-568133b8]{padding:2rem;width:100%;display:grid;grid-template-columns:auto max-content;box-sizing:border-box}.nav[data-v-568133b8]>*{margin:auto}.nav a[data-v-568133b8]{cursor:pointer;text-decoration:none;color:inherit;transition:opacity .2s ease;opacity:.6;outline:0}.nav a[data-v-568133b8]:hover{opacity:1;text-decoration-color:inherit}.nav .right[data-v-568133b8]{display:grid;grid-gap:1.2rem;grid-auto-flow:column}.nav .right[data-v-568133b8]>*{margin:auto}.mathjax-container[data-v-011ba505]{color:inherit;font-family:inherit;line-height:inherit}</style><link rel="preload" href="/assets/app.baf61519.css" as="style"><link rel="modulepreload" crossorigin="" href="/assets/Model-Utils-LMDeploy.e2611358.js"><link rel="modulepreload" crossorigin="" href="/assets/Post.14d9d5ce.js"><link rel="preload" href="/assets/Post.63afffbd.css" as="style"><meta property="og:title" content="LMDeploy：高效的大语言模型推理和服务部署工具"><meta name="head:count" content="1"></head><body class="font-sans text-gray-700 dark:text-gray-200"><div id="app" data-server-rendered="true"><!--[--><header class="header z-40" data-v-568133b8=""><a href="/" class="w-10 h-10 absolute lg:fixed m-6 select-none outline-none" focusable="false" data-v-568133b8=""><img src="/logo-dark.svg" alt="logo" style="display:none" data-v-568133b8=""><img src="/logo.svg" alt="logo" data-v-568133b8=""></a><nav class="nav" data-v-568133b8=""><div class="spacer" data-v-568133b8=""></div><div class="right" data-v-568133b8=""><a href="/posts" class="" data-v-568133b8=""><span class="lt-md:hidden" data-v-568133b8="">Blog</span><svg style="vertical-align:sub" class="inline md:hidden" width="1.2em" height="1.2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24" data-v-568133b8=""><path fill="currentColor" d="M20 22H4a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1h16a1 1 0 0 1 1 1v18a1 1 0 0 1-1 1m-1-2V4H5v16zM7 6h4v4H7zm0 6h10v2H7zm0 4h10v2H7zm6-9h4v2h-4z"></path></svg></a><a href="/llm" class="lt-md:hidden" data-v-568133b8="">LLM </a><a href="/notes" class="" title="Notes" data-v-568133b8=""><span class="lt-md:hidden" data-v-568133b8="">Notes</span><svg style="vertical-align:sub" class="inline md:hidden" width="1.2em" height="1.2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24" data-v-568133b8=""><path fill="currentColor" d="m17.85 11.698l-.708-.707l-9.9 9.9H3v-4.243L14.314 5.334l5.657 5.657a1 1 0 0 1 0 1.414L12.9 19.477l-1.415-1.415zm-2.122-2.121l-1.414-1.414L5 17.477v1.414h1.414zm2.828-7.071l2.829 2.828a1 1 0 0 1 0 1.415L19.97 8.163L15.728 3.92l1.414-1.414a1 1 0 0 1 1.414 0"></path></svg></a><a href="https://github.com/ZhengKe0110" target="_blank" title="GitHub" class="lt-md:hidden" data-v-568133b8=""><svg style="vertical-align:sub" class="inline" width="1.2em" height="1.2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24" data-v-568133b8=""><path fill="currentColor" d="M10.07 20.503a1 1 0 0 0-1.18-.983c-1.31.24-2.963.276-3.402-.958a5.7 5.7 0 0 0-1.837-2.415a1 1 0 0 1-.167-.11a1 1 0 0 0-.93-.645h-.005a1 1 0 0 0-1 .995c-.004.815.81 1.338 1.141 1.514a4.4 4.4 0 0 1 .924 1.36c.365 1.023 1.423 2.576 4.466 2.376l.003.098l.004.268a1 1 0 0 0 2 0l-.005-.318c-.005-.19-.012-.464-.012-1.182M20.737 5.377q.049-.187.09-.42a6.3 6.3 0 0 0-.408-3.293a1 1 0 0 0-.615-.58c-.356-.12-1.67-.357-4.184 1.25a13.9 13.9 0 0 0-6.354 0C6.762.75 5.455.966 5.102 1.079a1 1 0 0 0-.631.584a6.3 6.3 0 0 0-.404 3.357q.037.191.079.354a6.27 6.27 0 0 0-1.256 3.83a8 8 0 0 0 .043.921c.334 4.603 3.334 5.984 5.424 6.459a5 5 0 0 0-.118.4a1 1 0 0 0 1.942.479a1.7 1.7 0 0 1 .468-.878a1 1 0 0 0-.546-1.745c-3.454-.395-4.954-1.802-5.18-4.899a7 7 0 0 1-.033-.738a4.26 4.26 0 0 1 .92-2.713a3 3 0 0 1 .195-.231a1 1 0 0 0 .188-1.025a3.4 3.4 0 0 1-.155-.555a4.1 4.1 0 0 1 .079-1.616a7.5 7.5 0 0 1 2.415 1.18a1 1 0 0 0 .827.133a11.8 11.8 0 0 1 6.173.001a1 1 0 0 0 .83-.138a7.6 7.6 0 0 1 2.406-1.19a4 4 0 0 1 .087 1.578a3.2 3.2 0 0 1-.169.607a1 1 0 0 0 .188 1.025c.078.087.155.18.224.268A4.12 4.12 0 0 1 20 9.203a7 7 0 0 1-.038.777c-.22 3.056-1.725 4.464-5.195 4.86a1 1 0 0 0-.546 1.746a1.63 1.63 0 0 1 .466.908a3 3 0 0 1 .093.82v2.333c-.01.648-.01 1.133-.01 1.356a1 1 0 1 0 2 0c0-.217 0-.692.01-1.34v-2.35a5 5 0 0 0-.155-1.311a4 4 0 0 0-.116-.416a6.51 6.51 0 0 0 5.445-6.424A9 9 0 0 0 22 9.203a6.13 6.13 0 0 0-1.263-3.826"></path></svg></a><a class="select-none" title="Toggle Color Scheme" data-v-568133b8=""><svg style="vertical-align:sub;display:none" class="inline" width="1.2em" height="1.2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="currentColor" d="M10 7a7 7 0 0 0 12 4.9v.1c0 5.523-4.477 10-10 10S2 17.523 2 12S6.477 2 12 2h.1A6.98 6.98 0 0 0 10 7m-6 5a8 8 0 0 0 15.062 3.762A9 9 0 0 1 8.238 4.938A8 8 0 0 0 4 12"></path></svg><svg style="vertical-align:sub" class="inline" width="1.2em" height="1.2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="currentColor" d="M12 18a6 6 0 1 1 0-12a6 6 0 0 1 0 12m0-2a4 4 0 1 0 0-8a4 4 0 0 0 0 8M11 1h2v3h-2zm0 19h2v3h-2zM3.515 4.929l1.414-1.414L7.05 5.636L5.636 7.05zM16.95 18.364l1.414-1.414l2.121 2.121l-1.414 1.414zm2.121-14.85l1.414 1.415l-2.121 2.121l-1.414-1.414zM5.636 16.95l1.414 1.414l-2.121 2.121l-1.414-1.414zM23 11v2h-3v-2zM4 11v2H1v-2z"></path></svg></a></div></nav></header><main class="px-7 py-10"><!--[--><div class="prose m-auto mb-8"><h1 class="mb-0">LMDeploy：高效的大语言模型推理和服务部署工具</h1><p class="opacity-50 !-mt-2">Aug 17<!----></p><!----></div><article><div class="mathjax-container" data-v-011ba505=""><!--[--><!--[--><div class="prose m-auto"><p>LMDeploy 是一个由上海人工智能实验室开发的高效、友好的大语言模型（LLMs）和视觉-语言模型（VLMs）部署工具箱，功能涵盖了量化、推理和服务。本文将基于 24GB 显卡环境，详细介绍如何使用 LMDeploy 部署通义千问 Qwen2.5-7B-Instruct 模型。</p><h2 id="核心特性" tabindex="-1">核心特性 <a class="header-anchor" href="#核心特性" aria-hidden="true">#</a></h2><h3 id="高效推理-efficient-inference" tabindex="-1">高效推理 (Efficient Inference) <a class="header-anchor" href="#高效推理-efficient-inference" aria-hidden="true">#</a></h3><ul><li><strong>Persistent Batch</strong>：实现连续批处理，优化吞吐量</li><li><strong>Blocked K/V Cache</strong>：内存高效的缓存管理机制</li><li><strong>动态拆分和融合</strong>：智能的计算图优化</li><li><strong>张量并行</strong>：支持多 GPU 并行推理</li><li><strong>高效计算核心</strong>：优化的 CUDA kernel 实现</li></ul><h3 id="可靠量化-reliable-quantization" tabindex="-1">可靠量化 (Reliable Quantization) <a class="header-anchor" href="#可靠量化-reliable-quantization" aria-hidden="true">#</a></h3><ul><li><strong>权重量化</strong>：支持 4bit/8bit 权重量化</li><li><strong>AWQ 量化</strong>：激活感知权重量化技术</li><li><strong>性能优势</strong>：4bit 量化模型推理效率是 FP16 的 2.4 倍</li></ul><h3 id="便捷服务-convenient-service" tabindex="-1">便捷服务 (Convenient Service) <a class="header-anchor" href="#便捷服务-convenient-service" aria-hidden="true">#</a></h3><ul><li><strong>请求分发服务</strong>：支持多模型、多机、多卡推理服务</li><li><strong>OpenAI 兼容 API</strong>：无缝接入现有应用</li><li><strong>多种部署方式</strong>：支持 Docker、Kubernetes 等部署方案</li></ul><h3 id="卓越兼容性-outstanding-compatibility" tabindex="-1">卓越兼容性 (Outstanding Compatibility) <a class="header-anchor" href="#卓越兼容性-outstanding-compatibility" aria-hidden="true">#</a></h3><ul><li><strong>多框架支持</strong>：TurboMind 引擎和 PyTorch 引擎</li><li><strong>广泛模型支持</strong>：涵盖主流开源大语言模型</li><li><strong>多平台适配</strong>：Linux、Windows、华为昇腾等平台</li></ul><h2 id="系统要求" tabindex="-1">系统要求 <a class="header-anchor" href="#系统要求" aria-hidden="true">#</a></h2><h3 id="硬件要求" tabindex="-1">硬件要求 <a class="header-anchor" href="#硬件要求" aria-hidden="true">#</a></h3><ul><li><strong>GPU</strong>: 24GB+ 显存（RTX 4090/3090、A10/A100、RTX 6000）</li><li><strong>内存</strong>: 32GB+（推荐 64GB）</li><li><strong>存储</strong>: 30GB+ 可用空间（FP16 模型约 15GB，4bit 量化后约 4GB）</li></ul><h3 id="软件要求" tabindex="-1">软件要求 <a class="header-anchor" href="#软件要求" aria-hidden="true">#</a></h3><ul><li><strong>Python</strong>: 3.10+</li><li><strong>CUDA</strong>: 11.3+ 或 12.x（推荐）</li><li><strong>操作系统</strong>: Linux 或 Windows</li></ul><h3 id="性能参考" tabindex="-1">性能参考 <a class="header-anchor" href="#性能参考" aria-hidden="true">#</a></h3><table><thead><tr><th>GPU 型号</th><th>FP16 推理速度</th><th>4bit AWQ 推理速度</th><th>备注</th></tr></thead><tbody><tr><td>RTX 4090</td><td>35-50 tokens/s</td><td>50-80 tokens/s</td><td>4bit 性能提升</td></tr><tr><td>RTX 3090</td><td>28-40 tokens/s</td><td>40-65 tokens/s</td><td>显存占用减半</td></tr><tr><td>A10/A100</td><td>30-60 tokens/s</td><td>45-90 tokens/s</td><td>依据具体配置</td></tr></tbody></table><h2 id="环境准备" tabindex="-1">环境准备 <a class="header-anchor" href="#环境准备" aria-hidden="true">#</a></h2><h3 id="_1-创建-python-环境" tabindex="-1">1. 创建 Python 环境 <a class="header-anchor" href="#_1-创建-python-环境" aria-hidden="true">#</a></h3><pre class="language-bash"><code class="language-bash"><span class="token comment"># 创建独立的conda环境</span>
conda create <span class="token parameter variable">-n</span> lmdeploy <span class="token assign-left variable">python</span><span class="token operator">=</span><span class="token number">3.10</span> <span class="token parameter variable">-y</span>
conda activate lmdeploy
</code></pre><h3 id="_2-安装-lmdeploy" tabindex="-1">2. 安装 LMDeploy <a class="header-anchor" href="#_2-安装-lmdeploy" aria-hidden="true">#</a></h3><pre class="language-bash"><code class="language-bash"><span class="token comment"># 克隆仓库</span>
<span class="token function">git</span> clone https://github.com/InternLM/lmdeploy.git
<span class="token builtin class-name">cd</span> lmdeploy

<span class="token comment"># 安装依赖并编译</span>
pip <span class="token function">install</span> <span class="token parameter variable">-e</span> <span class="token builtin class-name">.</span>
</code></pre><h3 id="_3-验证安装" tabindex="-1">3. 验证安装 <a class="header-anchor" href="#_3-验证安装" aria-hidden="true">#</a></h3><pre class="language-bash"><code class="language-bash"><span class="token comment"># 检查安装状态</span>
python <span class="token parameter variable">-c</span> <span class="token string">"import lmdeploy; print(lmdeploy.__version__)"</span>

<span class="token comment"># 查看环境信息</span>
lmdeploy check_env
</code></pre><h2 id="使用-modelscope-下载-qwen2-5-7b-instruct-模型" tabindex="-1">使用 ModelScope 下载 Qwen2.5-7B-Instruct 模型 <a class="header-anchor" href="#使用-modelscope-下载-qwen2-5-7b-instruct-模型" aria-hidden="true">#</a></h2><p><strong>关于 Qwen2.5-7B-Instruct 模型</strong>：Qwen2.5-7B-Instruct 是阿里巴巴推出的 70 亿参数指令跟随模型，基于 Qwen2.5 架构，在 LMDeploy 中有完整的官方支持。该模型在保持高质量对话能力的同时，对硬件要求适中，非常适合 24GB 显存的部署环境。</p><pre class="language-bash"><code class="language-bash"><span class="token comment"># 安装modelscope</span>
pip <span class="token function">install</span> modelscope

<span class="token comment"># 使用命令行下载模型</span>
modelscope download <span class="token parameter variable">--model</span> qwen/Qwen2.5-7B-Instruct <span class="token parameter variable">--cache_dir</span> ./models
</code></pre><h2 id="离线推理部署" tabindex="-1">离线推理部署 <a class="header-anchor" href="#离线推理部署" aria-hidden="true">#</a></h2><h3 id="_1-基础推理示例" tabindex="-1">1. 基础推理示例 <a class="header-anchor" href="#_1-基础推理示例" aria-hidden="true">#</a></h3><pre class="language-python"><code class="language-python"><span class="token comment"># basic_inference.py</span>
<span class="token keyword">from</span> lmdeploy <span class="token keyword">import</span> pipeline

<span class="token comment"># 创建推理管道</span>
pipe <span class="token operator">=</span> pipeline<span class="token punctuation">(</span><span class="token string">"qwen/Qwen2.5-7B-Instruct"</span><span class="token punctuation">)</span>

<span class="token comment"># 单轮对话</span>
response <span class="token operator">=</span> pipe<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">"你好，请介绍一下自己"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"回答:"</span><span class="token punctuation">,</span> response<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>text<span class="token punctuation">)</span>

<span class="token comment"># 多轮对话</span>
messages <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token punctuation">{</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"user"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> <span class="token string">"什么是人工智能？"</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"assistant"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> <span class="token string">"人工智能（AI）是计算机科学的一个分支..."</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"user"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> <span class="token string">"它有哪些应用领域？"</span><span class="token punctuation">}</span>
<span class="token punctuation">]</span>
response <span class="token operator">=</span> pipe<span class="token punctuation">(</span>messages<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"回答:"</span><span class="token punctuation">,</span> response<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>text<span class="token punctuation">)</span>
</code></pre><p><img src="/images/notes/llm/Model-Utils-LMDeploy/qwen_success.png" alt="成功"></p><h3 id="_2-高级配置示例" tabindex="-1">2. 高级配置示例 <a class="header-anchor" href="#_2-高级配置示例" aria-hidden="true">#</a></h3><pre class="language-python"><code class="language-python"><span class="token comment"># advanced_inference.py</span>
<span class="token keyword">from</span> lmdeploy <span class="token keyword">import</span> pipeline<span class="token punctuation">,</span> TurbomindEngineConfig
<span class="token keyword">from</span> lmdeploy<span class="token punctuation">.</span>messages <span class="token keyword">import</span> GenerationConfig

<span class="token comment"># 配置TurboMind引擎（推荐用于24GB显卡）</span>
backend_config <span class="token operator">=</span> TurbomindEngineConfig<span class="token punctuation">(</span>
    max_batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>           <span class="token comment"># 最大批处理大小</span>
    enable_prefix_caching<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>  <span class="token comment"># 启用前缀缓存</span>
    cache_max_entry_count<span class="token operator">=</span><span class="token number">0.8</span><span class="token punctuation">,</span>   <span class="token comment"># KV缓存占用显存比例</span>
    session_len<span class="token operator">=</span><span class="token number">8192</span><span class="token punctuation">,</span>            <span class="token comment"># 最大序列长度</span>
    tp<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>                        <span class="token comment"># 张量并行度（单卡设为1）</span>
<span class="token punctuation">)</span>

<span class="token comment"># 创建推理管道</span>
pipe <span class="token operator">=</span> pipeline<span class="token punctuation">(</span>
    <span class="token string">'qwen/Qwen2.5-7B-Instruct'</span><span class="token punctuation">,</span>
    backend_config<span class="token operator">=</span>backend_config
<span class="token punctuation">)</span>

<span class="token comment"># 生成配置</span>
gen_config <span class="token operator">=</span> GenerationConfig<span class="token punctuation">(</span>
    max_new_tokens<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span>    <span class="token comment"># 最大生成token数</span>
    top_p<span class="token operator">=</span><span class="token number">0.8</span><span class="token punctuation">,</span>             <span class="token comment"># nucleus采样参数</span>
    top_k<span class="token operator">=</span><span class="token number">40</span><span class="token punctuation">,</span>              <span class="token comment"># top-k采样参数</span>
    temperature<span class="token operator">=</span><span class="token number">0.7</span><span class="token punctuation">,</span>       <span class="token comment"># 温度参数</span>
    repetition_penalty<span class="token operator">=</span><span class="token number">1.1</span>  <span class="token comment"># 重复惩罚</span>
<span class="token punctuation">)</span>

<span class="token comment"># 推理测试</span>
prompts <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token string">"请解释什么是机器学习"</span><span class="token punctuation">,</span>
    <span class="token string">"写一个Python快速排序算法"</span><span class="token punctuation">,</span>
    <span class="token string">"推荐几本人工智能入门书籍"</span>
<span class="token punctuation">]</span>

responses <span class="token operator">=</span> pipe<span class="token punctuation">(</span>prompts<span class="token punctuation">,</span> gen_config<span class="token operator">=</span>gen_config<span class="token punctuation">)</span>
<span class="token keyword">for</span> i<span class="token punctuation">,</span> resp <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>responses<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"问题 </span><span class="token interpolation"><span class="token punctuation">{</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">: </span><span class="token interpolation"><span class="token punctuation">{</span>prompts<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"回答: </span><span class="token interpolation"><span class="token punctuation">{</span>resp<span class="token punctuation">.</span>text<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"-"</span> <span class="token operator">*</span> <span class="token number">50</span><span class="token punctuation">)</span>
</code></pre><p><img src="/images/notes/llm/Model-Utils-LMDeploy/qwen_success2.png" alt="成功"></p><h2 id="api-服务部署" tabindex="-1">API 服务部署 <a class="header-anchor" href="#api-服务部署" aria-hidden="true">#</a></h2><h3 id="_1-启动基础-api-服务" tabindex="-1">1. 启动基础 API 服务 <a class="header-anchor" href="#_1-启动基础-api-服务" aria-hidden="true">#</a></h3><pre class="language-bash"><code class="language-bash"><span class="token comment"># 启动OpenAI兼容的API服务</span>
lmdeploy serve api_server ./models/qwen/Qwen2.5-7B-Instruct <span class="token punctuation">\</span>
    --server-port <span class="token number">23333</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--backend</span> turbomind <span class="token punctuation">\</span>
    <span class="token parameter variable">--tp</span> <span class="token number">1</span>
</code></pre><h3 id="_2-高级配置启动" tabindex="-1">2. 高级配置启动 <a class="header-anchor" href="#_2-高级配置启动" aria-hidden="true">#</a></h3><pre class="language-bash"><code class="language-bash"><span class="token comment"># 完整配置的API服务启动</span>
lmdeploy serve api_server ./models/qwen/Qwen2.5-7B-Instruct <span class="token punctuation">\</span>
    --server-port <span class="token number">23333</span> <span class="token punctuation">\</span>
    --server-name <span class="token number">0.0</span>.0.0 <span class="token punctuation">\</span>
    <span class="token parameter variable">--backend</span> turbomind <span class="token punctuation">\</span>
    <span class="token parameter variable">--tp</span> <span class="token number">1</span> <span class="token punctuation">\</span>
    --max-batch-size <span class="token number">32</span> <span class="token punctuation">\</span>
    --cache-max-entry-count <span class="token number">0.8</span> <span class="token punctuation">\</span>
    --session-len <span class="token number">8192</span> <span class="token punctuation">\</span>
    --enable-prefix-caching <span class="token punctuation">\</span>
    --log-level INFO
</code></pre><h3 id="_3-服务配置参数说明" tabindex="-1">3. 服务配置参数说明 <a class="header-anchor" href="#_3-服务配置参数说明" aria-hidden="true">#</a></h3><table><thead><tr><th>参数</th><th>说明</th><th>默认值</th><th>推荐设置</th></tr></thead><tbody><tr><td><code>--server-port</code></td><td>API 服务端口</td><td>23333</td><td>23333</td></tr><tr><td><code>--server-name</code></td><td>绑定 IP 地址</td><td>127.0.0.1</td><td>0.0.0.0</td></tr><tr><td><code>--backend</code></td><td>推理引擎</td><td>auto</td><td>turbomind</td></tr><tr><td><code>--tp</code></td><td>张量并行度</td><td>1</td><td>1（单卡）</td></tr><tr><td><code>--max-batch-size</code></td><td>最大批处理</td><td>128</td><td>32</td></tr><tr><td><code>--cache-max-entry-count</code></td><td>KV 缓存比例</td><td>0.8</td><td>0.8</td></tr><tr><td><code>--session-len</code></td><td>最大序列长度</td><td>auto</td><td>8192</td></tr><tr><td><code>--enable-prefix-caching</code></td><td>前缀缓存</td><td>False</td><td>True</td></tr></tbody></table><h3 id="_4-客户端调用示例" tabindex="-1">4. 客户端调用示例 <a class="header-anchor" href="#_4-客户端调用示例" aria-hidden="true">#</a></h3><pre class="language-python"><code class="language-python"><span class="token comment"># client_example.py</span>
<span class="token keyword">from</span> openai <span class="token keyword">import</span> OpenAI

<span class="token comment"># 创建客户端</span>
client <span class="token operator">=</span> OpenAI<span class="token punctuation">(</span>
    api_key<span class="token operator">=</span><span class="token string">'sk-any-key'</span><span class="token punctuation">,</span>  <span class="token comment"># 可以是任意字符串</span>
    base_url<span class="token operator">=</span><span class="token string">"http://localhost:23333/v1"</span>
<span class="token punctuation">)</span>

<span class="token comment"># 获取模型列表</span>
models <span class="token operator">=</span> client<span class="token punctuation">.</span>models<span class="token punctuation">.</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
model_name <span class="token operator">=</span> models<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">id</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"可用模型: </span><span class="token interpolation"><span class="token punctuation">{</span>model_name<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>

<span class="token comment"># 单轮对话</span>
response <span class="token operator">=</span> client<span class="token punctuation">.</span>chat<span class="token punctuation">.</span>completions<span class="token punctuation">.</span>create<span class="token punctuation">(</span>
    model<span class="token operator">=</span>model_name<span class="token punctuation">,</span>
    messages<span class="token operator">=</span><span class="token punctuation">[</span>
        <span class="token punctuation">{</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"system"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> <span class="token string">"你是一个有用的AI助手。"</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token punctuation">{</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"user"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> <span class="token string">"请介绍一下量子计算的基本原理"</span><span class="token punctuation">}</span>
    <span class="token punctuation">]</span><span class="token punctuation">,</span>
    temperature<span class="token operator">=</span><span class="token number">0.7</span><span class="token punctuation">,</span>
    max_tokens<span class="token operator">=</span><span class="token number">1024</span><span class="token punctuation">,</span>
    top_p<span class="token operator">=</span><span class="token number">0.8</span>
<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"AI回答:"</span><span class="token punctuation">,</span> response<span class="token punctuation">.</span>choices<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>message<span class="token punctuation">.</span>content<span class="token punctuation">)</span>

<span class="token comment"># 流式对话</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\n流式对话演示:"</span><span class="token punctuation">)</span>
stream <span class="token operator">=</span> client<span class="token punctuation">.</span>chat<span class="token punctuation">.</span>completions<span class="token punctuation">.</span>create<span class="token punctuation">(</span>
    model<span class="token operator">=</span>model_name<span class="token punctuation">,</span>
    messages<span class="token operator">=</span><span class="token punctuation">[</span>
        <span class="token punctuation">{</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"user"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> <span class="token string">"写一首关于春天的诗"</span><span class="token punctuation">}</span>
    <span class="token punctuation">]</span><span class="token punctuation">,</span>
    stream<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    temperature<span class="token operator">=</span><span class="token number">0.8</span>
<span class="token punctuation">)</span>

<span class="token keyword">for</span> chunk <span class="token keyword">in</span> stream<span class="token punctuation">:</span>
    <span class="token keyword">if</span> chunk<span class="token punctuation">.</span>choices<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>delta<span class="token punctuation">.</span>content <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>chunk<span class="token punctuation">.</span>choices<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>delta<span class="token punctuation">.</span>content<span class="token punctuation">,</span> end<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">,</span> flush<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><p><img src="/images/notes/llm/Model-Utils-LMDeploy/client_success.png" alt="客户端调用示例"></p><h2 id="模型量化部署" tabindex="-1">模型量化部署 <a class="header-anchor" href="#模型量化部署" aria-hidden="true">#</a></h2><h3 id="_1-4-bit-awq-量化" tabindex="-1">1. 4-bit AWQ 量化 <a class="header-anchor" href="#_1-4-bit-awq-量化" aria-hidden="true">#</a></h3><pre class="language-bash"><code class="language-bash"><span class="token comment"># 完整参数的AWQ量化命令</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">HF_MODEL</span><span class="token operator">=</span>qwen/Qwen2.5-7B-Instruct
<span class="token builtin class-name">export</span> <span class="token assign-left variable">WORK_DIR</span><span class="token operator">=</span>./qwen2.5-7b-instruct-4bit

lmdeploy lite auto_awq <span class="token punctuation">\</span>
    <span class="token variable">$HF_MODEL</span> <span class="token punctuation">\</span>
    --calib-dataset <span class="token string">'ptb'</span> <span class="token punctuation">\</span>
    --calib-samples <span class="token number">128</span> <span class="token punctuation">\</span>
    --calib-seqlen <span class="token number">2048</span> <span class="token punctuation">\</span>
    --w-bits <span class="token number">4</span> <span class="token punctuation">\</span>
    --w-group-size <span class="token number">128</span> <span class="token punctuation">\</span>
    --batch-size <span class="token number">1</span> <span class="token punctuation">\</span>
    --work-dir <span class="token variable">$WORK_DIR</span>

<span class="token comment"># 简化版本（推荐）</span>
lmdeploy lite auto_awq qwen/Qwen2.5-7B-Instruct --work-dir ./qwen2.5-7b-instruct-4bit

<span class="token comment"># 如果遇到网络连接问题，使用镜像源</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">HF_ENDPOINT</span><span class="token operator">=</span>https://hf-mirror.com
lmdeploy lite auto_awq qwen/Qwen2.5-7B-Instruct --work-dir ./qwen2.5-7b-instruct-4bit
</code></pre><p><strong>参数说明</strong>：</p><ul><li><code>--calib-dataset</code>: 校准数据集，默认’ptb’（Penn Treebank）</li><li><code>--calib-samples</code>: 校准样本数，默认 128</li><li><code>--calib-seqlen</code>: 序列长度，默认 2048</li><li><code>--w-bits</code>: 权重位数，默认 4</li><li><code>--w-group-size</code>: 分组大小，默认 128</li><li><code>--batch-size</code>: 批处理大小，默认 1</li><li><code>--work-dir</code>: 输出目录，建议包含模型名称</li></ul><p><strong>量化成功标志</strong>：</p><p>当看到以下输出时，表示量化成功完成：</p><p><img src="/images/notes/llm/Model-Utils-LMDeploy/quantification_success.png" alt="量化成功标志"></p><h3 id="_2-部署量化模型" tabindex="-1">2. 部署量化模型 <a class="header-anchor" href="#_2-部署量化模型" aria-hidden="true">#</a></h3><pre class="language-python"><code class="language-python"><span class="token comment"># quantized_inference.py</span>
<span class="token keyword">from</span> lmdeploy <span class="token keyword">import</span> pipeline<span class="token punctuation">,</span> TurbomindEngineConfig

<span class="token comment"># 使用量化模型的配置</span>
backend_config <span class="token operator">=</span> TurbomindEngineConfig<span class="token punctuation">(</span>model_format<span class="token operator">=</span><span class="token string">'awq'</span><span class="token punctuation">)</span>

<span class="token comment"># 创建量化模型推理管道（使用本地量化模型路径）</span>
pipe <span class="token operator">=</span> pipeline<span class="token punctuation">(</span>
    <span class="token string">'./qwen2.5-7b-instruct-4bit'</span><span class="token punctuation">,</span>  <span class="token comment"># 本地量化模型路径</span>
    backend_config<span class="token operator">=</span>backend_config
<span class="token punctuation">)</span>

<span class="token comment"># 性能测试</span>
<span class="token keyword">import</span> time
test_prompts <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"解释什么是大语言模型"</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">10</span>

start_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>
responses <span class="token operator">=</span> pipe<span class="token punctuation">(</span>test_prompts<span class="token punctuation">)</span>
end_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"量化模型推理时间: </span><span class="token interpolation"><span class="token punctuation">{</span>end_time <span class="token operator">-</span> start_time<span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string">s"</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"平均每个请求: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token punctuation">(</span>end_time <span class="token operator">-</span> start_time<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">10</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string">s"</span></span><span class="token punctuation">)</span>

<span class="token comment"># 单次对话测试</span>
response <span class="token operator">=</span> pipe<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">"你好，请介绍一下自己"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"量化模型回答:"</span><span class="token punctuation">,</span> response<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>text<span class="token punctuation">)</span>
</code></pre><p><img src="/images/notes/llm/Model-Utils-LMDeploy/out_success.png" alt="部署量化模型"></p><p><strong>推理配置说明</strong>：</p><ul><li>使用本地量化模型路径 <code>./qwen2.5-7b-instruct-4bit</code></li><li>必须指定 <code>model_format='awq'</code> 才能正确加载 AWQ 量化模型</li><li>量化模型相比 FP16 模型可显著降低显存占用和推理时间</li></ul><p><strong>量化验证</strong>：</p><pre class="language-bash"><code class="language-bash"><span class="token comment"># 启动量化模型API服务</span>
lmdeploy serve api_server ./qwen2.5-7b-instruct-4bit <span class="token punctuation">\</span>
    <span class="token parameter variable">--backend</span> turbomind <span class="token punctuation">\</span>
    --model-format awq <span class="token punctuation">\</span>
    --server-port <span class="token number">23333</span>

<span class="token comment"># 测试API服务（另开终端）</span>
<span class="token function">curl</span> <span class="token parameter variable">-X</span> POST http://localhost:23333/v1/chat/completions <span class="token punctuation">\</span>
  <span class="token parameter variable">-H</span> <span class="token string">"Content-Type: application/json"</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">-d</span> <span class="token string">'{
    "model": "./qwen2.5-7b-instruct-4bit",
    "messages": [{"role": "user", "content": "你好，请介绍一下自己"}],
    "temperature": 0.7,
    "max_tokens": 100
  }'</span>
</code></pre><p><img src="/images/notes/llm/Model-Utils-LMDeploy/chat_success.png" alt="验证"></p><p><strong>注意事项</strong>：</p><ul><li>量化过程需要较长时间，建议使用默认参数</li><li>如果量化精度有损，可开启 <code>--search-scale</code> 重新量化</li><li>显存不足时可减小 <code>--calib-seqlen</code> 或设置 <code>--batch-size 1</code></li><li>量化后的 4bit 模型推理速度比 FP16 提升约 2.4 倍</li></ul><h2 id="常见问题与解决方案" tabindex="-1">常见问题与解决方案 <a class="header-anchor" href="#常见问题与解决方案" aria-hidden="true">#</a></h2><h3 id="_1-显存不足-oom" tabindex="-1">1. 显存不足 (OOM) <a class="header-anchor" href="#_1-显存不足-oom" aria-hidden="true">#</a></h3><p><strong>问题现象</strong>：</p><pre class="language-bash"><code class="language-bash"><span class="token punctuation">[</span>TM<span class="token punctuation">]</span><span class="token punctuation">[</span>ERROR<span class="token punctuation">]</span> CUDA runtime error: out of memory
已中止
</code></pre><p><strong>解决方案</strong>：</p><pre class="language-bash"><code class="language-bash"><span class="token comment"># 量化模型内存优化启动（推荐）</span>
lmdeploy chat ./qwen2.5-7b-instruct-4bit --model-format awq <span class="token punctuation">\</span>
    --cache-max-entry-count <span class="token number">0.3</span> <span class="token punctuation">\</span>
    --session-len <span class="token number">2048</span> <span class="token punctuation">\</span>
    --max-batch-size <span class="token number">1</span>

<span class="token comment"># API 服务内存优化启动</span>
lmdeploy serve api_server ./qwen2.5-7b-instruct-4bit <span class="token punctuation">\</span>
    <span class="token parameter variable">--backend</span> turbomind <span class="token punctuation">\</span>
    --model-format awq <span class="token punctuation">\</span>
    --cache-max-entry-count <span class="token number">0.3</span> <span class="token punctuation">\</span>
    --session-len <span class="token number">2048</span> <span class="token punctuation">\</span>
    --max-batch-size <span class="token number">1</span> <span class="token punctuation">\</span>
    --server-port <span class="token number">23333</span>
</code></pre><h3 id="_2-量化失败" tabindex="-1">2. 量化失败 <a class="header-anchor" href="#_2-量化失败" aria-hidden="true">#</a></h3><p><strong>问题现象</strong>：</p><pre class="language-bash"><code class="language-bash">ConnectionError: Couldn<span class="token string">'t reach '</span>ptb_text_only' on the Hub
</code></pre><p><strong>解决方案</strong>：</p><pre class="language-bash"><code class="language-bash"><span class="token comment"># 使用离线数据集（推荐）</span>
lmdeploy lite auto_awq ./models/qwen/Qwen2.5-7B-Instruct <span class="token punctuation">\</span>
    --calib-dataset <span class="token string">'random'</span> <span class="token punctuation">\</span>
    --calib-samples <span class="token number">128</span> <span class="token punctuation">\</span>
    --work-dir ./qwen2.5-7b-instruct-4bit

<span class="token comment"># 或使用镜像源</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">HF_ENDPOINT</span><span class="token operator">=</span>https://hf-mirror.com
lmdeploy lite auto_awq ./models/qwen/Qwen2.5-7B-Instruct <span class="token punctuation">\</span>
    --calib-dataset <span class="token string">'c4'</span> <span class="token punctuation">\</span>
    --work-dir ./qwen2.5-7b-instruct-4bit
</code></pre><h3 id="_3-服务启动失败" tabindex="-1">3. 服务启动失败 <a class="header-anchor" href="#_3-服务启动失败" aria-hidden="true">#</a></h3><p><strong>常见错误</strong>：</p><pre class="language-bash"><code class="language-bash">Address already <span class="token keyword">in</span> use: <span class="token number">23333</span>
</code></pre><p><strong>解决方案</strong>：</p><pre class="language-bash"><code class="language-bash"><span class="token comment"># 检查并释放端口</span>
<span class="token function">lsof</span> <span class="token parameter variable">-i</span> :23333
<span class="token function">kill</span> <span class="token parameter variable">-9</span> <span class="token operator">&lt;</span>PID<span class="token operator">&gt;</span>

<span class="token comment"># 或使用其他端口</span>
lmdeploy serve api_server ./qwen2.5-7b-instruct-4bit <span class="token punctuation">\</span>
    --model-format awq --server-port <span class="token number">23334</span>
</code></pre><h2 id="总结" tabindex="-1">总结 <a class="header-anchor" href="#总结" aria-hidden="true">#</a></h2><p>LMDeploy 作为一个高效的大语言模型部署工具，在 24GB+ 显卡环境下能够很好地支持 Qwen2.5-7B-Instruct 模型的部署和推理。LMDeploy 降低了大语言模型部署的技术门槛，使得开发者可以更容易地将先进的 AI 能力集成到实际应用中，是构建 AI 应用的理想选择。</p></div><!--]--><!--]--></div></article><div class="prose m-auto mt-8 mb-8"><a class="font-mono no-underline opacity-50 hover:opacity-75">cd ..</a></div><!--]--><div class="mt-10 mb-6 prose m-auto opacity-50 flex"><span class="text-sm"><a target="_blank" href="https://beian.miit.gov.cn" style="color:inherit">浙ICP备2021022773号 &nbsp;&nbsp; </a>2022-PRESENT © ZhengKe</span><div class="flex-auto"></div></div></main><!--]--></div><link rel="stylesheet" href="/assets/app.baf61519.css"><link rel="stylesheet" href="/assets/Post.63afffbd.css"></body></html>