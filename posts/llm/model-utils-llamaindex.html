<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="author" content="ZhengKe"><meta http-equiv="X-UA-Compatible" content="chrome=1"><meta name="revisit-after" content="7 days"><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="icon" href="/favicon.svg" type="image/svg+xml"><script src="/lib/mermaid.min.js"></script><meta name="msapplication-TileColor" content="#ffffff"><meta name="theme-color" content="#ffffff"><title>LlamaIndex 学习笔记</title><script>(()=>{var e=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,a=localStorage.getItem("vueuse-color-scheme")||"auto";("dark"===a||e&&"light"!==a)&&document.documentElement.classList.toggle("dark",!0)})()</script><script type="module" crossorigin="" src="/assets/app.95384a36.js"></script><style>*,:after,:before{box-sizing:border-box;border-width:0;border-style:solid;border-color:currentColor}html{line-height:1.5;-webkit-text-size-adjust:100%;-moz-tab-size:4;tab-size:4;font-family:ui-sans-serif,system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,"Apple Color Emoji","Segoe UI Emoji",Segoe UI Symbol,"Noto Color Emoji"}body{margin:0;line-height:inherit}h1,h2,h3{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}strong{font-weight:bolder}code,pre{font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1em}table{text-indent:0;border-color:inherit;border-collapse:collapse}blockquote,h1,h2,h3,p,pre{margin:0}ol,ul{list-style:none;margin:0;padding:0}img,svg{display:block;vertical-align:middle}img{max-width:100%;height:auto}:root{--c-bg:#fff;--c-scrollbar:#eee;--c-scrollbar-hover:#bbb}html{background-color:var(--c-bg)}html{overflow:scroll}*{scrollbar-color:var(--c-scrollbar) var(--c-bg)}.prose{color:var(--fg);max-width:75ch;font-size:1rem;line-height:1.75}.prose a{color:var(--fg-deeper);text-decoration:none;font-weight:500}.prose strong{color:var(--fg-deep);font-weight:600}.prose ol>li{position:relative;padding-left:1.75em}.prose ol>li:before{content:counter(list-item,var(--list-counter-style,decimal)) ".";position:absolute;font-weight:400;color:#6b7280;left:0}.prose ul>li{position:relative;padding-left:1.75em}.prose ul>li:before{content:"";position:absolute;background-color:#d1d5db;border-radius:50%;width:.375em;height:.375em;top:.6875em;left:.25em}.prose blockquote{font-weight:500;font-style:italic;color:inherit;border-left-width:.25rem;border-color:#7d7d7d4d;quotes:"\201c""\201d""\2018""\2019";margin-top:1.6em;margin-bottom:1.6em;padding-left:1em}.prose blockquote p:first-of-type:before{content:open-quote}.prose blockquote p:last-of-type:after{content:close-quote}.prose h1{color:var(--fg-deeper);font-weight:800;font-size:2.25em;margin-top:0;margin-bottom:.8888889em;line-height:1.1111111}.prose h2{color:var(--fg-deep);font-weight:700;font-size:1.5em;margin-top:2em;margin-bottom:1em;line-height:1.3333333}.prose h3{color:inherit;font-weight:600;font-size:1.25em;margin-top:1.6em;margin-bottom:.6em;line-height:1.6}.prose code{color:var(--fg-deep);font-weight:600;font-size:.875em}.prose code:before{content:"`"}.prose code:after{content:"`"}.prose pre{color:#e5e7eb;background-color:#1f2937;overflow-x:auto;font-size:.875em;line-height:1.7142857;margin-top:1.7142857em;margin-bottom:1.7142857em;border-radius:.375rem;padding:.8571429em 1.1428571em}.prose pre code{background-color:transparent;border-width:0;border-radius:0;padding:0;font-weight:400;color:inherit;font-size:inherit;font-family:inherit;line-height:inherit}.prose pre code:before{content:none}.prose pre code:after{content:none}.prose table{width:100%;table-layout:auto;text-align:left;margin-top:2em;margin-bottom:2em;font-size:.875em;line-height:1.7142857}.prose thead{color:#111827;font-weight:600;border-bottom-width:1px;border-bottom-color:#d1d5db}.prose thead th{vertical-align:bottom;padding-right:.5714286em;padding-bottom:.5714286em;padding-left:.5714286em}.prose tbody tr{border-bottom-width:1px;border-bottom-color:#e5e7eb}.prose tbody tr:last-child{border-bottom-width:0}.prose tbody td{vertical-align:top;padding:.5714286em}.prose p{margin-top:1.25em;margin-bottom:1.25em}.prose ol,.prose ul{margin-top:1.25em;margin-bottom:1.25em;list-style-type:none}.prose li{margin-top:.5em;margin-bottom:.5em}.prose>ol>li>:first-child{margin-top:1.25em}.prose>ol>li>:last-child{margin-bottom:1.25em}.prose ol ul,.prose ul ul{margin-top:.75em;margin-bottom:.75em}.prose thead th:first-child{padding-left:0}.prose thead th:last-child{padding-right:0}.prose tbody td:first-child{padding-left:0}.prose tbody td:last-child{padding-right:0}.prose>:first-child{margin-top:0}.prose>:last-child{margin-bottom:0}:root{--prism-scheme:light;--prism-foreground:#6e6e6e;--prism-background:#f4f4f4;--prism-comment:#a8a8a8;--prism-string:#555555;--prism-literal:#333333;--prism-keyword:#000000;--prism-function:#4f4f4f;--prism-deleted:#333333;--prism-class:#333333;--prism-builtin:#757575;--prism-property:#333333;--prism-namespace:#4f4f4f;--prism-punctuation:#ababab;--prism-decorator:var(--prism-class);--prism-operator:var(--prism-punctuation);--prism-number:var(--prism-literal);--prism-boolean:var(--prism-literal);--prism-variable:var(--prism-literal);--prism-constant:var(--prism-literal);--prism-symbol:var(--prism-literal);--prism-interpolation:var(--prism-literal);--prism-selector:var(--prism-keyword);--prism-keyword-control:var(--prism-keyword);--prism-regex:var(--prism-string);--prism-json-property:var(--prism-property);--prism-inline-background:var(--prism-background);--prism-comment-style:italic;--prism-url-decoration:underline;--prism-line-number:#a5a5a5;--prism-line-number-gutter:#333333;--prism-line-highlight-background:#eeeeee;--prism-selection-background:#dddddd;--prism-marker-color:var(--prism-foreground);--prism-marker-opacity:.4;--prism-marker-font-size:.8em;--prism-font-size:1em;--prism-line-height:1.5em;--prism-font-family:monospace;--prism-inline-font-size:var(--prism-font-size);--prism-block-font-size:var(--prism-font-size);--prism-tab-size:2;--prism-block-padding-x:1em;--prism-block-padding-y:1em;--prism-block-margin-x:0;--prism-block-margin-y:.5em;--prism-block-radius:.3em;--prism-inline-padding-x:.3em;--prism-inline-padding-y:.1em;--prism-inline-radius:.3em}code[class*=language-],pre[class*=language-]{font-size:var(--prism-font-size);font-family:var(--prism-font-family);direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;line-height:var(--prism-line-height);-moz-tab-size:var(--prism-tab-size);-o-tab-size:var(--prism-tab-size);tab-size:var(--prism-tab-size);-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;color:var(--prism-foreground)!important}pre[class*=language-]{font-size:var(--prism-block-font-size);padding:var(--prism-block-padding-y) var(--prism-block-padding-x);margin:var(--prism-block-margin-y) var(--prism-block-margin-x);border-radius:var(--prism-block-radius);overflow:auto;background:var(--prism-background)}code[class*=language-] ::-moz-selection,code[class*=language-]::-moz-selection,pre[class*=language-] ::-moz-selection,pre[class*=language-]::-moz-selection{background:var(--prism-selection-background)}code[class*=language-] ::selection,code[class*=language-]::selection,pre[class*=language-] ::selection,pre[class*=language-]::selection{background:var(--prism-selection-background)}.token.comment{color:var(--prism-comment);font-style:var(--prism-comment-style)}.token.interpolation{color:var(--prism-interpolation)}.token.string{color:var(--prism-string)}.token.punctuation{color:var(--prism-punctuation)}.token.operator{color:var(--prism-operator)}.token.boolean{color:var(--prism-boolean)}.token.number{color:var(--prism-number)}.token.keyword{color:var(--prism-keyword)}.token.function{color:var(--prism-function)}:root{--prism-font-size:.9rem;--prism-font-family:"Fira Code",monospace}:root{--prism-font-family:"Input Mono",monospace}html:not(.dark){--prism-foreground:#393a34;--prism-background:#fbfbfb;--prism-comment:#a0ada0;--prism-string:#b56959;--prism-literal:#2f8a89;--prism-number:#296aa3;--prism-keyword:#1c6b48;--prism-function:#6c7834;--prism-boolean:#1c6b48;--prism-constant:#a65e2b;--prism-deleted:#a14f55;--prism-class:#2993a3;--prism-builtin:#ab5959;--prism-property:#b58451;--prism-namespace:#b05a78;--prism-punctuation:#8e8f8b;--prism-decorator:#bd8f8f;--prism-regex:#ab5e3f;--prism-json-property:#698c96}.prose{--fg:#555;--fg-deep:#222;--fg-deeper:#000;color:var(--fg)}.prose a{font-weight:inherit;text-decoration:none;border-bottom:1px solid rgba(125,125,125,.3);transition:border .3s ease-in-out}.prose a:hover{border-bottom:1px solid var(--fg)}a.header-anchor{float:left;margin-top:.125em;margin-left:-1.2em;padding-right:.5em;font-size:.85em;opacity:0;text-decoration:none;border:0!important}a.header-anchor:focus,a.header-anchor:hover{text-decoration:none}h2:focus .header-anchor,h2:hover .header-anchor,h3:focus .header-anchor,h3:hover .header-anchor{opacity:.5}.prose blockquote{font-weight:400;font-style:normal;line-height:1.5em;padding:.6em 1.2em;opacity:.8}.prose blockquote>:first-child{margin-top:0}.prose blockquote>:last-child{margin-bottom:0}.prose blockquote p:first-of-type:before{content:none}.prose blockquote p:first-of-type:after{content:none}.absolute{position:absolute}.z-40{z-index:40}.m-6{margin:1.5rem}.m-auto{margin:auto}.\!-mt-2{margin-top:-.5rem!important}.mb-0{margin-bottom:0}.mb-6{margin-bottom:1.5rem}.mb-8{margin-bottom:2rem}.mt-10{margin-top:2.5rem}.mt-8{margin-top:2rem}.h-10{height:2.5rem}.w-10{width:2.5rem}.flex{display:flex}.flex-auto{flex:1 1 auto}.select-none{user-select:none}.px-7{padding-left:1.75rem;padding-right:1.75rem}.py-10{padding-top:2.5rem;padding-bottom:2.5rem}.font-mono{font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace}.font-sans{font-family:Inter,Inter var,system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}.text-sm{font-size:.875rem;line-height:1.25rem}.text-gray-700{--un-text-opacity:1;color:rgba(55,65,81,var(--un-text-opacity))}.no-underline{text-decoration:none}.opacity-50{opacity:.5}.hover\:opacity-75:hover{opacity:.75}.outline-none{outline:2px solid transparent;outline-offset:2px}@media (max-width:768px){.lt-md\:hidden{display:none}}@media (min-width:768px){.md\:hidden{display:none}}@media (min-width:1024px){.lg\:fixed{position:fixed}}.nav[data-v-568133b8]{padding:2rem;width:100%;display:grid;grid-template-columns:auto max-content;box-sizing:border-box}.nav[data-v-568133b8]>*{margin:auto}.nav a[data-v-568133b8]{cursor:pointer;text-decoration:none;color:inherit;transition:opacity .2s ease;opacity:.6;outline:0}.nav a[data-v-568133b8]:hover{opacity:1;text-decoration-color:inherit}.nav .right[data-v-568133b8]{display:grid;grid-gap:1.2rem;grid-auto-flow:column}.nav .right[data-v-568133b8]>*{margin:auto}.mathjax-container[data-v-011ba505]{color:inherit;font-family:inherit;line-height:inherit}</style><link rel="preload" href="/assets/app.baf61519.css" as="style"><link rel="modulepreload" crossorigin="" href="/assets/Model-Utils-LlamaIndex.f925ce17.js"><link rel="modulepreload" crossorigin="" href="/assets/Post.14d9d5ce.js"><link rel="preload" href="/assets/Post.63afffbd.css" as="style"><meta property="og:title" content="LlamaIndex 学习笔记"><meta property="og:description" content="LlamaIndex 是一个强大的工具，用于构建和部署由大型语言模型（LLM）驱动的应用程序。本笔记旨在帮助初学者快速了解其核心概念和用法。"><meta name="description" content="LlamaIndex 是一个强大的工具，用于构建和部署由大型语言模型（LLM）驱动的应用程序。本笔记旨在帮助初学者快速了解其核心概念和用法。"><meta name="head:count" content="3"></head><body class="font-sans text-gray-700 dark:text-gray-200"><div id="app" data-server-rendered="true"><!--[--><header class="header z-40" data-v-568133b8=""><a href="/" class="w-10 h-10 absolute lg:fixed m-6 select-none outline-none" focusable="false" data-v-568133b8=""><img src="/logo-dark.svg" alt="logo" style="display:none" data-v-568133b8=""><img src="/logo.svg" alt="logo" data-v-568133b8=""></a><nav class="nav" data-v-568133b8=""><div class="spacer" data-v-568133b8=""></div><div class="right" data-v-568133b8=""><a href="/posts" class="" data-v-568133b8=""><span class="lt-md:hidden" data-v-568133b8="">Blog</span><svg style="vertical-align:sub" class="inline md:hidden" width="1.2em" height="1.2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24" data-v-568133b8=""><path fill="currentColor" d="M20 22H4a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1h16a1 1 0 0 1 1 1v18a1 1 0 0 1-1 1m-1-2V4H5v16zM7 6h4v4H7zm0 6h10v2H7zm0 4h10v2H7zm6-9h4v2h-4z"></path></svg></a><a href="/llm" class="lt-md:hidden" data-v-568133b8="">LLM </a><a href="/notes" class="" title="Notes" data-v-568133b8=""><span class="lt-md:hidden" data-v-568133b8="">Notes</span><svg style="vertical-align:sub" class="inline md:hidden" width="1.2em" height="1.2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24" data-v-568133b8=""><path fill="currentColor" d="m17.85 11.698l-.708-.707l-9.9 9.9H3v-4.243L14.314 5.334l5.657 5.657a1 1 0 0 1 0 1.414L12.9 19.477l-1.415-1.415zm-2.122-2.121l-1.414-1.414L5 17.477v1.414h1.414zm2.828-7.071l2.829 2.828a1 1 0 0 1 0 1.415L19.97 8.163L15.728 3.92l1.414-1.414a1 1 0 0 1 1.414 0"></path></svg></a><a href="https://github.com/ZhengKe0110" target="_blank" title="GitHub" class="lt-md:hidden" data-v-568133b8=""><svg style="vertical-align:sub" class="inline" width="1.2em" height="1.2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24" data-v-568133b8=""><path fill="currentColor" d="M10.07 20.503a1 1 0 0 0-1.18-.983c-1.31.24-2.963.276-3.402-.958a5.7 5.7 0 0 0-1.837-2.415a1 1 0 0 1-.167-.11a1 1 0 0 0-.93-.645h-.005a1 1 0 0 0-1 .995c-.004.815.81 1.338 1.141 1.514a4.4 4.4 0 0 1 .924 1.36c.365 1.023 1.423 2.576 4.466 2.376l.003.098l.004.268a1 1 0 0 0 2 0l-.005-.318c-.005-.19-.012-.464-.012-1.182M20.737 5.377q.049-.187.09-.42a6.3 6.3 0 0 0-.408-3.293a1 1 0 0 0-.615-.58c-.356-.12-1.67-.357-4.184 1.25a13.9 13.9 0 0 0-6.354 0C6.762.75 5.455.966 5.102 1.079a1 1 0 0 0-.631.584a6.3 6.3 0 0 0-.404 3.357q.037.191.079.354a6.27 6.27 0 0 0-1.256 3.83a8 8 0 0 0 .043.921c.334 4.603 3.334 5.984 5.424 6.459a5 5 0 0 0-.118.4a1 1 0 0 0 1.942.479a1.7 1.7 0 0 1 .468-.878a1 1 0 0 0-.546-1.745c-3.454-.395-4.954-1.802-5.18-4.899a7 7 0 0 1-.033-.738a4.26 4.26 0 0 1 .92-2.713a3 3 0 0 1 .195-.231a1 1 0 0 0 .188-1.025a3.4 3.4 0 0 1-.155-.555a4.1 4.1 0 0 1 .079-1.616a7.5 7.5 0 0 1 2.415 1.18a1 1 0 0 0 .827.133a11.8 11.8 0 0 1 6.173.001a1 1 0 0 0 .83-.138a7.6 7.6 0 0 1 2.406-1.19a4 4 0 0 1 .087 1.578a3.2 3.2 0 0 1-.169.607a1 1 0 0 0 .188 1.025c.078.087.155.18.224.268A4.12 4.12 0 0 1 20 9.203a7 7 0 0 1-.038.777c-.22 3.056-1.725 4.464-5.195 4.86a1 1 0 0 0-.546 1.746a1.63 1.63 0 0 1 .466.908a3 3 0 0 1 .093.82v2.333c-.01.648-.01 1.133-.01 1.356a1 1 0 1 0 2 0c0-.217 0-.692.01-1.34v-2.35a5 5 0 0 0-.155-1.311a4 4 0 0 0-.116-.416a6.51 6.51 0 0 0 5.445-6.424A9 9 0 0 0 22 9.203a6.13 6.13 0 0 0-1.263-3.826"></path></svg></a><a class="select-none" title="Toggle Color Scheme" data-v-568133b8=""><svg style="vertical-align:sub;display:none" class="inline" width="1.2em" height="1.2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="currentColor" d="M10 7a7 7 0 0 0 12 4.9v.1c0 5.523-4.477 10-10 10S2 17.523 2 12S6.477 2 12 2h.1A6.98 6.98 0 0 0 10 7m-6 5a8 8 0 0 0 15.062 3.762A9 9 0 0 1 8.238 4.938A8 8 0 0 0 4 12"></path></svg><svg style="vertical-align:sub" class="inline" width="1.2em" height="1.2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="currentColor" d="M12 18a6 6 0 1 1 0-12a6 6 0 0 1 0 12m0-2a4 4 0 1 0 0-8a4 4 0 0 0 0 8M11 1h2v3h-2zm0 19h2v3h-2zM3.515 4.929l1.414-1.414L7.05 5.636L5.636 7.05zM16.95 18.364l1.414-1.414l2.121 2.121l-1.414 1.414zm2.121-14.85l1.414 1.415l-2.121 2.121l-1.414-1.414zM5.636 16.95l1.414 1.414l-2.121 2.121l-1.414-1.414zM23 11v2h-3v-2zM4 11v2H1v-2z"></path></svg></a></div></nav></header><main class="px-7 py-10"><!--[--><div class="prose m-auto mb-8"><h1 class="mb-0">LlamaIndex 学习笔记</h1><p class="opacity-50 !-mt-2">Aug 19<!----></p><!----></div><article><div class="mathjax-container" data-v-011ba505=""><!--[--><!--[--><div class="prose m-auto"><h2 id="_1-llamaindex-是什么？" tabindex="-1">1. LlamaIndex 是什么？ <a class="header-anchor" href="#_1-llamaindex-是什么？" aria-hidden="true">#</a></h2><p>LlamaIndex 是一个用于 LLM 应用程序的<strong>数据框架</strong>，旨在帮助开发者注入、结构化并访问私有或特定领域的数据。它通过**上下文增强（Context Augmentation）**技术，将您的数据安全地提供给大型语言模型（LLM），从而让 LLM 能够基于这些数据进行准确的回答。</p><p>最常见的上下文增强技术是<strong>检索增强生成（Retrieval-Augmented Generation, RAG）</strong>，它在推理时将从您的数据中检索到的相关上下文与用户的查询一起提供给 LLM。LlamaIndex 为构建从原型到生产的各种 RAG 应用提供了全面的工具。</p><h2 id="_2-核心组件" tabindex="-1">2. 核心组件 <a class="header-anchor" href="#_2-核心组件" aria-hidden="true">#</a></h2><p>LlamaIndex 提供了一系列可组合的模块，使得构建 LLM 应用变得更加简单：</p><ul><li><strong>数据连接器（Data Connectors）</strong>: 从各种数据源（如 API、PDF、SQL 数据库等）中提取数据，并将其转换为 LlamaIndex 支持的 <code>Document</code> 对象。</li><li><strong>文档（Documents）/ 节点（Nodes）</strong>: <code>Document</code> 是数据的通用容器，而 <code>Node</code> 是 <code>Document</code> 的分块，是 LlamaIndex 中最小的数据单元。这种分块处理有助于实现更精确和高效的数据检索。</li><li><strong>数据索引（Data Indexes）</strong>: 将 <code>Node</code> 对象结构化，以便于快速检索。最常见的索引是 <code>VectorStoreIndex</code>，它为每个 <code>Node</code> 生成向量嵌入。</li><li><strong>检索器（Retrievers）</strong>: 定义如何根据用户查询高效地从索引中检索相关的 <code>Node</code>。</li><li><strong>响应合成器（Response Synthesizers）</strong>: 根据用户查询和检索到的上下文（文本块），利用 LLM 生成最终的自然语言响应。</li><li><strong>引擎（Engines）</strong>: 提供与数据进行自然语言交互的端到端接口。<ul><li><strong>查询引擎（Query Engines）</strong>: 用于单次问答（Q&amp;A）的强大接口。</li><li><strong>聊天引擎（Chat Engines）</strong>: 用于多轮对话的交互式接口。</li></ul></li><li><strong>代理（Agents）</strong>: 由 LLM 驱动的自动化决策者，可以动态地使用各种工具（如查询引擎、其他 API）来完成更复杂的任务。</li></ul><h2 id="_3-深入理解-rag：llamaindex-的核心" tabindex="-1">3. 深入理解 RAG：LlamaIndex 的核心 <a class="header-anchor" href="#_3-深入理解-rag：llamaindex-的核心" aria-hidden="true">#</a></h2><p>RAG 是 LlamaIndex 应用的核心，它包含两个主要阶段：</p><h3 id="_3-1-索引阶段-indexing-phase" tabindex="-1">3.1 索引阶段 (Indexing Phase) <a class="header-anchor" href="#_3-1-索引阶段-indexing-phase" aria-hidden="true">#</a></h3><p>这是构建知识库的过程，目的是让数据为查询做好准备。</p><ol><li><strong>加载数据</strong>: 使用<strong>数据连接器</strong>从不同的数据源加载数据。</li><li><strong>解析与分块</strong>: LlamaIndex 将加载的 <code>Documents</code> 解析成一系列 <code>Nodes</code>（文本块）。</li><li><strong>生成嵌入与索引</strong>: 对每个 <code>Node</code>，LlamaIndex 会使用一个嵌入模型（如 OpenAI 的 <code>text-embedding-ada-002</code>）来生成向量嵌入，然后将这些 <code>Node</code> 和它们的嵌入存储在<strong>数据索引</strong>中。</li></ol><p>这个流程可以概括为：<strong>Data Sources → Data Connectors → Documents → Nodes → Knowledge Base (Index)</strong></p><h3 id="_3-2-查询阶段-querying-phase" tabindex="-1">3.2 查询阶段 (Querying Phase) <a class="header-anchor" href="#_3-2-查询阶段-querying-phase" aria-hidden="true">#</a></h3><p>当用户提出查询时，RAG 管道会从知识库中检索相关信息，并将其传递给 LLM 以生成响应。</p><ol><li><strong>查询嵌入</strong>: LlamaIndex 会使用相同的嵌入模型为用户的查询生成一个向量嵌入。</li><li><strong>检索上下文</strong>: <strong>检索器</strong>使用查询的向量在数据索引中进行相似度搜索，找出最相关的 <code>Nodes</code>（上下文）。</li><li><strong>后处理 (Optional)</strong>: <strong>Node Postprocessors</strong> 可以对检索到的 <code>Node</code> 列表进行转换、过滤或重新排序。</li><li><strong>合成响应</strong>: <strong>响应合成器</strong>将用户的原始查询和检索到的上下文一起发送给 LLM，由 LLM 生成最终的回答。</li></ol><p>这个流程可以概括为：<strong>Knowledge Base → Retriever → Node Postprocessors → Response Synthesizer → Final Response</strong></p><h2 id="_4-主要用例" tabindex="-1">4. 主要用例 <a class="header-anchor" href="#_4-主要用例" aria-hidden="true">#</a></h2><p>LlamaIndex 可以用于构建多种应用：</p><ul><li><strong>问答系统（RAG）</strong>: 基于您的文档或数据进行提问和回答。</li><li><strong>聊天机器人</strong>: 创建能与您的数据进行对话的机器人。</li><li><strong>文档理解与数据提取</strong>: 从非结构化文档中提取结构化信息。</li><li><strong>自主代理</strong>: 构建能够执行研究、采取行动的智能代理。</li><li><strong>多模态应用</strong>: 结合文本、图像等多种数据类型。</li><li><strong>模型微调</strong>: 在您的数据上微调模型以提升性能。</li></ul><h2 id="_5-实战案例" tabindex="-1">5. 实战案例 <a class="header-anchor" href="#_5-实战案例" aria-hidden="true">#</a></h2><h3 id="_5-1-案例一：基础对话（无-rag）-模型依赖通用知识" tabindex="-1">5.1 案例一：基础对话（无 RAG）- 模型依赖通用知识 <a class="header-anchor" href="#_5-1-案例一：基础对话（无-rag）-模型依赖通用知识" aria-hidden="true">#</a></h3><p>本案例展示<strong>传统的直接对话模式</strong>，模型只能依赖训练时学到的通用知识来回答问题，无法访问外部知识库或最新信息。</p><p><strong>技术特点</strong>:</p><ul><li>仅依赖模型内置知识</li><li>无法获取特定领域信息</li><li>对新概念或专业术语可能回答不准确</li><li>无法追溯信息来源</li></ul><p><strong>应用场景</strong>: 通用对话、基础常识问答、不需要特定领域知识的任务</p><p><strong>操作流程:</strong></p><ol><li><p><strong>环境准备</strong>:</p><p>首先，确保您已安装所有必需的 Python 库。</p><pre class="language-bash"><code class="language-bash">pip <span class="token function">install</span> llama-index-llms-huggingface torch accelerate
</code></pre></li><li><p><strong>创建并运行脚本</strong>:</p><p>假设您已将模型下载至 <code>./models/qwen/Qwen1.5-1.8B-Chat</code> 目录。将以下代码保存为 <code>basic_chat.py</code> 文件，它将直接加载本地模型进行对话。</p><pre class="language-python"><code class="language-python"><span class="token comment"># basic_chat.py</span>
<span class="token keyword">from</span> llama_index<span class="token punctuation">.</span>core<span class="token punctuation">.</span>llms <span class="token keyword">import</span> ChatMessage
<span class="token keyword">from</span> llama_index<span class="token punctuation">.</span>llms<span class="token punctuation">.</span>huggingface <span class="token keyword">import</span> HuggingFaceLLM

<span class="token comment"># --- 1. 指定本地模型路径 ---</span>
<span class="token comment"># 请确保此路径下已包含预先下载的模型文件</span>
model_local_path <span class="token operator">=</span> <span class="token string">"./models/qwen/Qwen1.5-1.8B-Chat"</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"正在从本地路径加载模型: </span><span class="token interpolation"><span class="token punctuation">{</span>model_local_path<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>

<span class="token comment"># --- 2. 初始化 LLM ---</span>
llm <span class="token operator">=</span> HuggingFaceLLM<span class="token punctuation">(</span>
    model_name<span class="token operator">=</span>model_local_path<span class="token punctuation">,</span>
    tokenizer_name<span class="token operator">=</span>model_local_path<span class="token punctuation">,</span>
    model_kwargs<span class="token operator">=</span><span class="token punctuation">{</span><span class="token string">"trust_remote_code"</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
    tokenizer_kwargs<span class="token operator">=</span><span class="token punctuation">{</span><span class="token string">"trust_remote_code"</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token punctuation">}</span>
<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"模型初始化完成。"</span><span class="token punctuation">)</span>

<span class="token comment"># --- 3. 执行对话 ---</span>
rsp <span class="token operator">=</span> llm<span class="token punctuation">.</span>chat<span class="token punctuation">(</span>messages<span class="token operator">=</span><span class="token punctuation">[</span>ChatMessage<span class="token punctuation">(</span>content<span class="token operator">=</span><span class="token string">"xtuner是什么？"</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\n--- 模型回答 ---"</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>rsp<span class="token punctuation">)</span>
</code></pre></li><li><p><strong>执行</strong>:</p><p>在终端中运行此脚本。</p><pre class="language-bash"><code class="language-bash">python basic_chat.py
</code></pre></li></ol><h3 id="_5-2-案例二：rag-应用-基于知识库的精准问答" tabindex="-1">5.2 案例二：RAG 应用 - 基于知识库的精准问答 <a class="header-anchor" href="#_5-2-案例二：rag-应用-基于知识库的精准问答" aria-hidden="true">#</a></h3><p>本案例展示<strong>RAG（检索增强生成）技术</strong>的强大能力，通过构建本地知识库，模型可以基于特定文档内容给出准确、具体、可追溯的回答。</p><p><strong>技术优势</strong>:</p><ul><li>基于真实文档内容回答</li><li>支持特定领域和专业知识</li><li>答案准确性和相关性更高</li><li>可以引用具体的文档来源</li><li>支持知识库实时更新</li></ul><p><strong>应用场景</strong>: 企业知识库问答、技术文档检索、专业领域咨询、客服系统</p><p><strong>操作流程:</strong></p><ol><li><p><strong>环境准备</strong>:</p><p>首先，确保您已安装所有必需的 Python 库。</p><pre class="language-bash"><code class="language-bash">pip <span class="token function">install</span> llama-index-embeddings-huggingface llama-index-llms-huggingface torch accelerate
</code></pre></li><li><p><strong>准备模型与数据</strong>:</p><ul><li><strong>模型</strong>: 确保您已预先下载了语言模型（LLM）和嵌入模型（Embedding Model），并分别存放在以下路径：<ul><li><code>./models/qwen/Qwen1.5-1.8B-Chat</code></li><li><code>./models/damo/sentence-transformers-paraphrase-multilingual-MiniLM-L12-v2</code></li></ul></li><li><strong>数据</strong>: 在项目根目录下创建一个 <code>data</code> 文件夹，并将您的知识库文档（如 <code>.txt</code>, <code>.md</code> 文件）放入其中。</li></ul></li><li><p><strong>创建并运行脚本</strong>:</p><p>将以下代码保存为 <code>rag_chat.py</code> 文件。该脚本将加载本地模型和数据，构建一个基于 RAG 的查询引擎，并回答您的问题。</p><pre class="language-python"><code class="language-python"><span class="token comment"># rag_chat.py</span>
<span class="token keyword">from</span> llama_index<span class="token punctuation">.</span>embeddings<span class="token punctuation">.</span>huggingface <span class="token keyword">import</span> HuggingFaceEmbedding
<span class="token keyword">from</span> llama_index<span class="token punctuation">.</span>core <span class="token keyword">import</span> Settings<span class="token punctuation">,</span> SimpleDirectoryReader<span class="token punctuation">,</span> VectorStoreIndex
<span class="token keyword">from</span> llama_index<span class="token punctuation">.</span>llms<span class="token punctuation">.</span>huggingface <span class="token keyword">import</span> HuggingFaceLLM

<span class="token comment"># --- 1. 定义模型和数据路径 ---</span>
llm_model_path <span class="token operator">=</span> <span class="token string">"./models/qwen/Qwen1.5-1.8B-Chat"</span>
embedding_model_path <span class="token operator">=</span> <span class="token string">"./models/damo/sentence-transformers-paraphrase-multilingual-MiniLM-L12-v2"</span>
data_path <span class="token operator">=</span> <span class="token string">"./data"</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"模型和数据路径已配置。"</span><span class="token punctuation">)</span>

<span class="token comment"># --- 2. 配置全局设置 (Settings) ---</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\n正在配置嵌入模型和语言模型..."</span><span class="token punctuation">)</span>
<span class="token comment"># 配置嵌入模型</span>
embed_model <span class="token operator">=</span> HuggingFaceEmbedding<span class="token punctuation">(</span>model_name<span class="token operator">=</span>embedding_model_path<span class="token punctuation">)</span>
Settings<span class="token punctuation">.</span>embed_model <span class="token operator">=</span> embed_model

<span class="token comment"># 配置语言模型</span>
llm <span class="token operator">=</span> HuggingFaceLLM<span class="token punctuation">(</span>
    model_name<span class="token operator">=</span>llm_model_path<span class="token punctuation">,</span>
    tokenizer_name<span class="token operator">=</span>llm_model_path<span class="token punctuation">,</span>
    model_kwargs<span class="token operator">=</span><span class="token punctuation">{</span><span class="token string">"trust_remote_code"</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
    tokenizer_kwargs<span class="token operator">=</span><span class="token punctuation">{</span><span class="token string">"trust_remote_code"</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token punctuation">}</span>
<span class="token punctuation">)</span>
Settings<span class="token punctuation">.</span>llm <span class="token operator">=</span> llm
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"全局设置配置完成。"</span><span class="token punctuation">)</span>

<span class="token comment"># --- 3. 加载数据并构建索引 ---</span>
documents <span class="token operator">=</span> SimpleDirectoryReader<span class="token punctuation">(</span>data_path<span class="token punctuation">)</span><span class="token punctuation">.</span>load_data<span class="token punctuation">(</span><span class="token punctuation">)</span>
index <span class="token operator">=</span> VectorStoreIndex<span class="token punctuation">.</span>from_documents<span class="token punctuation">(</span>documents<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"索引构建完成。"</span><span class="token punctuation">)</span>

<span class="token comment"># --- 4. 创建查询引擎并提问 ---</span>
query_engine <span class="token operator">=</span> index<span class="token punctuation">.</span>as_query_engine<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"查询引擎已就绪。"</span><span class="token punctuation">)</span>

rsp <span class="token operator">=</span> query_engine<span class="token punctuation">.</span>query<span class="token punctuation">(</span><span class="token string">"xtuner是什么？"</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\n--- 模型回答 ---"</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>rsp<span class="token punctuation">)</span>
</code></pre></li><li><p><strong>执行</strong>:</p><p>在终端中运行此脚本。</p><pre class="language-bash"><code class="language-bash">python rag_chat.py
</code></pre></li></ol><p><strong>核心差异对比</strong>:</p><table><thead><tr><th>对比维度</th><th>案例一（无 RAG）</th><th>案例二（RAG）</th></tr></thead><tbody><tr><td><strong>知识来源</strong></td><td>仅依赖模型训练时的通用知识</td><td>本地知识库文档 + 模型知识</td></tr><tr><td><strong>回答准确性</strong></td><td>对通用知识较好，专业领域可能不准确</td><td>基于文档内容，准确性显著提升</td></tr><tr><td><strong>信息时效性</strong></td><td>截止到模型训练时间</td><td>可实时更新知识库</td></tr><tr><td><strong>可追溯性</strong></td><td>无法追溯信息来源</td><td>可显示引用的具体文档片段</td></tr><tr><td><strong>专业领域</strong></td><td>表现一般，可能出现幻觉</td><td>表现优秀，基于真实文档</td></tr><tr><td><strong>部署复杂度</strong></td><td>简单，只需加载模型</td><td>复杂，需要构建索引和向量库</td></tr><tr><td><strong>资源消耗</strong></td><td>较低</td><td>较高（需要嵌入模型和向量存储）</td></tr></tbody></table><p><strong>实际效果对比</strong>:</p><ul><li>对于"xtuner 是什么？"这类问题：<ul><li>案例一：可能回答不准确或表示不知道</li><li>案例二：如果知识库中有相关文档，将给出准确且详细的回答</li></ul></li></ul><h3 id="_5-3-案例三：集成持久化向量数据库-chromadb" tabindex="-1">5.3 案例三：集成持久化向量数据库 ChromaDB <a class="header-anchor" href="#_5-3-案例三：集成持久化向量数据库-chromadb" aria-hidden="true">#</a></h3><p>案例二中的向量索引是存储在内存里的，程序每次重启都需要重新构建。此案例将展示如何使用 ChromaDB 将索引持久化到本地磁盘，实现“一次构建，多次使用”。</p><p><strong>技术优势</strong>:</p><ul><li><strong>持久化存储</strong>: 索引存储在磁盘上，重启程序无需重新计算。</li><li><strong>高效加载</strong>: 再次运行时直接加载现有索引，启动速度快。</li><li><strong>可扩展性</strong>: 支持更大的数据集，不受内存限制。</li></ul><p><strong>操作流程:</strong></p><ol><li><p><strong>环境准备</strong>:</p><p>安装 ChromaDB 及其与 LlamaIndex 集成的相关库。</p><pre class="language-bash"><code class="language-bash">pip <span class="token function">install</span> llama-index-vector-stores-chroma chromadb
</code></pre></li><li><p><strong>准备模型与数据</strong>:</p><p>与案例二的要求相同，确保所需的模型和数据文件已放置在正确目录下。</p></li><li><p><strong>创建并运行脚本</strong>:</p><p>将以下代码保存为 <code>rag_persistent.py</code>。该脚本会智能地处理索引：</p><ul><li><strong>首次运行</strong>：加载文档，创建索引，并将其保存到本地的 <code>chroma_db</code> 文件夹中。</li><li><strong>再次运行</strong>：直接从 <code>chroma_db</code> 文件夹加载现有索引，跳过耗时的创建过程。</li></ul><pre class="language-python"><code class="language-python"><span class="token comment"># rag_persistent.py</span>
<span class="token keyword">import</span> chromadb
<span class="token keyword">import</span> os
<span class="token keyword">from</span> llama_index<span class="token punctuation">.</span>core <span class="token keyword">import</span> Settings<span class="token punctuation">,</span> SimpleDirectoryReader<span class="token punctuation">,</span> VectorStoreIndex<span class="token punctuation">,</span> StorageContext
<span class="token keyword">from</span> llama_index<span class="token punctuation">.</span>embeddings<span class="token punctuation">.</span>huggingface <span class="token keyword">import</span> HuggingFaceEmbedding
<span class="token keyword">from</span> llama_index<span class="token punctuation">.</span>llms<span class="token punctuation">.</span>huggingface <span class="token keyword">import</span> HuggingFaceLLM
<span class="token keyword">from</span> llama_index<span class="token punctuation">.</span>vector_stores<span class="token punctuation">.</span>chroma <span class="token keyword">import</span> ChromaVectorStore

<span class="token comment"># --- 1. 定义所有路径 ---</span>
llm_model_path <span class="token operator">=</span> <span class="token string">"./models/qwen/Qwen1.5-1.8B-Chat"</span>
embedding_model_path <span class="token operator">=</span> <span class="token string">"./models/damo/sentence-transformers-paraphrase-multilingual-MiniLM-L12-v2"</span>
data_path <span class="token operator">=</span> <span class="token string">"./data"</span>
chroma_db_path <span class="token operator">=</span> <span class="token string">"./chroma_db"</span>
collection_name <span class="token operator">=</span> <span class="token string">"rag_knowledge_base"</span>

<span class="token comment"># --- 2. 配置全局模型 ---</span>
Settings<span class="token punctuation">.</span>embed_model <span class="token operator">=</span> HuggingFaceEmbedding<span class="token punctuation">(</span>model_name<span class="token operator">=</span>embedding_model_path<span class="token punctuation">)</span>
Settings<span class="token punctuation">.</span>llm <span class="token operator">=</span> HuggingFaceLLM<span class="token punctuation">(</span>
    model_name<span class="token operator">=</span>llm_model_path<span class="token punctuation">,</span>
    tokenizer_name<span class="token operator">=</span>llm_model_path<span class="token punctuation">,</span>
    model_kwargs<span class="token operator">=</span><span class="token punctuation">{</span><span class="token string">"trust_remote_code"</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
    tokenizer_kwargs<span class="token operator">=</span><span class="token punctuation">{</span><span class="token string">"trust_remote_code"</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token punctuation">}</span>
<span class="token punctuation">)</span>

<span class="token comment"># --- 3. 初始化 ChromaDB 并加载/创建索引 ---</span>
db <span class="token operator">=</span> chromadb<span class="token punctuation">.</span>PersistentClient<span class="token punctuation">(</span>path<span class="token operator">=</span>chroma_db_path<span class="token punctuation">)</span>
chroma_collection <span class="token operator">=</span> db<span class="token punctuation">.</span>get_or_create_collection<span class="token punctuation">(</span>collection_name<span class="token punctuation">)</span>
vector_store <span class="token operator">=</span> ChromaVectorStore<span class="token punctuation">(</span>chroma_collection<span class="token operator">=</span>chroma_collection<span class="token punctuation">)</span>

<span class="token comment"># 检查索引是否已在数据库中存在</span>
<span class="token keyword">if</span> chroma_collection<span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"知识库 '</span><span class="token interpolation"><span class="token punctuation">{</span>collection_name<span class="token punctuation">}</span></span><span class="token string">' 为空，正在从头构建..."</span></span><span class="token punctuation">)</span>
    documents <span class="token operator">=</span> SimpleDirectoryReader<span class="token punctuation">(</span>data_path<span class="token punctuation">)</span><span class="token punctuation">.</span>load_data<span class="token punctuation">(</span><span class="token punctuation">)</span>
    storage_context <span class="token operator">=</span> StorageContext<span class="token punctuation">.</span>from_defaults<span class="token punctuation">(</span>vector_store<span class="token operator">=</span>vector_store<span class="token punctuation">)</span>
    index <span class="token operator">=</span> VectorStoreIndex<span class="token punctuation">.</span>from_documents<span class="token punctuation">(</span>
        documents<span class="token punctuation">,</span> storage_context<span class="token operator">=</span>storage_context
    <span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"新索引已成功构建并保存。"</span><span class="token punctuation">)</span>
<span class="token keyword">else</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"已找到现有知识库 '</span><span class="token interpolation"><span class="token punctuation">{</span>collection_name<span class="token punctuation">}</span></span><span class="token string">'，正在直接加载..."</span></span><span class="token punctuation">)</span>
    index <span class="token operator">=</span> VectorStoreIndex<span class="token punctuation">.</span>from_vector_store<span class="token punctuation">(</span>
        vector_store<span class="token operator">=</span>vector_store<span class="token punctuation">,</span>
    <span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"现有索引加载完成。"</span><span class="token punctuation">)</span>

<span class="token comment"># --- 4. 创建查询引擎并提问 ---</span>
query_engine <span class="token operator">=</span> index<span class="token punctuation">.</span>as_query_engine<span class="token punctuation">(</span><span class="token punctuation">)</span>
rsp <span class="token operator">=</span> query_engine<span class="token punctuation">.</span>query<span class="token punctuation">(</span><span class="token string">"xtuner是什么？"</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\n--- 模型回答 ---"</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>rsp<span class="token punctuation">)</span>
</code></pre></li><li><p><strong>执行</strong>:</p><p>在终端中运行此脚本。</p><pre class="language-bash"><code class="language-bash">python rag_persistent.py
</code></pre><blockquote><p><strong>提示</strong>: 首次运行时，您会看到构建索引的日志。当您再次运行此脚本时，它会跳过构建步骤，直接加载索引，速度会快很多。</p></blockquote></li></ol><p>通过这个案例，我们学到了如何通过集成向量数据库来提升 RAG 应用的效率和实用性。</p><h2 id="_6-三种实现模式对比" tabindex="-1">6. 三种实现模式对比 <a class="header-anchor" href="#_6-三种实现模式对比" aria-hidden="true">#</a></h2><table><thead><tr><th>技术方案</th><th>案例一（基础对话）</th><th>案例二（RAG 应用）</th><th>案例三（持久化 RAG）</th></tr></thead><tbody><tr><td><strong>知识来源</strong></td><td>模型内置知识</td><td>本地知识库 + 模型知识</td><td>持久化知识库 + 模型知识</td></tr><tr><td><strong>存储方式</strong></td><td>无需存储</td><td>内存向量索引</td><td>磁盘持久化向量库</td></tr><tr><td><strong>启动效率</strong></td><td>快速</td><td>每次重建索引</td><td>一次构建，多次复用</td></tr><tr><td><strong>适用场景</strong></td><td>通用对话、原型验证</td><td>专业问答、实时构建</td><td>生产环境、大规模应用</td></tr><tr><td><strong>技术复杂度</strong></td><td>低</td><td>中等</td><td>高</td></tr></tbody></table></div><!--]--><!--]--></div></article><div class="prose m-auto mt-8 mb-8"><a class="font-mono no-underline opacity-50 hover:opacity-75">cd ..</a></div><!--]--><div class="mt-10 mb-6 prose m-auto opacity-50 flex"><span class="text-sm"><a target="_blank" href="https://beian.miit.gov.cn" style="color:inherit">浙ICP备2021022773号 &nbsp;&nbsp; </a>2022-PRESENT © ZhengKe</span><div class="flex-auto"></div></div></main><!--]--></div><link rel="stylesheet" href="/assets/app.baf61519.css"><link rel="stylesheet" href="/assets/Post.63afffbd.css"></body></html>